# -*- coding: utf-8 -*-
import os
import requests
from urllib.parse import urlparse, parse_qs
import base64
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
import argparse
import re
import yaml
import json
import csv
import hashlib
import ipaddress
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from collections import defaultdict
import random
import socket # æ–°å¢ï¼šç”¨äºDNSè§£æ
import functools # æ–°å¢ï¼šç”¨äºç¼“å­˜

# é…ç½®æ—¥å¿—
logging.basicConfig(
    filename='error.log',
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

# è¯·æ±‚å¤´
headers = {
    'User-Agent': (
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
        'AppleWebKit/537.36 (KHTML, like Gecko) '
        'Chrome/91.0.4472.124 Safari/537.36'
    ),
    'Accept-Encoding': 'gzip, deflate'
}

# å‘½ä»¤è¡Œå‚æ•°è§£æ
parser = argparse.ArgumentParser(description="URLå†…å®¹è·å–è„šæœ¬ï¼Œæ”¯æŒå¤šä¸ªURLæ¥æºå’ŒèŠ‚ç‚¹è§£æ")
parser.add_argument('--timeout', type=int, default=30, help="è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰")
parser.add_argument('--output', type=str, default='data/all_clash.yaml', help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
args = parser.parse_args()

# å…¨å±€å˜é‡
TIMEOUT = args.timeout
OUTPUT_FILE = args.output
TEMP_MERGED_NODES_RAW_FILE = 'temp_merged_nodes_raw.txt'
STATISTICS_FILE = 'data/url_statistics.csv'
SUCCESS_URLS_FILE = 'data/successful_urls.txt'
FAILED_URLS_FILE = 'data/failed_urls.txt'

# å®šä¹‰åˆ é™¤å…³é”®è¯
DELETE_KEYWORDS = [
    'å‰©ä½™æµé‡', 'å¥—é¤åˆ°æœŸ', 'æµé‡', 'åˆ°æœŸ', 'è¿‡æœŸ', 'å…è´¹', 'è¯•ç”¨', 'ä½“éªŒ', 'é™æ—¶', 'é™åˆ¶',
    'å·²ç”¨', 'å¯ç”¨', 'ä¸è¶³', 'åˆ°æœŸæ—¶é—´', 'å€ç‡', 'è¿”åˆ©', 'å……å€¼', 'ç»­è´¹', 'ç”¨é‡', 'è®¢é˜…'
]

# é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼
region_pattern = re.compile(r'\b(HK|TW|JP|SG|US|UK|DE|KR|MY|TH|PH|VN|ID|IN|AU|CA|RU|BR|IT|NL|CN|AE|AD|KZ)\b', re.IGNORECASE)
provider_pattern = re.compile(r'\b(AWS|Amazon|Akamai|Oracle|Alibaba|Google|Tencent|Vultr|OVH|DigitalOcean|Core Labs|Cloudflare)\b', re.IGNORECASE)
node_pattern = re.compile(
    r'(vmess://\S+|'
    r'trojan://\S+|'
    r'ss://\S+|'
    r'ssr://\S+|'
    r'vless://\S+|'
    r'hy://\S+|'
    r'hy2://\S+|'
    r'hysteria://\S+|'
    r'hysteria2://\S+)'
)

# å…¨å±€DNSç¼“å­˜
@functools.lru_cache(maxsize=512) # ç¼“å­˜512ä¸ªDNSè§£æç»“æœ
def resolve_hostname_cached(hostname):
    """ç¼“å­˜DNSè§£æç»“æœï¼Œé¿å…é‡å¤æŸ¥è¯¢"""
    try:
        # å°è¯•è§£æIPv4åœ°å€ï¼Œå¯¹äºå»é‡æ¥è¯´ï¼Œè§£æåˆ°ç›¸åŒIPé€šå¸¸æ„å‘³ç€åŒä¸€æœåŠ¡å™¨
        # æ³¨æ„ï¼šå¦‚æœåŒä¸€åŸŸåæœ‰å¤šä¸ªIPï¼Œè¿™é‡Œåªå–ç¬¬ä¸€ä¸ª
        return socket.gethostbyname(hostname)
    except socket.gaierror:
        return None

def is_valid_url(url):
    """éªŒè¯URLæ ¼å¼æ˜¯å¦åˆæ³•ï¼Œä»…æ¥å— http æˆ– https æ–¹æ¡ˆ"""
    try:
        result = urlparse(url)
        return all([result.scheme in ['http', 'https'], result.netloc])
    except Exception:
        return False

def is_valid_ip_address(host):
    """éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆçš„ IPv4 æˆ– IPv6 åœ°å€"""
    try:
        ipaddress.ip_address(host)
        return True
    except ValueError:
        try:
            if host.startswith('[') and host.endswith(']'):
                ipaddress.ip_address(host[1:-1])
                return True
            return False
        except ValueError:
            return False

def get_url_list_from_remote(url_source):
    """ä»ç»™å®šçš„å…¬å¼€ç½‘å€è·å– URL åˆ—è¡¨"""
    try:
        session = requests.Session()
        retries = Retry(total=3, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])
        session.mount('http://', HTTPAdapter(max_retries=retries))
        session.mount('https://', HTTPAdapter(max_retries=retries))
        response = session.get(url_source, headers=headers, timeout=10)
        response.raise_for_status()
        text_content = response.text.strip()
        raw_urls = [line.strip() for line in text_content.splitlines() if line.strip()]
        logging.info(f"ä» {url_source} è·å–åˆ° {len(raw_urls)} ä¸ªURL")
        return raw_urls
    except Exception as e:
        logging.error(f"è·å–URLåˆ—è¡¨å¤±è´¥: {url_source} - {e}")
        return []

def parse_content_to_nodes(content):
    """ä»æ–‡æœ¬å†…å®¹ä¸­è§£æå‡ºèŠ‚ç‚¹"""
    if not content:
        return []

    found_nodes = []
    processed_content = content

    # 1. å°è¯• Base64 è§£ç 
    try:
        decoded_bytes = base64.b64decode(content)
        processed_content = decoded_bytes.decode('utf-8')
        logging.info("å†…å®¹æˆåŠŸ Base64 è§£ç ã€‚")
    except Exception:
        pass

    # 2. å°è¯• YAML è§£æ
    try:
        parsed_data = yaml.safe_load(processed_content)
        if isinstance(parsed_data, dict) and 'proxies' in parsed_data and isinstance(parsed_data['proxies'], list):
            for proxy_entry in parsed_data['proxies']:
                if isinstance(proxy_entry, dict):
                    if 'name' in proxy_entry and not isinstance(proxy_entry['name'], str):
                        proxy_entry['name'] = str(proxy_entry['name'])
                    found_nodes.append(proxy_entry)
                elif isinstance(proxy_entry, str) and any(proxy_entry.startswith(p + '://') for p in ["vmess", "trojan", "ss", "ssr", "vless", "hy", "hy2", "hysteria", "hysteria2"]):
                    found_nodes.append(proxy_entry.strip())
            logging.info("å†…å®¹æˆåŠŸè§£æä¸º Clash YAMLã€‚")
        elif isinstance(parsed_data, list):
            for item in parsed_data:
                if isinstance(item, str):
                    found_nodes.append(item.strip())
                elif isinstance(item, dict):
                    if 'name' in item and not isinstance(item['name'], str):
                        item['name'] = str(item['name'])
                    found_nodes.append(item)
            logging.info("å†…å®¹æˆåŠŸè§£æä¸º YAML åˆ—è¡¨ã€‚")
    except yaml.YAMLError:
        pass
    except Exception as e:
        logging.error(f"YAML è§£æå¤±è´¥: {e}")
        pass

    # 3. é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼æå–èŠ‚ç‚¹
    matches = node_pattern.findall(content)
    for match in matches:
        found_nodes.append(match.strip())
    
    # å¯¹è§£ç åçš„å†…å®¹å†æ¬¡è¿›è¡Œæ­£åˆ™åŒ¹é…ï¼Œé˜²æ­¢æœ‰äº›è®¢é˜…æ˜¯åµŒå¥—çš„Base64ç¼–ç 
    if content != processed_content:
        matches_decoded = node_pattern.findall(processed_content)
        for match in matches_decoded:
            found_nodes.append(match.strip())

    return found_nodes

def fetch_and_parse_url(url):
    """è·å–URLå†…å®¹å¹¶è§£æå‡ºèŠ‚ç‚¹"""
    session = requests.Session()
    retries = Retry(total=2, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])
    session.mount('http://', HTTPAdapter(max_retries=retries))
    session.mount('https://', HTTPAdapter(max_retries=retries))

    try:
        logging.debug(f"å¼€å§‹è¯·æ±‚ URL: {url}")
        resp = session.get(url, headers=headers, timeout=TIMEOUT)
        resp.raise_for_status()
        content = resp.text.strip()
        
        if len(content) < 10:
            logging.warning(f"è·å–åˆ°å†…å®¹è¿‡çŸ­ï¼Œå¯èƒ½æ— æ•ˆ: {url}")
            return [], False, "å†…å®¹è¿‡çŸ­", resp.status_code
        
        nodes = parse_content_to_nodes(content)
        logging.debug(f"URL {url} è§£æåˆ° {len(nodes)} ä¸ªèŠ‚ç‚¹")
        return nodes, True, None, resp.status_code
    except requests.exceptions.Timeout:
        logging.error(f"è¯·æ±‚è¶…æ—¶: {url}")
        return [], False, "è¯·æ±‚è¶…æ—¶", None
    except requests.exceptions.ConnectionError as e:
        logging.error(f"è¿æ¥å¤±è´¥: {url} - {e}")
        return [], False, f"è¿æ¥å¤±è´¥: {e}", None
    except requests.exceptions.HTTPError as e:
        logging.error(f"HTTPé”™è¯¯: {url} - {e}")
        return [], False, f"HTTPé”™è¯¯: {e}", None
    except requests.exceptions.RequestException as e:
        logging.error(f"è¯·æ±‚å¤±è´¥: {url} - {e}")
        return [], False, f"è¯·æ±‚å¤±è´¥: {e}", None
    except Exception as e:
        logging.error(f"å¤„ç†URLå¼‚å¸¸: {url} - {e}")
        return [], False, f"æœªçŸ¥å¼‚å¸¸: {e}", None

def write_statistics_to_csv(statistics_data, filename):
    """å°†ç»Ÿè®¡æ•°æ®å†™å…¥CSVæ–‡ä»¶"""
    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
        fieldnames = ['URL', 'èŠ‚ç‚¹æ•°é‡', 'çŠ¶æ€', 'é”™è¯¯ä¿¡æ¯', 'çŠ¶æ€ç ']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        for row in statistics_data:
            writer.writerow(row)
    print(f"ç»Ÿè®¡æ•°æ®å·²ä¿å­˜è‡³ï¼š{filename}")

def write_urls_to_file(urls, filename):
    """å°†URLåˆ—è¡¨å†™å…¥æ–‡ä»¶"""
    with open(filename, 'w', encoding='utf-8') as f:
        for url in urls:
            f.write(url + '\n')
    print(f"URLåˆ—è¡¨å·²ä¿å­˜è‡³ï¼š{filename}")

def clean_node_name(name, index=None):
    """æ¸…ç†èŠ‚ç‚¹åç§°ï¼Œå¢å¼ºåœ°åŒºå’Œæä¾›å•†è§£æ"""
    if not isinstance(name, str):
        name = str(name)

    cleaned_name = name.strip()

    # åˆ é™¤æ— å…³ä¿¡æ¯
    cleaned_name = re.sub(r'ã€[^ã€‘]*?(æµé‡|åˆ°æœŸ|è¿‡æœŸ|å……å€¼|ç»­è´¹)[^ã€‘]*ã€‘', '', cleaned_name)
    cleaned_name = re.sub(r'\[[^]]*?(æµé‡|åˆ°æœŸ|è¿‡æœŸ|å……å€¼|ç»­è´¹)[^\]]*\]', '', cleaned_name)
    cleaned_name = re.sub(r'\([^)]*?(æµé‡|åˆ°æœŸ|è¿‡æœŸ|å……å€¼|ç»­è´¹)[^)]*\)', '', cleaned_name)
    cleaned_name = re.sub(r'ï¼ˆ[^ï¼‰]*?(æµé‡|åˆ°æœŸ|è¿‡æœŸ|å……å€¼|ç»­è´¹)[^ï¼‰]*ï¼‰', '', cleaned_name)

    redundant_keywords = [
        r'\d+%', r'\d{4}-\d{2}-\d{2}', r'\d{2}-\d{2}', r'x\d+',
        r'ç§’æ€', r'æ´»åŠ¨', r'æ–°å¹´', r'ç¦åˆ©', r'VIP\d*', r'Pro', r'Lite', r'Plus',
        r'è‡ªåŠ¨', r'æ‰‹åŠ¨', r'è‡ªé€‰', r'(\d+\.\d+kbps)', r'(\d+\.\d+mbps)', r'(\d+kbps)', r'(\d+mbps)',
        r'\\n', r'\\r', r'\d+\.\d+G|\d+G',
    ]
    for keyword in redundant_keywords:
        cleaned_name = re.sub(keyword, ' ', cleaned_name, flags=re.IGNORECASE).strip()

    cleaned_name = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\s\.\-_@#|]', ' ', cleaned_name)
    cleaned_name = re.sub(r'\s+', ' ', cleaned_name).strip()

    # åœ°åŒºå’Œæä¾›å•†æ˜ å°„
    region_map = {
        'é¦™æ¸¯': 'HK', 'å°æ¹¾': 'TW', 'æ—¥æœ¬': 'JP', 'æ–°åŠ å¡': 'SG', 'ç¾å›½': 'US', 'è‹±å›½': 'UK',
        'å¾·å›½': 'DE', 'éŸ©å›½': 'KR', 'é©¬æ¥è¥¿äºš': 'MY', 'æ³°å›½': 'TH', 'è²å¾‹å®¾': 'PH', 'è¶Šå—': 'VN',
        'å°å°¼': 'ID', 'å°åº¦': 'IN', 'æ¾³å¤§åˆ©äºš': 'AU', 'åŠ æ‹¿å¤§': 'CA', 'ä¿„ç½—æ–¯': 'RU', 'å·´è¥¿': 'BR',
        'æ„å¤§åˆ©': 'IT', 'è·å…°': 'NL', 'ä¸­å›½': 'CN', 'é˜¿è”é…‹': 'AE', 'å®‰é“å°”': 'AD', 'å“ˆè¨å…‹æ–¯å¦': 'KZ'
    }
    provider_map = {
        'Amazon': 'AWS', 'Oracle': 'Oracle', 'Alibaba': 'Alibaba', 'Google': 'Google',
        'Tencent': 'Tencent', 'Vultr': 'Vultr', 'OVH': 'OVH', 'DigitalOcean': 'DO',
        'Akamai': 'Akamai', 'Core Labs': 'CoreLabs', 'Cloudflare': 'CF'
    }

    # æå–åœ°åŒºå’Œæä¾›å•†
    region = None
    provider = None
    region_match = region_pattern.search(cleaned_name)
    provider_match = provider_pattern.search(cleaned_name)
    if region_match:
        region = region_match.group(0).upper()
    if provider_match:
        provider = provider_match.group(0).title()
        for full_name, short_name in provider_map.items():
            if full_name.lower() in provider.lower():
                provider = short_name
                break

    # ä¿ç•™å…³é”®ä¿¡æ¯
    meaningful_keywords = ['IPLC', 'IEPL', 'ä¸“çº¿', 'ä¸­è½¬', 'ç›´è¿']
    preserved_info = [kw for kw in meaningful_keywords if kw.lower() in cleaned_name.lower()]
    
    node_number_match = re.search(r'(?<!\d)\d{1,2}(?!\d)|Node\d{1,2}', cleaned_name, re.IGNORECASE)
    if node_number_match:
        preserved_info.append(node_number_match.group(0))

    # æ„å»ºåç§°
    parts = []
    if region:
        parts.append(region)
    if provider:
        parts.append(provider)
    if preserved_info:
        parts.append('_'.join(preserved_info))
    if not parts:
        parts.append('Node')
    if index is not None:
        parts.append(f"{index:02d}")
    
    cleaned_name = '-'.join(parts)
    
    if len(cleaned_name) > 80:
        cleaned_name = cleaned_name[:80].rstrip() + '...'

    return cleaned_name if cleaned_name else f"Node-{index:02d}" if index is not None else "Unknown Node"

def _generate_node_fingerprint(node):
    """ä¸ºèŠ‚ç‚¹ç”Ÿæˆå”¯ä¸€æŒ‡çº¹ï¼ŒåŠ å…¥æ›´å¤šåè®®ç»†èŠ‚å’ŒDNSè§£æåçš„IPä»¥å®ç°æ›´å½»åº•å»é‡"""
    def normalize_value(value):
        return '' if value is None else str(value).lower().strip()

    if isinstance(node, dict):
        # åŸºç¡€æŒ‡çº¹æ•°æ®
        fingerprint_data = {
            'type': normalize_value(node.get('type')),
            'port': normalize_value(node.get('port')),
        }

        server = normalize_value(node.get('server'))
        resolved_ip = None
        if server and not is_valid_ip_address(server): # å¦‚æœæ˜¯åŸŸåï¼Œå°è¯•è§£æ
            resolved_ip = resolve_hostname_cached(server)
            if resolved_ip:
                fingerprint_data['server'] = normalize_value(resolved_ip) # ä½¿ç”¨è§£æIP
            else:
                fingerprint_data['server'] = server # è§£æå¤±è´¥ï¼Œé€€å›ä½¿ç”¨åŸŸå
        else:
            fingerprint_data['server'] = server # æ˜¯IPåœ°å€ï¼Œç›´æ¥ä½¿ç”¨

        # æ·»åŠ åè®®ç‰¹æœ‰å­—æ®µ
        node_type = node.get('type', '').lower()
        if node_type == 'vmess':
            fingerprint_data.update({
                'uuid': normalize_value(node.get('uuid')),
                'alterId': normalize_value(node.get('alterId')),
                'network': normalize_value(node.get('network')),
                'tls': normalize_value(node.get('tls')),
                'servername': normalize_value(node.get('servername')),
                'ws-path': normalize_value(node.get('ws-opts', {}).get('path')),
                'ws-headers-host': normalize_value(node.get('ws-opts', {}).get('headers', {}).get('Host')),
                'grpc-serviceName': normalize_value(node.get('grpc-opts', {}).get('serviceName')),
                'fingerprint': normalize_value(node.get('fingerprint')), # VMessæŒ‡çº¹ (ç”¨äºXTLS/Reality)
                'reality-pbk': normalize_value(node.get('reality-pbk')), # Realityå…¬é’¥
                'version': normalize_value(node.get('version')), # åè®®ç‰ˆæœ¬
            })
        elif node_type == 'trojan':
            fingerprint_data.update({
                'password': normalize_value(node.get('password')),
                'network': normalize_value(node.get('network')),
                'tls': normalize_value(node.get('tls')),
                'sni': normalize_value(node.get('sni')),
                'flow': normalize_value(node.get('flow')), # å¯¹äºXTLSå¾ˆé‡è¦
            })
        elif node_type == 'ss':
            fingerprint_data.update({
                'cipher': normalize_value(node.get('cipher')),
                'password': normalize_value(node.get('password')),
                'plugin': normalize_value(node.get('plugin')),
                'plugin-opts': normalize_value(node.get('plugin-opts')),
            })
        elif node_type == 'vless':
            fingerprint_data.update({
                'uuid': normalize_value(node.get('uuid')),
                'network': normalize_value(node.get('network')),
                'tls': normalize_value(node.get('tls')),
                'servername': normalize_value(node.get('servername')),
                'flow': normalize_value(node.get('flow')),
                'ws-path': normalize_value(node.get('ws-opts', {}).get('path')),
                'ws-headers-host': normalize_value(node.get('ws-opts', {}).get('headers', {}).get('Host')),
                'grpc-serviceName': normalize_value(node.get('grpc-opts', {}).get('serviceName')),
            })
        elif node_type in ['hysteria', 'hy']:
            fingerprint_data.update({
                'auth_str': normalize_value(node.get('auth_str')),
                'alpn': ','.join(sorted([normalize_value(a) for a in node.get('alpn', [])])), # ALPNåˆ—è¡¨æ’åºåæ‹¼æ¥
                'network': normalize_value(node.get('network')),
                'sni': normalize_value(node.get('sni')),
                'up_mbps': normalize_value(node.get('up_mbps')),
                'down_mbps': normalize_value(node.get('down_mbps')),
                'obfs': normalize_value(node.get('obfs')),
                'obfs-password': normalize_value(node.get('obfs-password')),
            })
        elif node_type in ['hysteria2', 'hy2']:
            fingerprint_data.update({
                'password': normalize_value(node.get('password')),
                'obfs': normalize_value(node.get('obfs')),
                'obfs-password': normalize_value(node.get('obfs-password')),
                'sni': normalize_value(node.get('sni')),
                'alpn': ','.join(sorted([normalize_value(a) for a in node.get('alpn', [])])),
            })

        stable_json = json.dumps(fingerprint_data, sort_keys=True, ensure_ascii=False)
        return hashlib.sha256(stable_json.encode('utf-8')).hexdigest()

    elif isinstance(node, str):
        # å¯¹äºURLå­—ç¬¦ä¸²ï¼Œåœ¨è§£ææˆClashå­—å…¸å‰ï¼Œä¹Ÿå¯ä»¥å°è¯•åšä¸€äº›åŸºç¡€çš„IPå»é‡
        try:
            parsed_url = urlparse(node)
            scheme = parsed_url.scheme.lower()
            netloc = parsed_url.netloc.lower()
            host = netloc.split(':')[0] if ':' in netloc else netloc
            port = netloc.split(':')[1] if ':' in netloc else ''

            resolved_ip = None
            if host and not is_valid_ip_address(host):
                resolved_ip = resolve_hostname_cached(host)
                if resolved_ip:
                    host = resolved_ip # ä½¿ç”¨è§£æIPè¿›è¡ŒæŒ‡çº¹è®¡ç®—
            
            # è¿™é‡Œä»…åšåŸºç¡€çš„ URL å­—ç¬¦ä¸²æŒ‡çº¹ï¼Œå› ä¸ºåç»­ä¼šå°è¯•è½¬æ¢ä¸º Clash å­—å…¸
            fingerprint_parts = [scheme, host, port]
            fingerprint = hashlib.sha256(''.join(fingerprint_parts).encode('utf-8')).hexdigest()
            return fingerprint
        except Exception:
            return None

    return None

def deduplicate_and_standardize_nodes(raw_nodes_list):
    """å»é‡é€»è¾‘ï¼Œè¾“å‡ºæ‰€æœ‰æŒ‡çº¹å”¯ä¸€çš„èŠ‚ç‚¹ï¼Œä¸å†æŒ‰ç±»åˆ«éšæœºé€‰æ‹©ï¼Œä»¥æœ€å¤§åŒ–å»é‡æ•ˆæœ"""
    unique_node_fingerprints = set()
    final_clash_proxies = []

    logging.info(f"å»é‡å‰èŠ‚ç‚¹æ•°: {len(raw_nodes_list)}")

    for idx, node in enumerate(raw_nodes_list):
        clash_proxy_dict = None
        node_raw_name = ""

        # å°† URL å­—ç¬¦ä¸²è½¬æ¢ä¸º Clash ä»£ç†å­—å…¸ï¼Œå¹¶å°½å¯èƒ½å¡«å……æ‰€æœ‰å­—æ®µ
        if isinstance(node, dict):
            clash_proxy_dict = node
            node_raw_name = str(node.get('name', ''))
        elif isinstance(node, str):
            try:
                parsed_url = urlparse(node)
                node_raw_name = str(parsed_url.fragment) # å°è¯•ä»fragmentè·å–åç§°
                
                # æ£€æŸ¥åè®®ç±»å‹ï¼Œå¹¶è¿›è¡Œè¯¦ç»†è§£æ
                if node.startswith("vmess://"):
                    decoded_bytes = base64.b64decode(node[len("vmess://"):].encode('utf-8'))
                    config = json.loads(decoded_bytes.decode('utf-8'))
                    clash_proxy_dict = {
                        'name': str(config.get('ps', 'VMess Node')),
                        'type': 'vmess',
                        'server': config.get('add'),
                        'port': int(config.get('port')),
                        'uuid': config.get('id'),
                        'alterId': int(config.get('aid', 0)),
                        'cipher': 'auto',
                        'network': config.get('net'),
                        'tls': True if config.get('tls') == 'tls' else False,
                        'skip-cert-verify': True if config.get('scy') == 'true' else False,
                        'servername': config.get('sni') or config.get('host'),
                        'ws-opts': {'path': config.get('path', '/'), 'headers': {'Host': config.get('host')}} if config.get('net') == 'ws' else None,
                        'grpc-opts': {'serviceName': config.get('path', '')} if config.get('net') == 'grpc' else None,
                        'fingerprint': config.get('fp'),
                        'reality-pbk': config.get('pbk'),
                        'version': config.get('v'),
                    }
                    if clash_proxy_dict.get('ws-opts') == {'path': '/', 'headers': {'Host': ''}}:
                        clash_proxy_dict['ws-opts'] = None
                    if clash_proxy_dict.get('grpc-opts') == {'serviceName': ''}:
                        clash_proxy_dict['grpc-opts'] = None
                elif node.startswith("trojan://"):
                    parsed = urlparse(node)
                    password = parsed.username
                    server = parsed.hostname
                    port = parsed.port
                    query = parse_qs(parsed.query)
                    clash_proxy_dict = {
                        'name': str(parsed.fragment or 'Trojan Node'),
                        'type': 'trojan',
                        'server': server,
                        'port': port,
                        'password': password,
                        'network': query.get('type', ['tcp'])[0],
                        'tls': True,
                        'skip-cert-verify': query.get('allowInsecure', ['0'])[0] == '1',
                        'sni': query.get('sni', [server])[0],
                        'flow': query.get('flow', [''])[0],
                    }
                elif node.startswith("ss://"):
                    decoded_part = node[len("ss://"):].split('#', 1)[0]
                    try:
                        decoded_info = base64.b64decode(decoded_part.encode('utf-8')).decode('utf-8')
                        parts = decoded_info.split('@', 1)
                        method_password = parts[0].split(':', 1)
                        method = method_password[0]
                        password = method_password[1] if len(method_password) > 1 else ''
                        server_port = parts[1].split(':', 1)
                        server = server_port[0]
                        port = int(server_port[1])
                        fragment = urlparse(node).fragment
                        query_params = parse_qs(urlparse(node).query) # ä»urlè·å–queryå‚æ•°
                        
                        clash_proxy_dict = {
                            'name': str(fragment or 'SS Node'),
                            'type': 'ss',
                            'server': server,
                            'port': port,
                            'cipher': method,
                            'password': password,
                            'plugin': query_params.get('plugin', [''])[0],
                            'plugin-opts': query_params.get('obfs', [''])[0], # SS plugin-opts é€šå¸¸åœ¨obfså­—æ®µ
                        }
                    except Exception:
                        clash_proxy_dict = None
                elif node.startswith("vless://"):
                    parsed = urlparse(node)
                    uuid = parsed.username
                    server = parsed.hostname
                    port = parsed.port
                    query = parse_qs(parsed.query)
                    clash_proxy_dict = {
                        'name': str(parsed.fragment or 'VLESS Node'),
                        'type': 'vless',
                        'server': server,
                        'port': port,
                        'uuid': uuid,
                        'network': query.get('type', ['tcp'])[0],
                        'tls': True if query.get('security', [''])[0] == 'tls' else False,
                        'skip-cert-verify': query.get('flow', [''])[0] == 'xtls-rprx-direct', # Clash vless flow is tied to skip-cert-verify
                        'servername': query.get('sni', [server])[0],
                        'flow': query.get('flow', [''])[0],
                        'ws-opts': {'path': query.get('path', ['/'])[0], 'headers': {'Host': query.get('host', [''])[0]}} if query.get('type', [''])[0] == 'ws' else None,
                        'grpc-opts': {'serviceName': query.get('serviceName', [''])[0]} if query.get('type', [''])[0] == 'grpc' else None,
                    }
                    if clash_proxy_dict.get('ws-opts') == {'path': '/', 'headers': {'Host': ''}}:
                        clash_proxy_dict['ws-opts'] = None
                    if clash_proxy_dict.get('grpc-opts') == {'serviceName': ''}:
                        clash_proxy_dict['grpc-opts'] = None
                elif node.startswith("hysteria://") or node.startswith("hy://"):
                    parsed = urlparse(node)
                    server = parsed.hostname
                    port = parsed.port
                    query = parse_qs(parsed.query)
                    clash_proxy_dict = {
                        'name': str(parsed.fragment or 'Hysteria Node'),
                        'type': 'hysteria',
                        'server': server,
                        'port': port,
                        'auth_str': query.get('auth', [''])[0],
                        'alpn': query.get('alpn', [''])[0].split(','),
                        'network': query.get('protocol', ['udp'])[0],
                        'skip-cert-verify': query.get('insecure', ['0'])[0] == '1',
                        'sni': query.get('peer', [server])[0],
                        'up_mbps': int(query.get('upmbps', ['0'])[0]),
                        'down_mbps': int(query.get('downmbps', ['0'])[0]),
                        'obfs': query.get('obfs', [''])[0],
                        'obfs-password': query.get('obfsParam', [''])[0],
                    }
                    if not clash_proxy_dict['alpn'] or clash_proxy_dict['alpn'] == ['']:
                        clash_proxy_dict.pop('alpn')
                    if not clash_proxy_dict['obfs']:
                        clash_proxy_dict.pop('obfs')
                    if not clash_proxy_dict['obfs-password']:
                        clash_proxy_dict.pop('obfs-password')
                elif node.startswith("hysteria2://") or node.startswith("hy2://"):
                    parsed = urlparse(node)
                    password = parsed.username
                    server = parsed.hostname
                    port = parsed.port
                    query = parse_qs(parsed.query)
                    clash_proxy_dict = {
                        'name': str(parsed.fragment or 'Hysteria2 Node'),
                        'type': 'hysteria2',
                        'server': server,
                        'port': port,
                        'password': password,
                        'obfs': query.get('obfs', [''])[0],
                        'obfs-password': query.get('obfs-password', [''])[0],
                        'tls': True,
                        'skip-cert-verify': query.get('insecure', ['0'])[0] == '1',
                        'sni': query.get('sni', [server])[0],
                        'alpn': query.get('alpn', [''])[0].split(',')
                    }
                    if not clash_proxy_dict['obfs']:
                        clash_proxy_dict.pop('obfs')
                    if not clash_proxy_dict['obfs-password']:
                        clash_proxy_dict.pop('obfs-password')
                    if not clash_proxy_dict['alpn'] or clash_proxy_dict['alpn'] == ['']:
                        clash_proxy_dict.pop('alpn')
                else:
                    # å¦‚æœä¸æ˜¯å·²çŸ¥çš„åè®®URLï¼Œè·³è¿‡
                    continue
            except Exception as e:
                logging.warning(f"URLèŠ‚ç‚¹è½¬æ¢ä¸ºClashå­—å…¸å¤±è´¥: {node[:50]}... - {e}")
                clash_proxy_dict = None

        if clash_proxy_dict:
            # 1. è¿‡æ»¤æ‰åç§°ä¸­åŒ…å«åˆ é™¤å…³é”®è¯çš„èŠ‚ç‚¹
            if any(keyword.lower() in node_raw_name.lower() for keyword in DELETE_KEYWORDS):
                continue
            
            # 2. éªŒè¯æœåŠ¡å™¨åœ°å€æ˜¯å¦åˆæ³•ï¼ˆIPæˆ–åŸŸåæ ¼å¼ï¼‰
            server_host = clash_proxy_dict.get('server', '')
            if server_host and not (is_valid_ip_address(server_host) or re.match(r'^[a-zA-Z0-9\-\.]+$', server_host)):
                continue

            # 3. ç”Ÿæˆæ›´ç²¾ç¡®çš„èŠ‚ç‚¹æŒ‡çº¹
            fingerprint = _generate_node_fingerprint(clash_proxy_dict)
            
            # 4. å¦‚æœæŒ‡çº¹æ˜¯å”¯ä¸€çš„ï¼Œåˆ™æ·»åŠ åˆ°æœ€ç»ˆåˆ—è¡¨
            if fingerprint and fingerprint not in unique_node_fingerprints:
                unique_node_fingerprints.add(fingerprint)
                # æ¸…ç†å’Œæ ‡å‡†åŒ–èŠ‚ç‚¹åç§°
                clash_proxy_dict['name'] = clean_node_name(node_raw_name, len(final_clash_proxies) + 1)
                final_clash_proxies.append(clash_proxy_dict) # ç›´æ¥æ·»åŠ æŒ‡çº¹å”¯ä¸€çš„èŠ‚ç‚¹

    logging.info(f"å»é‡åèŠ‚ç‚¹æ•°: {len(final_clash_proxies)}")
    return final_clash_proxies

# ä¸»ç¨‹åºæµç¨‹
URL_SOURCE = os.environ.get("URL_SOURCE")
print(f"è°ƒè¯•ä¿¡æ¯ - è¯»å–åˆ°çš„ URL_SOURCE å€¼: {URL_SOURCE}")

if not URL_SOURCE:
    print("é”™è¯¯ï¼šç¯å¢ƒå˜é‡ 'URL_SOURCE' æœªè®¾ç½®ã€‚æ— æ³•è·å–è®¢é˜…é“¾æ¥ã€‚")
    logging.error("æœªè®¾ç½® URL_SOURCE ç¯å¢ƒå˜é‡")
    exit(1)

os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)
os.makedirs(os.path.dirname(STATISTICS_FILE), exist_ok=True)

raw_urls_from_source = get_url_list_from_remote(URL_SOURCE)

urls_to_fetch = set()
url_statistics = []
successful_urls = []
failed_urls = []
all_parsed_nodes_raw = []

print("\n--- é¢„å¤„ç†åŸå§‹URL/å­—ç¬¦ä¸²åˆ—è¡¨ ---")
for entry in raw_urls_from_source:
    if is_valid_url(entry):
        urls_to_fetch.add(entry)
    else:
        print(f"å‘ç°éHTTP/HTTPSæ¡ç›®ï¼Œå°è¯•ç›´æ¥è§£æ: {entry[:80]}...")
        parsed_nodes = parse_content_to_nodes(entry)
        if parsed_nodes:
            all_parsed_nodes_raw.extend(parsed_nodes)
            stat_entry = {
                'URL': entry,
                'èŠ‚ç‚¹æ•°é‡': len(parsed_nodes),
                'çŠ¶æ€': 'æˆåŠŸ',
                'é”™è¯¯ä¿¡æ¯': 'ç›´æ¥è§£ææˆåŠŸ',
                'çŠ¶æ€ç ': None
            }
            url_statistics.append(stat_entry)
            successful_urls.append(entry)
        else:
            stat_entry = {
                'URL': entry,
                'èŠ‚ç‚¹æ•°é‡': 0,
                'çŠ¶æ€': 'å¤±è´¥',
                'é”™è¯¯ä¿¡æ¯': 'éURLä¸”æ— æ³•è§£æä¸ºèŠ‚ç‚¹',
                'çŠ¶æ€ç ': None
            }
            url_statistics.append(stat_entry)
            failed_urls.append(entry)

print("\n--- é˜¶æ®µä¸€ï¼šè·å–å¹¶åˆå¹¶æ‰€æœ‰è®¢é˜…é“¾æ¥ä¸­çš„èŠ‚ç‚¹ ---")
total_urls_to_process_via_http = len(urls_to_fetch)

if total_urls_to_process_via_http > 0:
    with ThreadPoolExecutor(max_workers=16) as executor:
        future_to_url = {executor.submit(fetch_and_parse_url, url): url for url in urls_to_fetch}
        for future in tqdm(as_completed(future_to_url), total=total_urls_to_process_via_http, desc="é€šè¿‡HTTP/HTTPSè¯·æ±‚å¹¶è§£æèŠ‚ç‚¹", mininterval=1.0):
            url = future_to_url[future]
            nodes, success, error_message, status_code = future.result()
            stat_entry = {
                'URL': url,
                'èŠ‚ç‚¹æ•°é‡': len(nodes),
                'çŠ¶æ€': 'æˆåŠŸ' if success else 'å¤±è´¥',
                'é”™è¯¯ä¿¡æ¯': error_message if error_message else '',
                'çŠ¶æ€ç ': status_code
            }
            url_statistics.append(stat_entry)
            if success:
                successful_urls.append(url)
                all_parsed_nodes_raw.extend(nodes)
                print(f"æˆåŠŸå¤„ç† URL: {url}, èŠ‚ç‚¹æ•°: {len(nodes)}, çŠ¶æ€ç : {status_code}")
            else:
                failed_urls.append(url)
                print(f"å¤±è´¥ URL: {url}, é”™è¯¯: {error_message}")

# æ ¸å¿ƒå˜åŒ–ï¼šè°ƒç”¨å»é‡å’Œæ ‡å‡†åŒ–å‡½æ•°ï¼Œç°åœ¨å®ƒä¼šè¿”å›æ‰€æœ‰æŒ‡çº¹ç‹¬ç‰¹çš„èŠ‚ç‚¹
final_clash_proxies = deduplicate_and_standardize_nodes(all_parsed_nodes_raw)

with open(TEMP_MERGED_NODES_RAW_FILE, 'w', encoding='utf-8') as temp_file:
    for node in final_clash_proxies:
        if isinstance(node, dict):
            temp_file.write(json.dumps(node, ensure_ascii=False) + '\n')
        else:
            temp_file.write(node.strip() + '\n')

print(f"\né˜¶æ®µä¸€å®Œæˆã€‚åˆå¹¶åˆ° {len(final_clash_proxies)} ä¸ªå”¯ä¸€Clashä»£ç†å­—å…¸ï¼Œå·²ä¿å­˜è‡³ {TEMP_MERGED_NODES_RAW_FILE}")

write_statistics_to_csv(url_statistics, STATISTICS_FILE)
write_urls_to_file(successful_urls, SUCCESS_URLS_FILE)
write_urls_to_file(failed_urls, FAILED_URLS_FILE)

print("\n--- é˜¶æ®µäºŒï¼šè¾“å‡ºæœ€ç»ˆ Clash YAML é…ç½® ---")
if not OUTPUT_FILE.endswith(('.yaml', '.yml')):
    OUTPUT_FILE = os.path.splitext(OUTPUT_FILE)[0] + '.yaml'

proxies_to_output = final_clash_proxies

proxy_names_in_group = []
for node in proxies_to_output:
    if isinstance(node, dict) and 'name' in node:
        proxy_names_in_group.append(node['name'])
    # else: # è¿™ä¸€è¡Œé€šå¸¸ä¸ä¼šè¢«è§¦å‘ï¼Œå› ä¸º deduplicate_and_standardize_nodes æ€»æ˜¯è¿”å›å­—å…¸
    #     proxy_names_in_group.append(f"{node.get('type', 'Unknown')} {node.get('server', '')}")

clash_config = {
    'proxies': proxies_to_output,
    'proxy-groups': [
        {
            'name': 'ğŸš€ èŠ‚ç‚¹é€‰æ‹©',
            'type': 'select',
            'proxies': ['DIRECT'] + proxy_names_in_group
        },
        {
            'name': 'â™»ï¸ è‡ªåŠ¨é€‰æ‹©',
            'type': 'url-test',
            'url': 'http://www.gstatic.com/generate_204',
            'interval': 300,
            'proxies': proxy_names_in_group
        }
    ],
    'rules': [
        'MATCH,ğŸš€ èŠ‚ç‚¹é€‰æ‹©'
    ]
}

success_count = len(proxies_to_output)

try:
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as out_file:
        yaml.dump(clash_config, out_file, allow_unicode=True, default_flow_style=False, sort_keys=False)
    print(f"æœ€ç»ˆ Clash YAML é…ç½®å·²ä¿å­˜è‡³ï¼š{OUTPUT_FILE}")
except Exception as e:
    logging.error(f"å†™å…¥æœ€ç»ˆ Clash YAML æ–‡ä»¶å¤±è´¥: {e}")
    print(f"é”™è¯¯ï¼šå†™å…¥æœ€ç»ˆ Clash YAML æ–‡ä»¶å¤±è´¥: {e}")

if os.path.exists(TEMP_MERGED_NODES_RAW_FILE):
    os.remove(TEMP_MERGED_NODES_RAW_FILE)
    print(f"å·²åˆ é™¤ä¸´æ—¶æ–‡ä»¶ï¼š{TEMP_MERGED_NODES_RAW_FILE}")

print("\n" + "=" * 50)
print("æœ€ç»ˆç»“æœï¼š")
print(f"åŸå§‹æ¥æºæ€»æ¡ç›®æ•°ï¼š{len(raw_urls_from_source)}")
print(f"å…¶ä¸­éœ€è¦HTTP/HTTPSè¯·æ±‚çš„è®¢é˜…é“¾æ¥æ•°ï¼š{len(urls_to_fetch)}")
print(f"å…¶ä¸­ç›´æ¥è§£æçš„éURLå­—ç¬¦ä¸²æ•°ï¼š{len(raw_urls_from_source) - len(urls_to_fetch)}")
print(f"æˆåŠŸå¤„ç†çš„URL/å­—ç¬¦ä¸²æ€»æ•°ï¼š{len(successful_urls)}")
print(f"å¤±è´¥çš„URL/å­—ç¬¦ä¸²æ€»æ•°ï¼š{len(failed_urls)}")
print(f"åˆæ­¥èšåˆçš„åŸå§‹èŠ‚ç‚¹æ•°ï¼ˆå»é‡å’Œè¿‡æ»¤å‰ï¼‰ï¼š{len(all_parsed_nodes_raw)}")
print(f"å»é‡ã€æ ‡å‡†åŒ–å’Œè¿‡æ»¤åçš„å”¯ä¸€Clashä»£ç†æ•°ï¼š{len(final_clash_proxies)}")
print(f"æœ€ç»ˆè¾“å‡ºåˆ°Clash YAMLæ–‡ä»¶çš„èŠ‚ç‚¹æ•°ï¼š{success_count}")
if len(final_clash_proxies) > 0:
    print(f"æœ€ç»ˆæœ‰æ•ˆå†…å®¹ç‡ï¼ˆç›¸å¯¹äºå»é‡è¿‡æ»¤åï¼‰ï¼š{success_count/len(final_clash_proxies):.1%}")
print(f"ç»“æœæ–‡ä»¶å·²ä¿å­˜è‡³ï¼š{OUTPUT_FILE}")
print(f"ç»Ÿè®¡æ•°æ®å·²ä¿å­˜è‡³ï¼š{STATISTICS_FILE}")
print(f"æˆåŠŸURLåˆ—è¡¨å·²ä¿å­˜è‡³ï¼š{SUCCESS_URLS_FILE}")
print(f"å¤±è´¥URLåˆ—è¡¨å·²ä¿å­˜è‡³ï¼š{FAILED_URLS_FILE}")
print("=" * 50)
