# -*- coding: utf-8 -*-
import os
import requests
from urllib.parse import urlparse, parse_qs
import base64
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
import argparse
import re
import yaml
import json
import csv
import hashlib
import ipaddress
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from collections import defaultdict
import random

# é…ç½®æ—¥å¿—
logging.basicConfig(
    filename='error.log',
    level=logging.INFO,  # å‡å°‘DEBUGæ—¥å¿—ï¼Œæé«˜æ€§èƒ½
    format='%(asctime)s - %(levelname)s - %(message)s'
)

# è¯·æ±‚å¤´
headers = {
    'User-Agent': (
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
        'AppleWebKit/537.36 (KHTML, like Gecko) '
        'Chrome/91.0.4472.124 Safari/537.36'
    ),
    'Accept-Encoding': 'gzip, deflate'
}

# å‘½ä»¤è¡Œå‚æ•°è§£æ
parser = argparse.ArgumentParser(description="URLå†…å®¹è·å–è„šæœ¬ï¼Œæ”¯æŒå¤šä¸ªURLæ¥æºå’ŒèŠ‚ç‚¹è§£æ")
parser.add_argument('--timeout', type=int, default=30, help="è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰")
parser.add_argument('--output', type=str, default='data/all_clash.yaml', help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
args = parser.parse_args()

# å…¨å±€å˜é‡
TIMEOUT = args.timeout
OUTPUT_FILE = args.output
TEMP_MERGED_NODES_RAW_FILE = 'temp_merged_nodes_raw.txt'
STATISTICS_FILE = 'data/url_statistics.csv'
SUCCESS_URLS_FILE = 'data/successful_urls.txt'
FAILED_URLS_FILE = 'data/failed_urls.txt'

# å®šä¹‰åˆ é™¤å…³é”®è¯
DELETE_KEYWORDS = [
    'å‰©ä½™æµé‡', 'å¥—é¤åˆ°æœŸ', 'æµé‡', 'åˆ°æœŸ', 'è¿‡æœŸ', 'å…è´¹', 'è¯•ç”¨', 'ä½“éªŒ', 'é™æ—¶', 'é™åˆ¶',
    'å·²ç”¨', 'å¯ç”¨', 'ä¸è¶³', 'åˆ°æœŸæ—¶é—´', 'å€ç‡', 'è¿”åˆ©', 'å……å€¼', 'ç»­è´¹', 'ç”¨é‡', 'è®¢é˜…'
]

# é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼ï¼Œæé«˜æ€§èƒ½
region_pattern = re.compile(r'\b(HK|TW|JP|SG|US|UK|DE|KR|MY|TH|PH|VN|ID|IN|AU|CA|RU|BR|IT|NL|CN|AE|AD|KZ)\b', re.IGNORECASE)
provider_pattern = re.compile(r'\b(AWS|Amazon|Akamai|Oracle|Alibaba|Google|Tencent|Vultr|OVH|DigitalOcean|Core Labs|Cloudflare)\b', re.IGNORECASE)
node_pattern = re.compile(
    r'(vmess://\S+|'
    r'trojan://\S+|'
    r'ss://\S+|'
    r'ssr://\S+|'
    r'vless://\S+|'
    r'hy://\S+|'
    r'hy2://\S+|'
    r'hysteria://\S+|'
    r'hysteria2://\S+)'
)

def is_valid_url(url):
    """éªŒè¯URLæ ¼å¼æ˜¯å¦åˆæ³•ï¼Œä»…æ¥å— http æˆ– https æ–¹æ¡ˆ"""
    try:
        result = urlparse(url)
        return all([result.scheme in ['http', 'https'], result.netloc])
    except Exception:
        return False

def is_valid_ip_address(host):
    """éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆçš„ IPv4 æˆ– IPv6 åœ°å€"""
    try:
        ipaddress.ip_address(host)
        return True
    except ValueError:
        try:
            if host.startswith('[') and host.endswith(']'):
                ipaddress.ip_address(host[1:-1])
                return True
            return False
        except ValueError:
            return False

def get_url_list_from_remote(url_source):
    """ä»ç»™å®šçš„å…¬å¼€ç½‘å€è·å– URL åˆ—è¡¨"""
    try:
        session = requests.Session()
        retries = Retry(total=3, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])
        session.mount('http://', HTTPAdapter(max_retries=retries))
        session.mount('https://', HTTPAdapter(max_retries=retries))
        response = session.get(url_source, headers=headers, timeout=10)
        response.raise_for_status()
        text_content = response.text.strip()
        raw_urls = [line.strip() for line in text_content.splitlines() if line.strip()]
        logging.info(f"ä» {url_source} è·å–åˆ° {len(raw_urls)} ä¸ªURL")
        return raw_urls
    except Exception as e:
        logging.error(f"è·å–URLåˆ—è¡¨å¤±è´¥: {url_source} - {e}")
        return []

def parse_content_to_nodes(content):
    """ä»æ–‡æœ¬å†…å®¹ä¸­è§£æå‡ºèŠ‚ç‚¹"""
    if not content:
        return []

    found_nodes = []
    processed_content = content

    # 1. å°è¯• Base64 è§£ç 
    try:
        decoded_bytes = base64.b64decode(content)
        processed_content = decoded_bytes.decode('utf-8')
        logging.info("å†…å®¹æˆåŠŸ Base64 è§£ç ã€‚")
    except Exception:
        pass

    # 2. å°è¯• YAML è§£æ
    try:
        parsed_data = yaml.safe_load(processed_content)
        if isinstance(parsed_data, dict) and 'proxies' in parsed_data and isinstance(parsed_data['proxies'], list):
            for proxy_entry in parsed_data['proxies']:
                if isinstance(proxy_entry, dict):
                    if 'name' in proxy_entry and not isinstance(proxy_entry['name'], str):
                        proxy_entry['name'] = str(proxy_entry['name'])
                    found_nodes.append(proxy_entry)
                elif isinstance(proxy_entry, str) and any(proxy_entry.startswith(p + '://') for p in ["vmess", "trojan", "ss", "ssr", "vless", "hy", "hy2", "hysteria", "hysteria2"]):
                    found_nodes.append(proxy_entry.strip())
            logging.info("å†…å®¹æˆåŠŸè§£æä¸º Clash YAMLã€‚")
        elif isinstance(parsed_data, list):
            for item in parsed_data:
                if isinstance(item, str):
                    found_nodes.append(item.strip())
                elif isinstance(item, dict):
                    if 'name' in item and not isinstance(item['name'], str):
                        item['name'] = str(item['name'])
                    found_nodes.append(item)
            logging.info("å†…å®¹æˆåŠŸè§£æä¸º YAML åˆ—è¡¨ã€‚")
    except yaml.YAMLError:
        pass
    except Exception as e:
        logging.error(f"YAML è§£æå¤±è´¥: {e}")
        pass

    # 3. é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼æå–èŠ‚ç‚¹
    matches = node_pattern.findall(content)
    for match in matches:
        found_nodes.append(match.strip())
    
    if content != processed_content:
        matches_decoded = node_pattern.findall(processed_content)
        for match in matches_decoded:
            found_nodes.append(match.strip())

    return found_nodes

def fetch_and_parse_url(url):
    """
    è·å–URLå†…å®¹å¹¶è§£æå‡ºèŠ‚ç‚¹ã€‚
    è¿”å› (èŠ‚ç‚¹åˆ—è¡¨, æ˜¯å¦æˆåŠŸ, é”™è¯¯ä¿¡æ¯, çŠ¶æ€ç )
    """
    session = requests.Session()
    retries = Retry(total=2, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])
    session.mount('http://', HTTPAdapter(max_retries=retries))
    session.mount('https://', HTTPAdapter(max_retries=retries))

    try:
        logging.debug(f"å¼€å§‹è¯·æ±‚ URL: {url}")
        resp = session.get(url, headers=headers, timeout=TIMEOUT)
        resp.raise_for_status()
        content = resp.text.strip()
        
        if len(content) < 10:
            logging.warning(f"è·å–åˆ°å†…å®¹è¿‡çŸ­ï¼Œå¯èƒ½æ— æ•ˆ: {url}")
            return [], False, "å†…å®¹è¿‡çŸ­", resp.status_code
        
        nodes = parse_content_to_nodes(content)
        logging.debug(f"URL {url} è§£æåˆ° {len(nodes)} ä¸ªèŠ‚ç‚¹")
        return nodes, True, None, resp.status_code
    except requests.exceptions.Timeout:
        logging.error(f"è¯·æ±‚è¶…æ—¶: {url}")
        return [], False, "è¯·æ±‚è¶…æ—¶", None
    except requests.exceptions.ConnectionError as e:
        logging.error(f"è¿æ¥å¤±è´¥: {url} - {e}")
        return [], False, f"è¿æ¥å¤±è´¥: {e}", None
    except requests.exceptions.HTTPError as e:
        logging.error(f"HTTPé”™è¯¯: {url} - {e}")
        return [], False, f"HTTPé”™è¯¯: {e}", None
    except requests.exceptions.RequestException as e:
        logging.error(f"è¯·æ±‚å¤±è´¥: {url} - {e}")
        return [], False, f"è¯·æ±‚å¤±è´¥: {e}", None
    except Exception as e:
        logging.error(f"å¤„ç†URLå¼‚å¸¸: {url} - {e}")
        return [], False, f"æœªçŸ¥å¼‚å¸¸: {e}", None

def write_statistics_to_csv(statistics_data, filename):
    """å°†ç»Ÿè®¡æ•°æ®å†™å…¥CSVæ–‡ä»¶"""
    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
        fieldnames = ['URL', 'èŠ‚ç‚¹æ•°é‡', 'çŠ¶æ€', 'é”™è¯¯ä¿¡æ¯', 'çŠ¶æ€ç ']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        for row in statistics_data:
            writer.writerow(row)
    print(f"ç»Ÿè®¡æ•°æ®å·²ä¿å­˜è‡³ï¼š{filename}")

def write_urls_to_file(urls, filename):
    """å°†URLåˆ—è¡¨å†™å…¥æ–‡ä»¶"""
    with open(filename, 'w', encoding='utf-8') as f:
        for url in urls:
            f.write(url + '\n')
    print(f"URLåˆ—è¡¨å·²ä¿å­˜è‡³ï¼š{filename}")

def clean_node_name(name, index=None):
    """æ¸…ç†èŠ‚ç‚¹åç§°ï¼Œå¢å¼ºåœ°åŒºå’Œæä¾›å•†è§£æ"""
    if not isinstance(name, str):
        name = str(name)

    cleaned_name = name.strip()

    # åˆ é™¤æ— å…³ä¿¡æ¯
    cleaned_name = re.sub(r'ã€[^ã€‘]*?(æµé‡|åˆ°æœŸ|è¿‡æœŸ|å……å€¼|ç»­è´¹)[^ã€‘]*ã€‘', '', cleaned_name)
    cleaned_name = re.sub(r'\[[^]]*?(æµé‡|åˆ°æœŸ|è¿‡æœŸ|å……å€¼|ç»­è´¹)[^\]]*\]', '', cleaned_name)
    cleaned_name = re.sub(r'\([^)]*?(æµé‡|åˆ°æœŸ|è¿‡æœŸ|å……å€¼|ç»­è´¹)[^)]*\)', '', cleaned_name)
    cleaned_name = re.sub(r'ï¼ˆ[^ï¼‰]*?(æµé‡|åˆ°æœŸ|è¿‡æœŸ|å……å€¼|ç»­è´¹)[^ï¼‰]*ï¼‰', '', cleaned_name)

    redundant_keywords = [
        r'\d+%', r'\d{4}-\d{2}-\d{2}', r'\d{2}-\d{2}', r'x\d+',
        r'ç§’æ€', r'æ´»åŠ¨', r'æ–°å¹´', r'ç¦åˆ©', r'VIP\d*', r'Pro', r'Lite', r'Plus',
        r'è‡ªåŠ¨', r'æ‰‹åŠ¨', r'è‡ªé€‰', r'(\d+\.\d+kbps)', r'(\d+\.\d+mbps)', r'(\d+kbps)', r'(\d+mbps)',
        r'\\n', r'\\r', r'\d+\.\d+G|\d+G',
    ]
    for keyword in redundant_keywords:
        cleaned_name = re.sub(keyword, ' ', cleaned_name, flags=re.IGNORECASE).strip()

    cleaned_name = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\s\.\-_@#|]', ' ', cleaned_name)
    cleaned_name = re.sub(r'\s+', ' ', cleaned_name).strip()

    # åœ°åŒºå’Œæä¾›å•†æ˜ å°„
    region_map = {
        'é¦™æ¸¯': 'HK', 'å°æ¹¾': 'TW', 'æ—¥æœ¬': 'JP', 'æ–°åŠ å¡': 'SG', 'ç¾å›½': 'US', 'è‹±å›½': 'UK',
        'å¾·å›½': 'DE', 'éŸ©å›½': 'KR', 'é©¬æ¥è¥¿äºš': 'MY', 'æ³°å›½': 'TH', 'è²å¾‹å®¾': 'PH', 'è¶Šå—': 'VN',
        'å°å°¼': 'ID', 'å°åº¦': 'IN', 'æ¾³å¤§åˆ©äºš': 'AU', 'åŠ æ‹¿å¤§': 'CA', 'ä¿„ç½—æ–¯': 'RU', 'å·´è¥¿': 'BR',
        'æ„å¤§åˆ©': 'IT', 'è·å…°': 'NL', 'ä¸­å›½': 'CN', 'é˜¿è”é…‹': 'AE', 'å®‰é“å°”': 'AD', 'å“ˆè¨å…‹æ–¯å¦': 'KZ'
    }
    provider_map = {
        'Amazon': 'AWS', 'Oracle': 'Oracle', 'Alibaba': 'Alibaba', 'Google': 'Google',
        'Tencent': 'Tencent', 'Vultr': 'Vultr', 'OVH': 'OVH', 'DigitalOcean': 'DO',
        'Akamai': 'Akamai', 'Core Labs': 'CoreLabs', 'Cloudflare': 'CF'
    }

    # æå–åœ°åŒºå’Œæä¾›å•†
    region = None
    provider = None
    region_match = region_pattern.search(cleaned_name)
    provider_match = provider_pattern.search(cleaned_name)
    if region_match:
        region = region_match.group(0).upper()
    if provider_match:
        provider = provider_match.group(0).title()
        for full_name, short_name in provider_map.items():
            if full_name.lower() in provider.lower():
                provider = short_name
                break

    # ä¿ç•™å…³é”®ä¿¡æ¯
    meaningful_keywords = ['IPLC', 'IEPL', 'ä¸“çº¿', 'ä¸­è½¬', 'ç›´è¿']
    preserved_info = [kw for kw in meaningful_keywords if kw.lower() in cleaned_name.lower()]
    
    node_number_match = re.search(r'(?<!\d)\d{1,2}(?!\d)|Node\d{1,2}', cleaned_name, re.IGNORECASE)
    if node_number_match:
        preserved_info.append(node_number_match.group(0))

    # æ„å»ºåç§°
    parts = []
    if region:
        parts.append(region)
    if provider:
        parts.append(provider)
    if preserved_info:
        parts.append('_'.join(preserved_info))
    if not parts:
        parts.append('Node')
    if index is not None:
        parts.append(f"{index:02d}")
    
    cleaned_name = '-'.join(parts)
    
    if len(cleaned_name) > 80:
        cleaned_name = cleaned_name[:80].rstrip() + '...'

    return cleaned_name if cleaned_name else f"Node-{index:02d}" if index is not None else "Unknown Node"

def _generate_node_fingerprint(node):
    """ä¸ºèŠ‚ç‚¹ç”Ÿæˆå”¯ä¸€æŒ‡çº¹ï¼Œç®€åŒ–å­—æ®µä»¥å‡å°‘é‡å¤"""
    def normalize_value(value):
        return '' if value is None else str(value).lower().strip()

    if isinstance(node, dict):
        fingerprint_data = {
            'type': normalize_value(node.get('type')),
            'server': normalize_value(node.get('server')),
            'port': normalize_value(node.get('port')),
            'network': normalize_value(node.get('network')),
            'tls': normalize_value(node.get('tls')),
            'sni': normalize_value(node.get('sni') or node.get('host')),
        }
        stable_json = json.dumps(fingerprint_data, sort_keys=True, ensure_ascii=False)
        fingerprint = hashlib.sha256(stable_json.encode('utf-8')).hexdigest()
        return fingerprint

    elif isinstance(node, str):
        try:
            if not any(node.startswith(p + '://') for p in ["vmess", "trojan", "ss", "ssr", "vless", "hy", "hy2", "hysteria", "hysteria2"]):
                return None
            parsed_url = urlparse(node)
            scheme = parsed_url.scheme.lower()
            netloc = parsed_url.netloc.lower()
            host = netloc.split(':')[0] if ':' in netloc else netloc
            port = netloc.split(':')[1] if ':' in netloc else ''
            if is_valid_ip_address(host) and host.startswith('[') and host.endswith(']'):
                host = host[1:-1]
            elif not is_valid_ip_address(host) and not re.match(r'^[a-zA-Z0-9\-\.]+$', host):
                return None
            fingerprint_parts = [scheme, host, port]
            fingerprint = hashlib.sha256(''.join(fingerprint_parts).encode('utf-8')).hexdigest()
            return fingerprint
        except Exception:
            return None

    return None

def deduplicate_and_standardize_nodes(raw_nodes_list):
    """å»é‡é€»è¾‘ï¼Œè¾“å‡ºæ‰€æœ‰å”¯ä¸€èŠ‚ç‚¹ï¼Œä¿æŒå¤šæ ·æ€§"""
    unique_node_fingerprints = set()
    grouped_nodes = defaultdict(list)

    logging.info(f"å»é‡å‰èŠ‚ç‚¹æ•°: {len(raw_nodes_list)}")

    for idx, node in enumerate(raw_nodes_list):
        clash_proxy_dict = None
        node_raw_name = ""

        if isinstance(node, dict):
            clash_proxy_dict = node
            node_raw_name = str(node.get('name', '')) 
        elif isinstance(node, str):
            try:
                parsed_url = urlparse(node)
                node_raw_name = str(parsed_url.fragment)
                if not any(node.startswith(p + '://') for p in ["vmess", "trojan", "ss", "ssr", "vless", "hy", "hy2", "hysteria", "hysteria2"]):
                    continue
                host = parsed_url.hostname or ''
                if host and not (is_valid_ip_address(host) or re.match(r'^[a-zA-Z0-9\-\.]+$', host)):
                    continue

                if node.startswith("vmess://"):
                    decoded = base64.b64decode(node[len("vmess://"):].encode('utf-8')).decode('utf-8')
                    config = json.loads(decoded)
                    clash_proxy_dict = {
                        'name': str(config.get('ps', 'VMess Node')),
                        'type': 'vmess',
                        'server': config.get('add'),
                        'port': int(config.get('port')),
                        'uuid': config.get('id'),
                        'alterId': int(config.get('aid', 0)),
                        'cipher': 'auto',
                        'network': config.get('net'),
                        'tls': True if config.get('tls') == 'tls' else False,
                        'skip-cert-verify': True if config.get('scy') == 'true' else False,
                        'servername': config.get('sni') or config.get('host'),
                        'ws-opts': {'path': config.get('path', '/'), 'headers': {'Host': config.get('host')}} if config.get('net') == 'ws' else None,
                        'grpc-opts': {'serviceName': config.get('path', '')} if config.get('net') == 'grpc' else None,
                    }
                    if clash_proxy_dict.get('ws-opts') == {'path': '/', 'headers': {'Host': ''}}:
                        clash_proxy_dict['ws-opts'] = None
                    if clash_proxy_dict.get('grpc-opts') == {'serviceName': ''}:
                        clash_proxy_dict['grpc-opts'] = None
                elif node.startswith("trojan://"):
                    parsed = urlparse(node)
                    password = parsed.username
                    server = parsed.hostname
                    port = parsed.port
                    query = parse_qs(parsed.query)
                    clash_proxy_dict = {
                        'name': str(parsed.fragment or 'Trojan Node'),
                        'type': 'trojan',
                        'server': server,
                        'port': port,
                        'password': password,
                        'network': query.get('type', ['tcp'])[0],
                        'tls': True,
                        'skip-cert-verify': query.get('allowInsecure', ['0'])[0] == '1',
                        'sni': query.get('sni', [server])[0]
                    }
                elif node.startswith("ss://"):
                    decoded_part = node[len("ss://"):].split('#', 1)[0]
                    try:
                        decoded_info = base64.b64decode(decoded_part.encode('utf-8')).decode('utf-8')
                        parts = decoded_info.split('@', 1)
                        method_password = parts[0].split(':', 1)
                        method = method_password[0]
                        password = method_password[1] if len(method_password) > 1 else ''
                        server_port = parts[1].split(':', 1)
                        server = server_port[0]
                        port = int(server_port[1])
                        clash_proxy_dict = {
                            'name': str(parsed_url.fragment or 'SS Node'),
                            'type': 'ss',
                            'server': server,
                            'port': port,
                            'cipher': method,
                            'password': password,
                        }
                    except Exception:
                        clash_proxy_dict = None
                elif node.startswith("vless://"):
                    parsed = urlparse(node)
                    uuid = parsed.username
                    server = parsed.hostname
                    port = parsed.port
                    query = parse_qs(parsed.query)
                    clash_proxy_dict = {
                        'name': str(parsed.fragment or 'VLESS Node'),
                        'type': 'vless',
                        'server': server,
                        'port': port,
                        'uuid': uuid,
                        'network': query.get('type', ['tcp'])[0],
                        'tls': True if query.get('security', [''])[0] == 'tls' else False,
                        'skip-cert-verify': query.get('flow', [''])[0] == 'xtls-rprx-direct',
                        'servername': query.get('sni', [server])[0],
                        'flow': query.get('flow', [''])[0],
                        'ws-opts': {'path': query.get('path', ['/'])[0], 'headers': {'Host': query.get('host', [''])[0]}} if query.get('type', [''])[0] == 'ws' else None,
                        'grpc-opts': {'serviceName': query.get('serviceName', [''])[0]} if query.get('type', [''])[0] == 'grpc' else None,
                    }
                    if clash_proxy_dict.get('ws-opts') == {'path': '/', 'headers': {'Host': ''}}:
                        clash_proxy_dict['ws-opts'] = None
                    if clash_proxy_dict.get('grpc-opts') == {'serviceName': ''}:
                        clash_proxy_dict['grpc-opts'] = None
                elif node.startswith("hysteria://") or node.startswith("hy://"):
                    parsed = urlparse(node)
                    server = parsed.hostname
                    port = parsed.port
                    query = parse_qs(parsed.query)
                    clash_proxy_dict = {
                        'name': str(parsed.fragment or 'Hysteria Node'),
                        'type': 'hysteria',
                        'server': server,
                        'port': port,
                        'auth_str': query.get('auth', [''])[0],
                        'alpn': query.get('alpn', [''])[0].split(','),
                        'network': query.get('protocol', ['udp'])[0],
                        'skip-cert-verify': query.get('insecure', ['0'])[0] == '1',
                        'sni': query.get('peer', [server])[0]
                    }
                    if not clash_proxy_dict['alpn']:
                        del clash_proxy_dict['alpn']
                elif node.startswith("hysteria2://") or node.startswith("hy2://"):
                    parsed = urlparse(node)
                    password = parsed.username
                    server = parsed.hostname
                    port = parsed.port
                    query = parse_qs(parsed.query)
                    clash_proxy_dict = {
                        'name': str(parsed.fragment or 'Hysteria2 Node'),
                        'type': 'hysteria2',
                        'server': parsed_url,
                        'port': 'port',
                        'password': password,
                        'obfs': query.get('obfs', [''])[0],
                        'obfs-password': query.get('password', [''])[0],
                        'tls': True,
                        'skip-cert-verify': query.get('insecure', ['0'])[0] == '1',
                        'sni': query.get('sni', [server])[0],
                        'alpn': query.get('alpn', [''])[0].split(',')
                    }
                    if not clash_proxy_dict['obfs']:
                        del clash_proxy_dict['obfs']
                    if not clash_proxy_dict['obfs-password']:
                        del clash_proxy_dict['obfs-password']
                    if not clash_proxy_dict['alpn']:
                        del clash_proxy_dict['alpn']
                except Exception as e:
                    logging.warning(f"URLèŠ‚ç‚¹è½¬æ¢ä¸ºClashå­—å…¸å¤±è´¥: {node[:50]}... - {e}")
                    clash_proxy_dict = None

        if clash_proxy_dict:
            # è¿‡æ»¤åŒ…å«åˆ é™¤å…³é”®è¯çš„èŠ‚ç‚¹
            if any(keyword.lower() in node_raw_name.lower() for keyword in DELETE_KEYWORDS:
                continue

            server = clash_proxy_dict.get('server', '')
            if server and not (is_valid_ip_address(server) or re.match(r'^[a-zA-Z0-9\-\.]+$', server)):
                logging.warning(f"å¿½ç•¥æ— æ•ˆæœåŠ¡å™¨åœ°å€çš„èŠ‚ç‚¹: {server}")
                continue

            # æå–åœ°åŒºå’Œæä¾›å•†
            region = 'Unknown'
            provider = 'Unknown'
            region_match = region_pattern.search(node_raw_name)
            provider_match = provider_pattern.search(node_raw_name)
            if region_match:
                region = region_match.group(0).upper()
            if provider_match:
                provider = provider_match.group(0).title()

            # æ¸…ç†èŠ‚ç‚¹åç§°
            clash_proxy_dict['name'] = clean_node_name(node_raw_name, idx + 1)

            # ç”ŸæˆæŒ‡çº¹å¹¶å»é‡
            fingerprint = _generate_node_fingerprint(clash_proxy_dict)
            if fingerprint and fingerprint not in unique_node_fingerprints:
                unique_node_fingerprints.add(fingerprint)
                group_key = (region, provider, clash_proxy_dict.get('type', 'Unknown'))
                grouped_nodes[group_key].append(clash_proxy_dict)

    # åŠ¨æ€åˆ†é…æ‰€æœ‰å”¯ä¸€èŠ‚ç‚¹ï¼Œä¿æŒå¤šæ ·æ€§
    final_clash_proxies = []
    region_counts = defaultdict(int)
    protocol_counts = defaultdict(int)

    # æŒ‰åœ°åŒºå’Œåè®®æ’åºï¼Œç¡®ä¿å¤šæ ·æ€§
    sorted_groups = sorted(grouped_nodes.items(), key=lambda x: len(x[1]), reverse=True)
    
    for (region, provider, protocol), nodes in sorted_groups:
        # éšæœºé€‰æ‹©ä¸€ä¸ªä»£è¡¨æ€§èŠ‚ç‚¹ï¼ˆå¯æ‰©å±•ä¸ºæŒ‰è´¨é‡æ’åºï¼‰
        selected = random.choice(nodes)
        final_clash_proxies.append(selected)
        region_counts[region] += 1
        protocol_counts[protocol.lower()] += 1

    logging.info(f"å»é‡åèŠ‚ç‚¹æ•°ï¼š{len(final_clash_proxies)}")
    logging.info(f"åœ°åŒºåˆ†å¸ƒï¼š{dict(region_counts)}")
    logging.info(f"åè®®åˆ†å¸ƒï¼š{dict(protocol_counts)}")
    return final_clash_proxies

# ä¸»ç¨‹åºæµç¨‹
URL_SOURCE = os.environ.get('URL_SOURCE')
print(f"è°ƒè¯•ä¿¡æ¯ - è¯»å–åˆ°çš„ URL æ¥æºï¼š{source: {URL_SOURCE}")

if not URL_SOURCE:
    print("é”™è¯¯ï¼šç¯å¢ƒå˜é‡ 'URL_SOURCE' æœªè®¾ç½®ã€‚æ— æ³•è·å–è®¢é˜…é“¾æ¥ã€‚")
    logging.error("æœªè®¾ç½® URL_SOURCE ç¯å¢ƒå˜é‡")
    exit(1)

os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)
os.makedirs(os.path.dirname(STATISTICS_FILE), exist_ok=True)

raw_urls_from_source = get_url_list_from_remote(URL_SOURCE)

urls_to_fetch = set()
url_statistics = []
successful_urls = []
failed_urls = []
all_parsed_nodes_raw = []

print("\n--- é¢„å¤„ç†åŸå§‹URL/å­—ç¬¦ä¸²åˆ—è¡¨ ---")
for entry in raw_urls_from_source:
    if is_valid_url(entry):
        urls_to_fetch.add(entry)
    else:
        print(f"å‘ç°éHTTP/HTTPSæ¡ç›®ï¼Œå°è¯•ç›´æ¥è§£æï¼š{entry[:80]}...")
        parsed_nodes = parse_content_to_nodes(entry.strip())
        if parsed_nodes:
            all_parsed_nodes_raw.extend(parsed_nodes)
            stat_entry = {'URL': 'entry['URL'], 'èŠ‚ç‚¹æ•°é‡': len(parsed_nodes)}, 'Status': 'Success', 'Error Message': 'Directly parsed successfully'}, 'Status Code': None}
            url_statistics.append(stat_entry)
            successful_urls.append(entry)
        else:
            stat_entry = {'entry: 'URL', 'failed_urls': 0}, 'Status': 'Failed', 'Error Message': 'éURLä¸”æ— æ³•è§£æä¸ºèŠ‚ç‚¹'}, 'Status Code': None}
            url_statistics.append(stat_entry)
            failed_urls.append(entry)

print("\n--- é˜¶æ®µ 1ï¼šè·å–å¹¶åˆå¹¶æ‰€æœ‰è®¢é˜…é“¾æ¥ä¸­çš„èŠ‚ç‚¹ ---")
total_urls_to_process_via_http = len(urls_to_fetch)

if total_urls_to_process_via_http > 0:
    with ThreadPoolExecutor(max_workers=16) as executor:
        future_to_url = {executor.submit(fetch_and_parse_url, url): url for url in urls_to_fetch}
        for future in tqdm(as_completed(future_to_url), total=total_urls_to_process_via_http, desc="é€šè¿‡HTTP/HTTPSè¯·æ±‚å¹¶è§£æèŠ‚ç‚¹", mininterval=1.0):
            url = future_to_url[future]
            nodes, success, error_message, status_code = future.result()
            stat_entry = {
                'URL': url,
                'èŠ‚ç‚¹æ•°é‡': len(nodes),
                'çŠ¶æ€': 'æˆåŠŸ' if success else 'å¤±è´¥',
                'é”™è¯¯ä¿¡æ¯': error_message if error_message else '',
                'çŠ¶æ€ç ': status_code
            }
            url_statistics.append(stat_entry)

            if success:
                successful_urls.append(url)
                all_parsed_nodes_raw.extend(nodes)
                print(f"æˆåŠŸå¤„ç† URL: {url}, èŠ‚ç‚¹æ•°: {len(nodes)}, çŠ¶æ€ç : {status_code}")
            else:
                failed_urls.append(url)
                print(f"å¤±è´¥ URL: {url}, é”™è¯¯: {error_message}")

final_clash_proxies = deduplicate_and_standardize_nodes(all_parsed_nodes_raw)

with open(TEMP_MERGED_NODES_RAW_FILE, 'w', encoding='utf-8') as f:
    for node in final_clash_proxies:
        if isinstance(node, dict):
            f.write(json.dumps(node, ensure_ascii=False) + '\n')
        else:
            f.write(str(node).strip() + '\n')

print(f"\né˜¶æ®µä¸€å®Œæˆã€‚åˆå¹¶åˆ° {len(final_clash_proxies)} ä¸ªå”¯ä¸€Clashä»£ç†å­—å…¸ï¼Œä¿å­˜è‡³ {TEMP_MERGED_NODES_RAW_FILE}")

write_statistics_to_csv(url_statistics, STATISTICS_FILE)
write_urls_to_file(successful_urls, SUCCESS_URLS_FILE)
write_urls_to_file(failed_urls, FAILED_URLS_FILE))

print("\n--- é˜¶æ®µäºŒï¼šè¾“å‡ºæœ€ç»ˆ Clash YAML é…ç½® ---")
if not OUTPUT_FILE.endswith(('.yaml', '.yml')):
    OUTPUT_FILE = os.path.splitext(OUTPUT_FILE)[0] + '.yaml'

proxies_to_output = final_clash_proxies  # è¾“å‡ºæ‰€æœ‰å”¯ä¸€èŠ‚ç‚¹

proxy_names_in_group = []
for node in proxies_to_output:
    if isinstance(node, dict) and 'name' in node:
        proxy_names_in_group.append(node['name'])
    else:
        proxy_names_in_group.append(f"{node.get('type', 'Unknown')} {node.get('server', '')}")

clash_config = {
    'proxies': proxies_to_output,
    'proxy-groups': [
        {
            'name': 'ğŸš– èŠ‚ç‚¹é€‰æ‹©',
            'type': 'select',
            'proxies': ['DIRECT'] + proxy_names_in_group
        },
        {
            'name': 'â™»ï¸ è‡ªåŠ¨é€‰æ‹©',
            'type': 'url-test',
            'url': 'http://www.gstatic.com/generate_204',
            'interval': 300,
            'proxies': proxy_names_in_group
        }
    ],
    'rules': [
        'MATCH', 'ğŸš– èŠ‚ç‚¹é€‰æ‹©'
    ]
}

success_count = len(proxies_to_output)

try:
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as out_file:
        yaml.dump(clash_config, out_file, allow_unicode=True, default_flow_style=False, sort_keys=False)
    print(f"æœ€ç»ˆ Clash YAML é…ç½®å·²ä¿å­˜è‡³ï¼š{OUTPUT_FILE}")

except Exception as e:
    logging.error(f"å†™å…¥æœ€ç»ˆ Clash YAML æ–‡ä»¶å¤±è´¥: {e}")
    print(f"é”™è¯¯ï¼šå†™å…¥æœ€ç»ˆ Clash YAML æ–‡ä»¶å¤±è´¥: {str(e)}")

if os.path.exists(TEMP_MERGED_NODES_RAW_FILE):
    os.remove(TEMP_MERGED_NODES_RAW_FILE)
    print(f"å·²åˆ é™¤ä¸´æ—¶æ–‡ä»¶ï¼š{TEMP_MERGED_NODES_RAW_FILE}")

print("\n" + "="*50)
print("æœ€ç»ˆç»“æœï¼š")
print(f"åŸå§‹æ¥æºæ€»æ¡ç›®æ•°ï¼š{len(raw_urls_from_source}")
print(f"å…¶ä¸­éœ€è¦HTTP/HTTPSè¯·æ±‚çš„è®¢é˜…é“¾æ¥æ•°ï¼š{len(urls_to_fetch)}")
print(f"å…¶ä¸­ç›´æ¥è§£æçš„éURLå­—ç¬¦ä¸²æ•°ï¼š{len(raw_urls_from_source) - len(urls_to_fetch)}")
print(f"æˆåŠŸå¤„ç†çš„URL/å­—ç¬¦ä¸²æ€»æ•°ï¼š{len(successful_urls)}")
print(f"å¤±è´¥çš„URL/å­—ç¬¦ä¸²æ€»æ•°ï¼š{len(failed_urls)}")
print(f"åˆæ­¥èšåˆçš„åŸå§‹èŠ‚ç‚¹æ•°ï¼ˆå»é‡å’Œè¿‡æ»¤å‰ï¼‰ï¼š{len(all_failed_urls_raw)}")
print(f"å»é‡ã€æ ‡å‡†åŒ–å’Œè¿‡æ»¤åçš„èŠ‚ç‚¹æ•°ï¼š{len(final_clash_proxies)}")
print(f"æœ€ç»ˆè¾“å‡ºåˆ°Clash YAMLæ–‡ä»¶çš„èŠ‚ç‚¹æ•°ï¼š{success_count}")
if len(final_clash_proxies) > 0:
    print(f"æœ€ç»ˆæœ‰æ•ˆå†…å®¹ç‡ï¼ˆç›¸å¯¹äºå»é‡è¿‡æ»¤åï¼‰ï¼š{success_count/len(final_clash_proxies):.1%}")
print(f"ç»“æœæ–‡ä»¶å·²ä¿å­˜è‡³ï¼š{OUTPUT_FILE}")
print(f"ç»Ÿè®¡æ•°æ®å·²ä¿å­˜è‡³ï¼š{STATISTICS_FILE}")
print(f"æˆåŠŸURLåˆ—è¡¨å·²ä¿å­˜è‡³ï¼š{SUCCESS_URLS_FILE}")
print(f"å¤±è´¥URLåˆ—è¡¨å·²ä¿å­˜è‡³ï¼š{FAILED_URLS_FILE}")
print("=" * 50)
