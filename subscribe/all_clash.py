# -*- coding: utf-8 -*-
import os
import requests
from urllib.parse import urlparse, parse_qs, urlencode
import base64
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
import argparse
import re
import yaml
import json
import csv
import hashlib
import ipaddress
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# é…ç½®æ—¥å¿—
# æ—¥å¿—æ–‡ä»¶åä¸º error.logï¼Œçº§åˆ«è®¾ç½®ä¸º DEBUGï¼Œæ–¹ä¾¿è°ƒè¯•å’Œé—®é¢˜è¿½æº¯
logging.basicConfig(filename='error.log', level=logging.DEBUG,
                    format='%(asctime)s - %(levelname)s - %(message)s')

# è¯·æ±‚å¤´
# æ¨¡æ‹Ÿæµè§ˆå™¨è¡Œä¸ºï¼Œé˜²æ­¢è¢«æœåŠ¡å™¨è¯†åˆ«ä¸ºæœºå™¨äºº
headers = {
    'User-Agent': (
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
        'AppleWebKit/537.36 (KHTML, like Gecko) '
        'Chrome/91.0.4472.124 Safari/537.36'
    ),
    'Accept-Encoding': 'gzip, deflate' # æ¥å—gzipå’Œdeflateç¼–ç ï¼Œæé«˜ä¼ è¾“æ•ˆç‡
}

# å‘½ä»¤è¡Œå‚æ•°è§£æ
# å…è®¸ç”¨æˆ·é€šè¿‡å‘½ä»¤è¡Œè‡ªå®šä¹‰è„šæœ¬è¡Œä¸º
parser = argparse.ArgumentParser(description="URLå†…å®¹è·å–è„šæœ¬ï¼Œæ”¯æŒå¤šä¸ªURLæ¥æºå’ŒèŠ‚ç‚¹è§£æ")
parser.add_argument('--max_success', type=int, default=99999, help="ç›®æ ‡æˆåŠŸæ•°é‡ï¼Œè¾¾åˆ°æ­¤æ•°é‡åè„šæœ¬å¯èƒ½ä¼šæå‰ç»ˆæ­¢")
parser.add_argument('--timeout', type=int, default=30, help="è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰")
parser.add_argument('--output', type=str, default='data/all_clash.yaml', help="è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œç”Ÿæˆçš„Clash YAMLé…ç½®å°†ä¿å­˜åˆ°æ­¤æ–‡ä»¶")
args = parser.parse_args()

# å…¨å±€å˜é‡
MAX_SUCCESS = args.max_success
TIMEOUT = args.timeout
OUTPUT_FILE = args.output
TEMP_MERGED_NODES_RAW_FILE = 'temp_merged_nodes_raw.txt' # ä¸´æ—¶æ–‡ä»¶ï¼Œç”¨äºå­˜å‚¨åŸå§‹è§£æåˆ°çš„èŠ‚ç‚¹
STATISTICS_FILE = 'data/url_statistics.csv' # ç»Ÿè®¡æ–‡ä»¶ï¼Œè®°å½•æ¯ä¸ªURLçš„å¤„ç†ç»“æœ
SUCCESS_URLS_FILE = 'data/successful_urls.txt' # æˆåŠŸè·å–å¹¶è§£æçš„URLåˆ—è¡¨
FAILED_URLS_FILE = 'data/failed_urls.txt' # å¤±è´¥çš„URLåˆ—è¡¨

# å®šä¹‰åˆ é™¤å…³é”®è¯
# åŒ…å«è¿™äº›å…³é”®è¯çš„èŠ‚ç‚¹åç§°å°†è¢«è·³è¿‡ï¼Œé€šå¸¸æ˜¯å¹¿å‘Šã€æµé‡ä¿¡æ¯ç­‰
DELETE_KEYWORDS = [
    'å‰©ä½™æµé‡', 'å¥—é¤åˆ°æœŸ', 'æµé‡', 'åˆ°æœŸ', 'è¿‡æœŸ', 'å…è´¹', 'è¯•ç”¨', 'ä½“éªŒ', 'é™æ—¶', 'é™åˆ¶',
    'å·²ç”¨', 'å¯ç”¨', 'ä¸è¶³', 'åˆ°æœŸæ—¶é—´', 'å€ç‡', 'è¿”åˆ©', 'å……å€¼', 'ç»­è´¹', 'ç”¨é‡', 'è®¢é˜…'
]

def is_valid_url(url):
    """
    éªŒè¯URLæ ¼å¼æ˜¯å¦åˆæ³•ï¼Œä»…æ¥å— http æˆ– https æ–¹æ¡ˆã€‚
    ä½¿ç”¨ urllib.parse.urlparse è¿›è¡Œè§£æå’ŒéªŒè¯ã€‚
    """
    try:
        result = urlparse(url)
        return all([result.scheme in ['http', 'https'], result.netloc])
    except Exception:
        return False

def is_valid_ip_address(host):
    """
    éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆçš„ IPv4 æˆ– IPv6 åœ°å€ã€‚
    ä½¿ç”¨ ipaddress æ¨¡å—è¿›è¡ŒéªŒè¯ã€‚
    """
    try:
        # å°è¯•è§£æä¸ºIPv4æˆ–IPv6
        ipaddress.ip_address(host)
        return True
    except ValueError:
        # é’ˆå¯¹IPv6åœ°å€å¯èƒ½å¸¦æ–¹æ‹¬å·çš„æƒ…å†µè¿›è¡Œé¢å¤–å¤„ç†
        try:
            if host.startswith('[') and host.endswith(']'):
                ipaddress.ip_address(host[1:-1])
                return True
            return False
        except ValueError:
            return False

def get_url_list_from_remote(url_source):
    """
    ä»ç»™å®šçš„å…¬å¼€ç½‘å€è·å– URL åˆ—è¡¨ã€‚
    é€šå¸¸è¿™ä¸ªurl_sourceä¼šæ˜¯ä¸€ä¸ªåŒ…å«è®¢é˜…é“¾æ¥çš„æ–‡æœ¬æ–‡ä»¶ã€‚
    """
    try:
        session = requests.Session()
        # é…ç½®é‡è¯•ç­–ç•¥ï¼Œå¤„ç†å¸¸è§çš„ç½‘ç»œé”™è¯¯å’ŒçŠ¶æ€ç 
        retries = Retry(total=3, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])
        session.mount('http://', HTTPAdapter(max_retries=retries))
        session.mount('https://', HTTPAdapter(max_retries=retries))
        response = session.get(url_source, headers=headers, timeout=10)
        response.raise_for_status() # å¦‚æœçŠ¶æ€ç ä¸æ˜¯2xxï¼Œä¼šæŠ›å‡ºHTTPError
        text_content = response.text.strip()
        # å°†å†…å®¹æŒ‰è¡Œåˆ†å‰²ï¼Œè¿‡æ»¤ç©ºè¡Œ
        raw_urls = [line.strip() for line in text_content.splitlines() if line.strip()]
        print(f"ä» {url_source} è·å–åˆ° {len(raw_urls)} ä¸ªURLæˆ–å­—ç¬¦ä¸²")
        return raw_urls
    except Exception as e:
        logging.error(f"è·å–URLåˆ—è¡¨å¤±è´¥: {url_source} - {e}")
        return []

def parse_content_to_nodes(content):
    """
    ä»æ–‡æœ¬å†…å®¹ä¸­è§£æå‡ºèŠ‚ç‚¹ã€‚
    å†…å®¹å¯èƒ½æ˜¯Base64ç¼–ç çš„ï¼Œä¹Ÿå¯èƒ½æ˜¯Clash YAMLæ ¼å¼ï¼Œæˆ–è€…ç›´æ¥æ˜¯å¤šç§åè®®çš„URLåˆ—è¡¨ã€‚
    """
    if not content:
        return []

    found_nodes = []
    processed_content = content

    # 1. å°è¯• Base64 è§£ç 
    # è®¢é˜…å†…å®¹é€šå¸¸æ˜¯Base64ç¼–ç çš„ï¼Œä¼˜å…ˆå°è¯•è§£ç 
    try:
        decoded_bytes = base64.b64decode(content)
        processed_content = decoded_bytes.decode('utf-8')
        logging.info("å†…å®¹æˆåŠŸ Base64 è§£ç ã€‚")
    except Exception:
        # å¦‚æœä¸æ˜¯Base64ç¼–ç ï¼Œåˆ™ä¿æŒåŸæ ·
        pass

    # 2. å°è¯• YAML è§£æ
    # å¦‚æœæ˜¯Clash YAMLé…ç½®ï¼Œè§£æå…¶ä¸­çš„proxieséƒ¨åˆ†
    try:
        parsed_data = yaml.safe_load(processed_content)
        if isinstance(parsed_data, dict) and 'proxies' in parsed_data and isinstance(parsed_data['proxies'], list):
            for proxy_entry in parsed_data['proxies']:
                if isinstance(proxy_entry, dict):
                    # ç¡®ä¿nameæ˜¯å­—ç¬¦ä¸²ï¼Œä»¥é˜²æŸäº›é…ç½®ä¸­nameæ˜¯æ•°å­—
                    if 'name' in proxy_entry and not isinstance(proxy_entry['name'], str):
                        proxy_entry['name'] = str(proxy_entry['name'])
                    found_nodes.append(proxy_entry)
                elif isinstance(proxy_entry, str) and any(proxy_entry.startswith(p + '://') for p in ["vmess", "trojan", "ss", "ssr", "vless", "hy", "hy2", "hysteria", "hysteria2"]):
                    # ç›´æ¥æ˜¯URLå­—ç¬¦ä¸²å½¢å¼çš„èŠ‚ç‚¹
                    found_nodes.append(proxy_entry.strip())
            logging.info("å†…å®¹æˆåŠŸè§£æä¸º Clash YAMLã€‚")
        elif isinstance(parsed_data, list):
            # å¦‚æœè§£æç»“æœç›´æ¥æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼ˆä¾‹å¦‚ï¼ŒæŸäº›è®¢é˜…ç›´æ¥è¿”å›èŠ‚ç‚¹URLåˆ—è¡¨ï¼‰
            for item in parsed_data:
                if isinstance(item, str):
                    found_nodes.append(item.strip())
                elif isinstance(item, dict):
                    if 'name' in item and not isinstance(item['name'], str):
                        item['name'] = str(item['name'])
                    found_nodes.append(item)
            logging.info("å†…å®¹æˆåŠŸè§£æä¸º YAML åˆ—è¡¨ã€‚")
    except yaml.YAMLError:
        pass # ä¸æ˜¯YAMLæ ¼å¼ï¼Œç»§ç»­å°è¯•å…¶ä»–è§£ææ–¹å¼
    except Exception as e:
        logging.error(f"YAML è§£æå¤±è´¥: {e}")
        pass

    # 3. é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼æå–èŠ‚ç‚¹
    # å°è¯•ä»åŸå§‹å†…å®¹æˆ–è§£ç åçš„å†…å®¹ä¸­ç›´æ¥åŒ¹é…èŠ‚ç‚¹URL
    node_pattern = re.compile(
        r'(vmess://\S+|'
        r'trojan://\S+|'
        r'ss://\S+|'
        r'ssr://\S+|'
        r'vless://\S+|'
        r'hy://\S+|'
        r'hy2://\S+|'
        r'hysteria://\S+|'
        r'hysteria2://\S+)'
    )
    
    # æ£€æŸ¥åŸå§‹å†…å®¹
    matches = node_pattern.findall(content)
    for match in matches:
        found_nodes.append(match.strip())
    
    # å¦‚æœå†…å®¹è¢«è§£ç è¿‡ï¼Œå†æ£€æŸ¥è§£ç åçš„å†…å®¹
    if content != processed_content:
        matches_decoded = node_pattern.findall(processed_content)
        for match in matches_decoded:
            found_nodes.append(match.strip())

    return found_nodes

def fetch_and_parse_url(url):
    """
    è·å–URLå†…å®¹å¹¶è§£æå‡ºèŠ‚ç‚¹ã€‚
    æ­¤å‡½æ•°ä¼šå°è¯•è¯·æ±‚ç»™å®šçš„URLï¼Œç„¶åè°ƒç”¨ parse_content_to_nodes è¿›è¡Œè§£æã€‚
    è¿”å› (èŠ‚ç‚¹åˆ—è¡¨, æ˜¯å¦æˆåŠŸ, é”™è¯¯ä¿¡æ¯, çŠ¶æ€ç )
    """
    session = requests.Session()
    retries = Retry(total=2, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])
    session.mount('http://', HTTPAdapter(max_retries=retries))
    session.mount('https://', HTTPAdapter(max_retries=retries))

    try:
        logging.debug(f"å¼€å§‹è¯·æ±‚ URL: {url}")
        resp = session.get(url, headers=headers, timeout=TIMEOUT)
        resp.raise_for_status() # å¯¹äº 4xx æˆ– 5xx çŠ¶æ€ç ï¼ŒæŠ›å‡ºå¼‚å¸¸
        content = resp.text.strip()
        
        if len(content) < 10: # å†…å®¹è¿‡çŸ­å¯èƒ½æ˜¯æ— æ•ˆè®¢é˜…
            logging.warning(f"è·å–åˆ°å†…å®¹è¿‡çŸ­ï¼Œå¯èƒ½æ— æ•ˆ: {url}")
            return [], False, "å†…å®¹è¿‡çŸ­", resp.status_code
            
        nodes = parse_content_to_nodes(content)
        logging.debug(f"URL {url} è§£æåˆ° {len(nodes)} ä¸ªèŠ‚ç‚¹")
        return nodes, True, None, resp.status_code
    except requests.exceptions.Timeout:
        logging.error(f"è¯·æ±‚è¶…æ—¶: {url}")
        return [], False, "è¯·æ±‚è¶…æ—¶", None
    except requests.exceptions.ConnectionError as e:
        logging.error(f"è¿æ¥å¤±è´¥: {url} - {e}")
        return [], False, f"è¿æ¥å¤±è´¥: {e}", None
    except requests.exceptions.HTTPError as e:
        logging.error(f"HTTPé”™è¯¯: {url} - {e}")
        return [], False, f"HTTPé”™è¯¯: {e}", None
    except requests.exceptions.RequestException as e:
        logging.error(f"è¯·æ±‚å¤±è´¥: {url} - {e}")
        return [], False, f"è¯·æ±‚å¤±è´¥: {e}", None
    except Exception as e:
        logging.error(f"å¤„ç†URLå¼‚å¸¸: {url} - {e}")
        return [], False, f"æœªçŸ¥å¼‚å¸¸: {e}", None

def write_statistics_to_csv(statistics_data, filename):
    """å°†ç»Ÿè®¡æ•°æ®å†™å…¥CSVæ–‡ä»¶"""
    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
        fieldnames = ['URL', 'èŠ‚ç‚¹æ•°é‡', 'çŠ¶æ€', 'é”™è¯¯ä¿¡æ¯', 'çŠ¶æ€ç ']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

        writer.writeheader()
        for row in statistics_data:
            writer.writerow(row)
    print(f"ç»Ÿè®¡æ•°æ®å·²ä¿å­˜è‡³ï¼š{filename}")

def write_urls_to_file(urls, filename):
    """å°†URLåˆ—è¡¨å†™å…¥æ–‡ä»¶"""
    with open(filename, 'w', encoding='utf-8') as f:
        for url in urls:
            f.write(url + '\n')
    print(f"URLåˆ—è¡¨å·²ä¿å­˜è‡³ï¼š{filename}")

def clean_node_name(name, index=None):
    """
    æ¸…ç†èŠ‚ç‚¹åç§°ï¼Œå»é™¤å¤šä½™ä¿¡æ¯ï¼Œæ ‡å‡†åŒ–åŒºåŸŸåç§°ï¼Œå¹¶æ·»åŠ åºå·ã€‚
    è¿™ä¸ªå‡½æ•°ä»…ç”¨äºç¾åŒ–èŠ‚ç‚¹åç§°ï¼Œä¸å½±å“å»é‡é€»è¾‘ã€‚
    """
    if not isinstance(name, str):
        name = str(name)

    cleaned_name = name.strip()

    # ç§»é™¤åŒ…å«ç‰¹å®šå…³é”®è¯çš„æ‹¬å·å†…å®¹ï¼Œä¾‹å¦‚â€œã€å‰©ä½™æµé‡2Gã€‘â€
    cleaned_name = re.sub(r'ã€[^ã€‘]*?(æµé‡|åˆ°æœŸ|è¿‡æœŸ|å……å€¼|ç»­è´¹)[^ã€‘]*ã€‘', '', cleaned_name)
    cleaned_name = re.sub(r'\[[^]]*?(æµé‡|åˆ°æœŸ|è¿‡æœŸ|å……å€¼|ç»­è´¹)[^\]]*\]', '', cleaned_name)
    cleaned_name = re.sub(r'\([^)]*?(æµé‡|åˆ°æœŸ|è¿‡æœŸ|å……å€¼|ç»­è´¹)[^)]*\)', '', cleaned_name)
    cleaned_name = re.sub(r'ï¼ˆ[^ï¼‰]*?(æµé‡|åˆ°æœŸ|è¿‡æœŸ|å……å€¼|ç»­è´¹)[^ï¼‰]*ï¼‰', '', cleaned_name)

    # ç§»é™¤å…¶ä»–å†—ä½™å…³é”®è¯æˆ–æ¨¡å¼
    redundant_keywords_to_remove = [
        r'\d+%', r'\d{4}-\d{2}-\d{2}', r'\d{2}-\d{2}', r'x\d+', # 100%, æ—¥æœŸ, x2å€ç‡
        r'ç§’æ€', r'æ´»åŠ¨', r'æ–°å¹´', r'ç¦åˆ©', r'VIP\d*', r'Pro', r'Lite', r'Plus', # ä¿ƒé”€è¯
        r'è‡ªåŠ¨', r'æ‰‹åŠ¨', r'è‡ªé€‰', # é€‰æ‹©æ–¹å¼
        r'(\d+\.\d+kbps)', r'(\d+\.\d+mbps)', r'(\d+kbps)', r'(\d+mbps)', # é€Ÿåº¦ä¿¡æ¯
        r'\\n', r'\\r', r'\d+\.\d+G|\d+G', # æ¢è¡Œç¬¦ï¼Œæµé‡Gæ•°
    ]

    for keyword in redundant_keywords_to_remove:
        cleaned_name = re.sub(keyword, ' ', cleaned_name, flags=re.IGNORECASE).strip()

    # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œåªä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—ã€ç©ºæ ¼å’Œä¸€äº›å¸¸ç”¨ç¬¦å·
    cleaned_name = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\s\.\-_@#|]', ' ', cleaned_name)
    # åˆå¹¶å¤šä¸ªç©ºæ ¼ä¸ºå•ä¸ªç©ºæ ¼
    cleaned_name = re.sub(r'\s+', ' ', cleaned_name).strip()

    # å°†ä¸­æ–‡åŒºåŸŸåæ›¿æ¢ä¸ºè‹±æ–‡ç®€ç§°
    region_map = {
        'é¦™æ¸¯': 'HK', 'å°æ¹¾': 'TW', 'æ—¥æœ¬': 'JP', 'æ–°åŠ å¡': 'SG', 'ç¾å›½': 'US', 'è‹±å›½': 'UK',
        'å¾·å›½': 'DE', 'éŸ©å›½': 'KR', 'é©¬æ¥': 'MY', 'æ³°å›½': 'TH', 'PH': 'PH', 'è¶Šå—': 'VN',
        'å°å°¼': 'ID', 'å°åº¦': 'IN', 'æ¾³æ´²': 'AU', 'åŠ æ‹¿å¤§': 'CA', 'ä¿„ç½—æ–¯': 'RU', 'å·´è¥¿': 'BR',
        'æ„å¤§åˆ©': 'IT', 'è·å…°': 'NL', 'ä¸­å›½': 'CN' # æ·»åŠ ä¸­å›½
    }
    for full_name, short_name in region_map.items():
        cleaned_name = cleaned_name.replace(full_name, short_name)

    # å°è¯•ä¿ç•™ä¸€äº›æœ‰æ„ä¹‰çš„å…³é”®è¯ï¼Œä¾‹å¦‚ä¸“çº¿ä¿¡æ¯
    meaningful_keywords = ['IPLC', 'IEPL', 'ä¸“çº¿', 'ä¸­è½¬', 'ç›´è¿']
    preserved_info = []
    for keyword in meaningful_keywords:
        if keyword.lower() in cleaned_name.lower():
            preserved_info.append(keyword)
    
    # å°è¯•ä¿ç•™èŠ‚ç‚¹ç¼–å·
    node_number_match = re.search(r'(?<!\d)\d{1,2}(?!\d)|Node\d{1,2}', cleaned_name, re.IGNORECASE)
    if node_number_match:
        preserved_info.append(node_number_match.group(0))

    # å¦‚æœæ¸…ç†ååç§°è¿‡çŸ­æˆ–ä¸ºç©ºï¼Œå°è¯•ä½¿ç”¨åŒºåŸŸåæˆ–é»˜è®¤åç§°
    if not cleaned_name or len(cleaned_name) <= 3:
        cleaned_name = 'Node'
        # å¦‚æœåŸå§‹åç§°ä¸­åŒ…å«åŒºåŸŸä¿¡æ¯ï¼Œä¼˜å…ˆä½¿ç”¨åŒºåŸŸä¿¡æ¯
        if any(region in name for region in region_map.values()):
            for region in region_map.values():
                if region in name:
                    cleaned_name = region
                    break
        if preserved_info:
            cleaned_name += ' ' + ' '.join(preserved_info) # è¡¥å……ä¿ç•™çš„ä¿¡æ¯

    # æ·»åŠ åºå·ï¼Œç¡®ä¿åç§°å”¯ä¸€æ€§ (åœ¨æœ€ç»ˆè¾“å‡ºæ—¶å†ç»Ÿä¸€æ·»åŠ ï¼Œè¿™é‡Œåªæ˜¯ä¸€ä¸ªé€šç”¨æ¸…ç†å‡½æ•°)
    # è„šæœ¬çš„å®é™…å®ç°ä¸­ï¼Œåºå·æ˜¯åœ¨ deduplicate_and_standardize_nodes ä¸­æ·»åŠ çš„
    if index is not None:
        cleaned_name += f"-{index:02d}"

    # é™åˆ¶åç§°é•¿åº¦
    if len(cleaned_name) > 80:
        cleaned_name = cleaned_name[:80].rstrip() + '...'

    return cleaned_name if cleaned_name else f"Node-{index:02d}" if index is not None else "Unknown Node"

def _generate_node_fingerprint(node):
    """
    ä¸ºèŠ‚ç‚¹ç”Ÿæˆå”¯ä¸€æŒ‡çº¹ï¼ˆå“ˆå¸Œå€¼ï¼‰ã€‚
    è¿™æ˜¯å»é‡é€»è¾‘çš„æ ¸å¿ƒã€‚
    æ”¹è¿›ç‚¹ï¼šæ›´å…¨é¢åœ°å¤„ç† ws-opts å’Œ grpc-optsï¼Œç¡®ä¿å…¶å®Œæ•´å†…å®¹å½±å“æŒ‡çº¹ã€‚
    """
    if isinstance(node, dict):
        # æå–æ ¸å¿ƒå‚æ•°ï¼Œè¿™äº›å‚æ•°æ˜¯èŠ‚ç‚¹èº«ä»½çš„å…³é”®
        fingerprint_data = {
            'type': node.get('type'),
            'server': node.get('server'),
            'port': node.get('port'),
        }

        node_type = node.get('type')
        if node_type == 'vmess':
            fingerprint_data['uuid'] = node.get('uuid') or node.get('id')
            fingerprint_data['alterId'] = node.get('alterId') or node.get('aid')
            fingerprint_data['cipher'] = node.get('cipher') # vmess ä¹Ÿæœ‰ cipher
            fingerprint_data['network'] = node.get('network')
            fingerprint_data['tls'] = node.get('tls')
            fingerprint_data['servername'] = node.get('servername') or node.get('host') or node.get('sni') # ç»Ÿä¸€ servername/sni/host
            
            # æ”¹è¿›ï¼šå°†æ•´ä¸ª ws-opts æˆ– grpc-opts å­—å…¸è¿›è¡Œå“ˆå¸Œ
            if node.get('ws-opts'):
                # ç¡®ä¿ ws-opts å†…éƒ¨é”®æ’åºä¸€è‡´ï¼Œç„¶åå“ˆå¸Œæ•´ä¸ªå­—å…¸
                ws_opts_str = json.dumps(node['ws-opts'], sort_keys=True, ensure_ascii=False)
                fingerprint_data['ws-opts-hash'] = hashlib.sha256(ws_opts_str.encode('utf-8')).hexdigest()
            if node.get('grpc-opts'):
                grpc_opts_str = json.dumps(node['grpc-opts'], sort_keys=True, ensure_ascii=False)
                fingerprint_data['grpc-opts-hash'] = hashlib.sha256(grpc_opts_str.encode('utf-8')).hexdigest()

        elif node_type == 'trojan':
            fingerprint_data['password'] = node.get('password')
            fingerprint_data['network'] = node.get('network')
            fingerprint_data['tls'] = node.get('tls')
            fingerprint_data['servername'] = node.get('servername') or node.get('sni') or node.get('host')
            fingerprint_data['skip-cert-verify'] = node.get('skip-cert-verify')
            # trojan ä¹Ÿå¯èƒ½æœ‰ ws-opts/grpc-optsï¼Œä½† Clash é…ç½®ä¸­é€šå¸¸ç›´æ¥å†™åœ¨ network ä¸‹
            # å¦‚æœ Clash é…ç½®ä¸­ trojan ä¹Ÿæœ‰ç‹¬ç«‹çš„ ws-opts/grpc-opts å­—æ®µï¼Œæ­¤å¤„éœ€å¢åŠ 
            if node.get('ws-opts'):
                ws_opts_str = json.dumps(node['ws-opts'], sort_keys=True, ensure_ascii=False)
                fingerprint_data['ws-opts-hash'] = hashlib.sha256(ws_opts_str.encode('utf-8')).hexdigest()
            if node.get('grpc-opts'):
                grpc_opts_str = json.dumps(node['grpc-opts'], sort_keys=True, ensure_ascii=False)
                fingerprint_data['grpc-opts-hash'] = hashlib.sha256(grpc_opts_str.encode('utf-8')).hexdigest()

        elif node_type == 'ss':
            fingerprint_data['cipher'] = node.get('cipher')
            fingerprint_data['password'] = node.get('password')
            fingerprint_data['plugin'] = node.get('plugin') # SSå¯èƒ½å¸¦plugin
            if node.get('plugin-opts'): # å¤„ç†plugin-opts
                plugin_opts_str = json.dumps(node['plugin-opts'], sort_keys=True, ensure_ascii=False)
                fingerprint_data['plugin-opts-hash'] = hashlib.sha256(plugin_opts_str.encode('utf-8')).hexdigest()

        elif node_type == 'vless':
            fingerprint_data['uuid'] = node.get('uuid') or node.get('id')
            fingerprint_data['network'] = node.get('network')
            fingerprint_data['tls'] = node.get('tls')
            fingerprint_data['servername'] = node.get('servername') or node.get('sni') or node.get('host')
            fingerprint_data['flow'] = node.get('flow') # VLESS ç‹¬æœ‰ flow
            fingerprint_data['skip-cert-verify'] = node.get('skip-cert-verify')

            # æ”¹è¿›ï¼šå°†æ•´ä¸ª ws-opts æˆ– grpc-opts å­—å…¸è¿›è¡Œå“ˆå¸Œ
            if node.get('ws-opts'):
                ws_opts_str = json.dumps(node['ws-opts'], sort_keys=True, ensure_ascii=False)
                fingerprint_data['ws-opts-hash'] = hashlib.sha256(ws_opts_str.encode('utf-8')).hexdigest()
            if node.get('grpc-opts'):
                grpc_opts_str = json.dumps(node['grpc-opts'], sort_keys=True, ensure_ascii=False)
                fingerprint_data['grpc-opts-hash'] = hashlib.sha256(grpc_opts_str.encode('utf-8')).hexdigest()
            
            # xtls settings for vless
            fingerprint_data['xudp'] = node.get('xudp')
            fingerprint_data['udp-over-tcp'] = node.get('udp-over-tcp')

        elif node_type in ['hysteria', 'hysteria2', 'hy', 'hy2']:
            fingerprint_data['password'] = node.get('password') # Hysteria2 ç”¨ password
            fingerprint_data['auth_str'] = node.get('auth_str') # Hysteria ç”¨ auth_str
            fingerprint_data['obfs'] = node.get('obfs')
            fingerprint_data['obfs-password'] = node.get('obfs-password')
            fingerprint_data['tls'] = node.get('tls')
            fingerprint_data['servername'] = node.get('servername') or node.get('sni') or node.get('peer')
            fingerprint_data['alpn'] = sorted(node.get('alpn', [])) # ALPN åˆ—è¡¨æ’åºååŠ å…¥
            fingerprint_data['skip-cert-verify'] = node.get('skip-cert-verify')
            fingerprint_data['protocol'] = node.get('protocol') # Hysteria å¯èƒ½æœ‰ protocol (udp/webrtc)
            fingerprint_data['up'] = node.get('up') # bandwidth
            fingerprint_data['down'] = node.get('down') # bandwidth

        # å°†æ‰€æœ‰æŒ‡çº¹æ•°æ®é¡¹æ ‡å‡†åŒ–ä¸ºå­—ç¬¦ä¸²ï¼Œå¹¶è½¬æ¢ä¸ºå°å†™ï¼Œå»é™¤é¦–å°¾ç©ºç™½
        # ç¡®ä¿ None å€¼å¤„ç†ä¸º ''ï¼Œä½¿ä¸åŒè¡¨ç¤ºçš„ç©ºå€¼å…·æœ‰ç›¸åŒæŒ‡çº¹
        normalized_data = {k: str(v).lower().strip() if v is not None else '' for k, v in fingerprint_data.items()}
        
        # å°†æ ‡å‡†åŒ–åçš„æ•°æ®è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²ï¼Œå¹¶æ’åºé”®ä»¥ä¿è¯ä¸€è‡´æ€§ï¼Œæœ€åè¿›è¡ŒSHA256å“ˆå¸Œ
        stable_json = json.dumps(normalized_data, sort_keys=True, ensure_ascii=False)
        # logging.debug(f"Generated fingerprint JSON for node {node.get('name', 'N/A')}: {stable_json}")
        return hashlib.sha256(stable_json.encode('utf-8')).hexdigest()
    
    elif isinstance(node, str):
        # å¯¹äº URL å­—ç¬¦ä¸²å½¢å¼çš„èŠ‚ç‚¹ï¼Œç›´æ¥ä» URL ä¸­æå–å…³é”®ä¿¡æ¯ç”ŸæˆæŒ‡çº¹
        try:
            if not any(node.startswith(p + '://') for p in ["vmess", "trojan", "ss", "ssr", "vless", "hy", "hy2", "hysteria", "hysteria2"]):
                logging.warning(f"æ— æ•ˆçš„èŠ‚ç‚¹åè®®: {node[:50]}...")
                return None

            parsed_url = urlparse(node)
            scheme = parsed_url.scheme
            netloc = parsed_url.netloc
            path = parsed_url.path
            # è§£ææŸ¥è¯¢å‚æ•°ï¼Œå¹¶è¿›è¡Œæ ‡å‡†åŒ–
            query_params = parse_qs(parsed_url.query)
            
            host = netloc.split(':')[0] if ':' in netloc else netloc
            if is_valid_ip_address(host) and host.startswith('[') and host.endswith(']'):
                host = host[1:-1] # ç§»é™¤IPv6åœ°å€çš„æ–¹æ‹¬å·
            elif not is_valid_ip_address(host) and not re.match(r'^[a-zA-Z0-9\-\.]+$', host):
                logging.warning(f"æ— æ•ˆçš„ä¸»æœºå: {host} in {node[:50]}...")
                return None

            normalized_query_params = {}
            for k, v in query_params.items():
                # ç»Ÿä¸€å¤„ç†æŸ¥è¯¢å‚æ•°ï¼Œä¾‹å¦‚å°†åˆ—è¡¨å€¼å–ç¬¬ä¸€ä¸ªï¼Œå¹¶æ ‡å‡†åŒ–ä¸ºå°å†™å­—ç¬¦ä¸²
                normalized_query_params[k.lower()] = str(v[0]).lower().strip()
            
            # æ„å»ºæŒ‡çº¹éƒ¨ä»¶åˆ—è¡¨ï¼Œè¿™äº›éƒ¨ä»¶åº”è¯¥æ˜¯å½±å“èŠ‚ç‚¹å”¯ä¸€æ€§çš„æ ¸å¿ƒå‚æ•°
            fingerprint_parts = [
                scheme,
                host.lower(),
                # ç«¯å£å·ï¼Œç¡®ä¿ä¸€è‡´æ€§
                netloc.lower().split(':')[-1] if ':' in netloc else '',
                path.lower()
            ]

            # æ’é™¤ä¸å½±å“èŠ‚ç‚¹å”¯ä¸€æ€§çš„æŸ¥è¯¢å‚æ•° (å¦‚åç§°ã€æµé‡ä¿¡æ¯ç­‰)
            # è¿™äº›å‚æ•°é€šå¸¸æ˜¯åŠ¨æ€å˜åŒ–çš„ï¼Œä¸åº”è¯¥ä½œä¸ºå»é‡ä¾æ®
            excluded_keys = ['name', 'ps', 'remarks', 'info', 'flow', 'usage', 'expire', 'ud', 'up', 'dn', 'package', 'nodeName', 'nodeid', 'ver', 'hash', 'group'] # æ·»åŠ  hash, group
            
            # å°†å‰©ä½™çš„æŸ¥è¯¢å‚æ•°æŒ‰é”®åæ’åºååŠ å…¥æŒ‡çº¹éƒ¨ä»¶
            sorted_query_keys = sorted(normalized_query_params.keys())
            for k in sorted_query_keys:
                if k not in excluded_keys:
                    fingerprint_parts.append(f"{k}={normalized_query_params[k]}")

            # å¯¹æ‰€æœ‰éƒ¨ä»¶è¿›è¡Œå“ˆå¸Œ
            fingerprint_str = "".join(fingerprint_parts)
            # logging.debug(f"Generated fingerprint string for URL {node[:50]}: {fingerprint_str}")
            return hashlib.sha256(fingerprint_str.encode('utf-8')).hexdigest()
        except Exception as e:
            logging.warning(f"ç”ŸæˆURLèŠ‚ç‚¹æŒ‡çº¹å¤±è´¥: {node[:50]}... - {e}")
            return None
    return None

def deduplicate_and_standardize_nodes(raw_nodes_list):
    """
    å¯¹èŠ‚ç‚¹è¿›è¡Œå»é‡å’Œæ ‡å‡†åŒ–ã€‚
    å°†å„ç§åŸå§‹èŠ‚ç‚¹æ ¼å¼è½¬æ¢ä¸ºç»Ÿä¸€çš„Clashä»£ç†å­—å…¸æ ¼å¼ï¼Œå¹¶åŸºäºæŒ‡çº¹è¿›è¡Œå»é‡ã€‚
    """
    unique_node_fingerprints = set()
    final_clash_proxies = []

    for idx, node in enumerate(raw_nodes_list):
        clash_proxy_dict = None
        node_raw_name = "" # ç”¨äºä¿ç•™åŸå§‹åç§°ä»¥è¿›è¡Œå…³é”®è¯æ£€æŸ¥

        if isinstance(node, dict):
            # å¦‚æœå·²ç»æ˜¯å­—å…¸æ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨
            clash_proxy_dict = node
            node_raw_name = str(node.get('name', '')) # è·å–åŸå§‹åç§°
        elif isinstance(node, str):
            # å¦‚æœæ˜¯URLå­—ç¬¦ä¸²ï¼Œå°è¯•è§£æä¸ºClashå­—å…¸æ ¼å¼
            try:
                parsed_url = urlparse(node)
                # æå–åŸå§‹èŠ‚ç‚¹åç§°ï¼ˆé€šå¸¸åœ¨URLç‰‡æ®µä¸­ï¼‰
                node_raw_name = str(parsed_url.fragment or '') 
                
                # æ£€æŸ¥åè®®æ˜¯å¦æœ‰æ•ˆ
                if not any(node.startswith(p + '://') for p in ["vmess", "trojan", "ss", "ssr", "vless", "hy", "hy2", "hysteria", "hysteria2"]):
                    logging.warning(f"è·³è¿‡æ— æ•ˆåè®®çš„èŠ‚ç‚¹: {node[:50]}...")
                    continue
                
                # æ£€æŸ¥ä¸»æœºåæ˜¯å¦æœ‰æ•ˆ
                host = parsed_url.hostname or ''
                if host and not (is_valid_ip_address(host) or re.match(r'^[a-zA-Z0-9\-\.]+$', host)):
                    logging.warning(f"è·³è¿‡æ— æ•ˆä¸»æœºåçš„èŠ‚ç‚¹: {host} in {node[:50]}...")
                    continue

                # æ ¹æ®åè®®ç±»å‹è¿›è¡Œè§£æå¹¶è½¬æ¢ä¸ºClashå­—å…¸æ ¼å¼
                if node.startswith("vmess://"):
                    decoded = base64.b64decode(node[len("vmess://"):].encode('utf-8')).decode('utf-8')
                    config = json.loads(decoded)
                    clash_proxy_dict = {
                        'name': str(config.get('ps', 'VMess Node')),
                        'type': 'vmess',
                        'server': config.get('add'),
                        'port': int(config.get('port')),
                        'uuid': config.get('id'),
                        'alterId': int(config.get('aid', 0)),
                        'cipher': config.get('scy', 'auto'), # vmess ä¹Ÿæœ‰ scy å­—æ®µè¡¨ç¤ºåŠ å¯†æ–¹å¼
                        'network': config.get('net'),
                        'tls': True if config.get('tls') == 'tls' else False,
                        'skip-cert-verify': True if config.get('scy') == 'true' else False, # Clash scy å’Œ skip-cert-verify è¡Œä¸ºæœ‰å·®å¼‚ï¼Œè¿™é‡Œåªä½œä¸ºå‚è€ƒ
                        'servername': config.get('sni') or config.get('host') or config.get('add'), # sni/host/add ä¼˜å…ˆçº§
                    }
                    if config.get('net') == 'ws':
                        clash_proxy_dict['ws-opts'] = {
                            'path': config.get('path', '/'),
                            'headers': {'Host': config.get('host') or config.get('add')} # Host ä¸ºç©ºæ—¶ä½¿ç”¨ add
                        }
                        if clash_proxy_dict['ws-opts']['headers']['Host'] == '': # å¦‚æœHostè¿˜æ˜¯ç©ºï¼Œåˆ™åˆ é™¤headers
                             del clash_proxy_dict['ws-opts']['headers']['Host']
                             if not clash_proxy_dict['ws-opts']['headers']: # å¦‚æœheadersä¸ºç©ºï¼Œåˆ™åˆ é™¤headers
                                 del clash_proxy_dict['ws-opts']['headers']
                        if clash_proxy_dict['ws-opts']['path'] == '/' and not clash_proxy_dict['ws-opts'].get('headers'): # å¦‚æœpathå’Œheaderséƒ½ä¸ºç©ºï¼Œåˆ™åˆ é™¤ws-opts
                            clash_proxy_dict['ws-opts'] = None
                    if config.get('net') == 'grpc':
                        clash_proxy_dict['grpc-opts'] = {
                            'serviceName': config.get('path', '')
                        }
                        if clash_proxy_dict['grpc-opts']['serviceName'] == '':
                            clash_proxy_dict['grpc-opts'] = None
                        
                elif node.startswith("trojan://"):
                    parsed = urlparse(node)
                    password = parsed.username
                    server = parsed.hostname
                    port = parsed.port
                    query = parse_qs(parsed.query)
                    clash_proxy_dict = {
                        'name': str(parsed.fragment or 'Trojan Node'),
                        'type': 'trojan',
                        'server': server,
                        'port': port,
                        'password': password,
                        'network': query.get('type', ['tcp'])[0],
                        'tls': True,
                        'skip-cert-verify': query.get('allowInsecure', ['0'])[0] == '1',
                        'servername': query.get('sni', [server])[0]
                    }
                    # Trojanåè®®ä¹Ÿå¯èƒ½é€šè¿‡æŸ¥è¯¢å‚æ•°æºå¸¦wsæˆ–grpcä¿¡æ¯
                    if query.get('type', [''])[0] == 'ws':
                        clash_proxy_dict['ws-opts'] = {
                            'path': query.get('path', ['/'])[0],
                            'headers': {'Host': query.get('host', [''])[0]}
                        }
                        if clash_proxy_dict['ws-opts']['headers']['Host'] == '':
                             del clash_proxy_dict['ws-opts']['headers']['Host']
                             if not clash_proxy_dict['ws-opts']['headers']:
                                 del clash_proxy_dict['ws-opts']['headers']
                        if clash_proxy_dict['ws-opts']['path'] == '/' and not clash_proxy_dict['ws-opts'].get('headers'):
                            clash_proxy_dict['ws-opts'] = None
                    if query.get('type', [''])[0] == 'grpc':
                        clash_proxy_dict['grpc-opts'] = {
                            'serviceName': query.get('serviceName', [''])[0]
                        }
                        if clash_proxy_dict['grpc-opts']['serviceName'] == '':
                            clash_proxy_dict['grpc-opts'] = None

                elif node.startswith("ss://"):
                    decoded_part = node[len("ss://"):].split('#', 1)[0]
                    try:
                        # SSé“¾æ¥å¯èƒ½åŒ…å«Base64ç¼–ç çš„ç”¨æˆ·ä¿¡æ¯
                        decoded_info = base64.b64decode(decoded_part.encode('utf-8')).decode('utf-8')
                        parts = decoded_info.split('@', 1)
                        method_password = parts[0].split(':', 1)
                        method = method_password[0]
                        password = method_password[1] if len(method_password) > 1 else ''
                        server_port = parts[1].split(':', 1)
                        server = server_port[0]
                        port = int(server_port[1])
                        
                        clash_proxy_dict = {
                            'name': str(parsed_url.fragment or 'SS Node'),
                            'type': 'ss',
                            'server': server,
                            'port': port,
                            'cipher': method,
                            'password': password,
                        }
                        # å¤„ç† SS æ’ä»¶
                        if 'plugin' in parsed_url.query:
                            query_params = parse_qs(parsed_url.query)
                            clash_proxy_dict['plugin'] = query_params.get('plugin', [''])[0]
                            plugin_opts = query_params.get('plugin_opts', [''])[0]
                            if plugin_opts:
                                clash_proxy_dict['plugin-opts'] = {}
                                for opt in plugin_opts.split(';'):
                                    if '=' in opt:
                                        k, v = opt.split('=', 1)
                                        clash_proxy_dict['plugin-opts'][k] = v
                    except Exception as e:
                        logging.warning(f"SSèŠ‚ç‚¹è§£æå¤±è´¥: {node[:50]}... - {e}")
                        clash_proxy_dict = None

                elif node.startswith("vless://"):
                    parsed = urlparse(node)
                    uuid = parsed.username
                    server = parsed.hostname
                    port = parsed.port
                    query = parse_qs(parsed.query)
                    
                    clash_proxy_dict = {
                        'name': str(parsed.fragment or 'VLESS Node'),
                        'type': 'vless',
                        'server': server,
                        'port': port,
                        'uuid': uuid,
                        'network': query.get('type', ['tcp'])[0],
                        'tls': True if query.get('security', [''])[0] == 'tls' else False,
                        'skip-cert-verify': query.get('flow', [''])[0] == 'xtls-rprx-direct' or query.get('allowInsecure', ['0'])[0] == '1',
                        'servername': query.get('sni', [server])[0],
                        'flow': query.get('flow', [''])[0],
                        'xudp': query.get('xudp', [''])[0] == '1', # xudp
                        'udp-over-tcp': query.get('udp_over_tcp', [''])[0] == 'true', # udp-over-tcp
                    }
                    if query.get('type', [''])[0] == 'ws':
                        clash_proxy_dict['ws-opts'] = {
                            'path': query.get('path', ['/'])[0],
                            'headers': {'Host': query.get('host', [''])[0]}
                        }
                        if clash_proxy_dict['ws-opts']['headers']['Host'] == '':
                             del clash_proxy_dict['ws-opts']['headers']['Host']
                             if not clash_proxy_dict['ws-opts']['headers']:
                                 del clash_proxy_dict['ws-opts']['headers']
                        if clash_proxy_dict['ws-opts']['path'] == '/' and not clash_proxy_dict['ws-opts'].get('headers'):
                            clash_proxy_dict['ws-opts'] = None
                    if query.get('type', [''])[0] == 'grpc':
                        clash_proxy_dict['grpc-opts'] = {
                            'serviceName': query.get('serviceName', [''])[0]
                        }
                        if clash_proxy_dict['grpc-opts']['serviceName'] == '':
                            clash_proxy_dict['grpc-opts'] = None
                        
                elif node.startswith("hysteria://") or node.startswith("hy://"):
                    parsed = urlparse(node)
                    server = parsed.hostname
                    port = parsed.port
                    query = parse_qs(parsed.query)

                    clash_proxy_dict = {
                        'name': str(parsed.fragment or 'Hysteria Node'),
                        'type': 'hysteria',
                        'server': server,
                        'port': port,
                        'auth_str': query.get('auth', [''])[0], # Hysteria 1 ç”¨ auth_str
                        'alpn': query.get('alpn', [''])[0].split(','),
                        'network': query.get('protocol', ['udp'])[0],
                        'skip-cert-verify': query.get('insecure', ['0'])[0] == '1',
                        'servername': query.get('peer', [server])[0],
                        'up': int(query.get('up_mbps', ['0'])[0]),
                        'down': int(query.get('down_mbps', ['0'])[0])
                    }
                    if not clash_proxy_dict['alpn'] or clash_proxy_dict['alpn'] == ['']: # ç¡®ä¿ alpn éç©º
                        del clash_proxy_dict['alpn']
                    if clash_proxy_dict['up'] == 0: del clash_proxy_dict['up']
                    if clash_proxy_dict['down'] == 0: del clash_proxy_dict['down']
                    
                elif node.startswith("hysteria2://") or node.startswith("hy2://"):
                    parsed = urlparse(node)
                    password = parsed.username
                    server = parsed.hostname
                    port = parsed.port
                    query = parse_qs(parsed.query)

                    clash_proxy_dict = {
                        'name': str(parsed.fragment or 'Hysteria2 Node'),
                        'type': 'hysteria2',
                        'server': server,
                        'port': port,
                        'password': password,
                        'obfs': query.get('obfs', [''])[0],
                        'obfs-password': query.get('obfsParam', [''])[0],
                        'tls': True,
                        'skip-cert-verify': query.get('insecure', ['0'])[0] == '1',
                        'servername': query.get('sni', [server])[0],
                        'alpn': query.get('alpn', [''])[0].split(',')
                    }
                    if not clash_proxy_dict['obfs']: del clash_proxy_dict['obfs']
                    if not clash_proxy_dict['obfs-password']: del clash_proxy_dict['obfs-password']
                    if not clash_proxy_dict['alpn'] or clash_proxy_dict['alpn'] == ['']:
                        del clash_proxy_dict['alpn']

            except Exception as e:
                logging.warning(f"URLèŠ‚ç‚¹è½¬æ¢ä¸ºClashå­—å…¸å¤±è´¥: {node[:50]}... - {e}")
                clash_proxy_dict = None

        if clash_proxy_dict:
            # æ£€æŸ¥èŠ‚ç‚¹åç§°æ˜¯å¦åŒ…å«åˆ é™¤å…³é”®è¯
            name_to_check = str(node_raw_name or clash_proxy_dict.get('name', '')) # ä¼˜å…ˆä½¿ç”¨åŸå§‹åç§°è¿›è¡Œæ£€æŸ¥
            
            should_delete_node = False
            for keyword in DELETE_KEYWORDS:
                try:
                    if keyword.lower() in name_to_check.lower():
                        logging.info(f"èŠ‚ç‚¹ '{name_to_check}' åŒ…å«åˆ é™¤å…³é”®è¯ '{keyword}'ï¼Œå·²è·³è¿‡ã€‚")
                        should_delete_node = True
                        break
                except AttributeError as e: # é˜²æ­¢ name_to_check ä¸æ˜¯å­—ç¬¦ä¸²
                    logging.error(f"æ£€æŸ¥åˆ é™¤å…³é”®è¯æ—¶å‡ºé”™: name_to_check={name_to_check}, type={type(name_to_check)}, node={clash_proxy_dict.get('name', 'Unknown')} - {e}")
                    should_delete_node = True
                    break
            
            if should_delete_node:
                continue

            # æ£€æŸ¥æœåŠ¡å™¨åœ°å€æ˜¯å¦æœ‰æ•ˆ
            server = clash_proxy_dict.get('server', '')
            if server and not (is_valid_ip_address(server) or re.match(r'^[a-zA-Z0-9\-\.]+$', server)):
                logging.warning(f"è·³è¿‡æ— æ•ˆæœåŠ¡å™¨åœ°å€çš„èŠ‚ç‚¹: {server} in {clash_proxy_dict.get('name', 'Unknown')}")
                continue

            # ç”ŸæˆæŒ‡çº¹å¹¶è¿›è¡Œå»é‡
            fingerprint = _generate_node_fingerprint(clash_proxy_dict)
            if fingerprint and fingerprint not in unique_node_fingerprints:
                unique_node_fingerprints.add(fingerprint)
                # æ¸…ç†èŠ‚ç‚¹åç§°ï¼Œå¹¶æ·»åŠ åºå·
                clash_proxy_dict['name'] = clean_node_name(
                    clash_proxy_dict.get('name', f"{clash_proxy_dict.get('type', 'Unknown')} {clash_proxy_dict.get('server', '')}:{clash_proxy_dict.get('port', '')}"),
                    index=len(final_clash_proxies) + 1 # ä½¿ç”¨å½“å‰å·²æ”¶é›†åˆ°çš„èŠ‚ç‚¹æ•°é‡ä½œä¸ºåºå·
                )
                final_clash_proxies.append(clash_proxy_dict)
            else:
                logging.debug(f"é‡å¤èŠ‚ç‚¹ï¼ˆæŒ‰æŒ‡çº¹ï¼‰ï¼š{clash_proxy_dict.get('name', '')} - {fingerprint}")

    return final_clash_proxies

# --- ä¸»ç¨‹åºæµç¨‹ ---

# ä»ç¯å¢ƒå˜é‡ä¸­è·å– URL_SOURCE
URL_SOURCE = os.environ.get("URL_SOURCE")
print(f"è°ƒè¯•ä¿¡æ¯ - è¯»å–åˆ°çš„ URL_SOURCE å€¼: {URL_SOURCE}")

if not URL_SOURCE:
    print("é”™è¯¯ï¼šç¯å¢ƒå˜é‡ 'URL_SOURCE' æœªè®¾ç½®ã€‚è¯·è®¾ç½®ä¸€ä¸ªåŒ…å«è®¢é˜…é“¾æ¥çš„è¿œç¨‹æ–‡æœ¬æ–‡ä»¶URLã€‚")
    exit(1)

# åˆ›å»ºè¾“å‡ºç›®å½•
os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)
os.makedirs(os.path.dirname(STATISTICS_FILE), exist_ok=True)

# é˜¶æ®µä¸€ï¼šè·å–åŸå§‹URL/å­—ç¬¦ä¸²åˆ—è¡¨
raw_urls_from_source = get_url_list_from_remote(URL_SOURCE)

urls_to_fetch = set() # éœ€è¦é€šè¿‡HTTP/HTTPSè¯·æ±‚çš„URL
url_statistics = [] # ç”¨äºè®°å½•å¤„ç†ç»Ÿè®¡
successful_urls = [] # æˆåŠŸå¤„ç†çš„URLåˆ—è¡¨
failed_urls = [] # å¤±è´¥çš„URLåˆ—è¡¨
all_parsed_nodes_raw = [] # æ‰€æœ‰è§£æåˆ°çš„åŸå§‹èŠ‚ç‚¹ï¼ˆæœªå»é‡å’Œæ ‡å‡†åŒ–ï¼‰

print("\n--- é¢„å¤„ç†åŸå§‹URL/å­—ç¬¦ä¸²åˆ—è¡¨ ---")
for entry in raw_urls_from_source:
    if is_valid_url(entry):
        # å¦‚æœæ˜¯æœ‰æ•ˆçš„HTTP/HTTPS URLï¼ŒåŠ å…¥å¾…è¯·æ±‚åˆ—è¡¨
        urls_to_fetch.add(entry)
    else:
        # å¦‚æœä¸æ˜¯æœ‰æ•ˆçš„URLï¼Œå°è¯•ç›´æ¥è§£æå…¶å†…å®¹ï¼ˆå¯èƒ½æ˜¯Base64ç¼–ç çš„èŠ‚ç‚¹åˆ—è¡¨æˆ–Clashé…ç½®ç‰‡æ®µï¼‰
        print(f"å‘ç°éHTTP/HTTPSæ¡ç›®ï¼Œå°è¯•ç›´æ¥è§£æ: {entry[:80]}...")
        parsed_nodes = parse_content_to_nodes(entry)
        if parsed_nodes:
            all_parsed_nodes_raw.extend(parsed_nodes)
            stat_entry = {'URL': entry, 'èŠ‚ç‚¹æ•°é‡': len(parsed_nodes), 'çŠ¶æ€': 'ç›´æ¥è§£ææˆåŠŸ', 'é”™è¯¯ä¿¡æ¯': '', 'çŠ¶æ€ç ': None}
            url_statistics.append(stat_entry)
            successful_urls.append(entry)
        else:
            stat_entry = {'URL': entry, 'èŠ‚ç‚¹æ•°é‡': 0, 'çŠ¶æ€': 'ç›´æ¥è§£æå¤±è´¥', 'é”™è¯¯ä¿¡æ¯': 'éURLä¸”æ— æ³•è§£æä¸ºèŠ‚ç‚¹', 'çŠ¶æ€ç ': None}
            url_statistics.append(stat_entry)
            failed_urls.append(entry)

print("\n--- é˜¶æ®µä¸€ï¼šå¹¶è¡Œè·å–å¹¶åˆå¹¶æ‰€æœ‰è®¢é˜…é“¾æ¥ä¸­çš„èŠ‚ç‚¹ ---")
total_urls_to_process_via_http = len(urls_to_fetch)

if total_urls_to_process_via_http > 0:
    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œè¯·æ±‚URLï¼Œæé«˜æ•ˆç‡
    # max_workers=16 æ˜¯ä¸€ä¸ªå¸¸ç”¨å€¼ï¼Œå¯ä»¥æ ¹æ®ç½‘ç»œå’ŒCPUæƒ…å†µè°ƒæ•´
    with ThreadPoolExecutor(max_workers=16) as executor:
        future_to_url = {executor.submit(fetch_and_parse_url, url): url for url in urls_to_fetch}
        # tqdm ç”¨äºæ˜¾ç¤ºè¿›åº¦æ¡
        for future in tqdm(as_completed(future_to_url), total=total_urls_to_process_via_http, desc="é€šè¿‡HTTP/HTTPSè¯·æ±‚å¹¶è§£æèŠ‚ç‚¹", mininterval=1.0):
            url = future_to_url[future]
            nodes, success, error_message, status_code = future.result()

            stat_entry = {
                'URL': url,
                'èŠ‚ç‚¹æ•°é‡': len(nodes),
                'çŠ¶æ€': 'æˆåŠŸ' if success else 'å¤±è´¥',
                'é”™è¯¯ä¿¡æ¯': error_message if error_message else '',
                'çŠ¶æ€ç ': status_code
            }
            url_statistics.append(stat_entry)

            if success:
                successful_urls.append(url)
                all_parsed_nodes_raw.extend(nodes)
                print(f"æˆåŠŸå¤„ç† URL: {url}, èŠ‚ç‚¹æ•°: {len(nodes)}, çŠ¶æ€ç : {status_code}")
            else:
                failed_urls.append(url)
                print(f"å¤±è´¥ URL: {url}, é”™è¯¯: {error_message}")

            # æå‰ç»ˆæ­¢æœºåˆ¶ï¼šå¦‚æœå·²æ”¶é›†åˆ°è¶³å¤Ÿå¤šçš„åŸå§‹èŠ‚ç‚¹ï¼ˆMAX_SUCCESSçš„ä¸¤å€ï¼Œè€ƒè™‘åˆ°å»é‡æŸå¤±ï¼‰ï¼Œåˆ™åœæ­¢è¯·æ±‚
            if len(all_parsed_nodes_raw) >= MAX_SUCCESS * 2:
                print(f"å·²æ”¶é›†è¶³å¤ŸåŸå§‹èŠ‚ç‚¹ ({len(all_parsed_nodes_raw)})ï¼Œè¾¾åˆ° MAX_SUCCESS * 2ï¼Œæå‰ç»ˆæ­¢åç»­è¯·æ±‚ã€‚")
                # æ˜¾å¼å…³é—­çº¿ç¨‹æ± ä¸­çš„çº¿ç¨‹
                executor._threads.clear() 
                break

# å¯¹æ‰€æœ‰æ”¶é›†åˆ°çš„åŸå§‹èŠ‚ç‚¹è¿›è¡Œå»é‡å’Œæ ‡å‡†åŒ–
final_unique_clash_proxies = deduplicate_and_standardize_nodes(all_parsed_nodes_raw)

# å°†å»é‡åçš„åŸå§‹èŠ‚ç‚¹ï¼ˆå­—å…¸å½¢å¼ï¼‰å†™å…¥ä¸´æ—¶æ–‡ä»¶ï¼Œæ–¹ä¾¿è°ƒè¯•
with open(TEMP_MERGED_NODES_RAW_FILE, 'w', encoding='utf-8') as temp_file:
    for node in final_unique_clash_proxies:
        if isinstance(node, dict):
            temp_file.write(json.dumps(node, ensure_ascii=False) + '\n')
        else: # ç†è®ºä¸Šåˆ°è¿™é‡Œéƒ½åº”è¯¥æ˜¯dictäº†ï¼Œä»¥é˜²ä¸‡ä¸€
            temp_file.write(str(node).strip() + '\n') # ç¡®ä¿å†™å…¥çš„æ˜¯å­—ç¬¦ä¸²

print(f"\né˜¶æ®µä¸€å®Œæˆã€‚åˆå¹¶åˆ° {len(final_unique_clash_proxies)} ä¸ªå”¯ä¸€Clashä»£ç†å­—å…¸ï¼Œå·²ä¿å­˜è‡³ {TEMP_MERGED_NODES_RAW_FILE}")

# å†™å…¥ç»Ÿè®¡æ•°æ®å’ŒURLåˆ—è¡¨
write_statistics_to_csv(url_statistics, STATISTICS_FILE)
write_urls_to_file(successful_urls, SUCCESS_URLS_FILE)
write_urls_to_file(failed_urls, FAILED_URLS_FILE)

print("\n--- é˜¶æ®µäºŒï¼šè¾“å‡ºæœ€ç»ˆ Clash YAML é…ç½® ---")

# ç¡®ä¿è¾“å‡ºæ–‡ä»¶æ˜¯ .yaml æˆ– .yml æ‰©å±•å
if not OUTPUT_FILE.endswith(('.yaml', '.yml')):
    OUTPUT_FILE = os.path.splitext(OUTPUT_FILE)[0] + '.yaml'

# å–å‡ºæœ€å¤š MAX_SUCCESS ä¸ªèŠ‚ç‚¹è¿›è¡Œè¾“å‡º
proxies_to_output = final_unique_clash_proxies[:MAX_SUCCESS]

# æ„å»ºä»£ç†ç»„çš„åç§°åˆ—è¡¨
proxy_names_in_group = []
for node in proxies_to_output:
    if isinstance(node, dict) and 'name' in node:
        proxy_names_in_group.append(node['name'])
    else:
        # å…œåº•å¤„ç†ï¼Œç¡®ä¿å³ä½¿æ²¡æœ‰nameä¹Ÿèƒ½æ·»åŠ åˆ°ç»„
        proxy_names_in_group.append(f"{node.get('type', 'Unknown')} {node.get('server', '')}")

# æ„å»ºæœ€ç»ˆçš„Clashé…ç½®å­—å…¸
clash_config = {
    'proxies': proxies_to_output,
    'proxy-groups': [
        {
            'name': 'ğŸš€ èŠ‚ç‚¹é€‰æ‹©', # æ‰‹åŠ¨é€‰æ‹©èŠ‚ç‚¹ç»„
            'type': 'select',
            'proxies': ['DIRECT'] + proxy_names_in_group # åŒ…å«ç›´è¿é€‰é¡¹
        },
        {
            'name': 'â™»ï¸ è‡ªåŠ¨é€‰æ‹©', # è‡ªåŠ¨æµ‹é€Ÿé€‰æ‹©æœ€ä½³èŠ‚ç‚¹ç»„
            'type': 'url-test',
            'url': 'http://www.gstatic.com/generate_204', # Googleçš„æ— å†…å®¹å“åº”é¡µé¢ï¼Œå¸¸ç”¨äºæµ‹é€Ÿ
            'interval': 300, # æµ‹é€Ÿé—´éš”300ç§’
            'proxies': proxy_names_in_group
        }
        # å¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ æ›´å¤šçš„ä»£ç†ç»„å’Œè§„åˆ™
    ],
    'rules': [
        # ä¾‹å¦‚ï¼š
        # 'DOMAIN-SUFFIX,google.com,â™»ï¸ è‡ªåŠ¨é€‰æ‹©',
        # 'GEOIP,CN,DIRECT',
        'MATCH,ğŸš€ èŠ‚ç‚¹é€‰æ‹©' # é»˜è®¤è§„åˆ™ï¼Œæ‰€æœ‰æœªåŒ¹é…çš„æµé‡èµ°èŠ‚ç‚¹é€‰æ‹©ç»„
    ]
}

success_count = len(proxies_to_output)

# å°†Clashé…ç½®å†™å…¥YAMLæ–‡ä»¶
try:
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as out_file:
        # allow_unicode=True ç¡®ä¿ä¸­æ–‡æ­£ç¡®ç¼–ç 
        # default_flow_style=False ç¡®ä¿è¾“å‡ºä¸ºå—æ ·å¼ï¼Œæé«˜å¯è¯»æ€§
        # sort_keys=False ä¿æŒå­—å…¸æ’å…¥é¡ºåºï¼ˆå¯¹äºproxieså’Œproxy-groupså¾ˆé‡è¦ï¼‰
        yaml.dump(clash_config, out_file, allow_unicode=True, default_flow_style=False, sort_keys=False)
    print(f"æœ€ç»ˆ Clash YAML é…ç½®å·²ä¿å­˜è‡³ï¼š{OUTPUT_FILE}")
except Exception as e:
    logging.error(f"å†™å…¥æœ€ç»ˆ Clash YAML æ–‡ä»¶å¤±è´¥: {e}")
    print(f"é”™è¯¯ï¼šå†™å…¥æœ€ç»ˆ Clash YAML æ–‡ä»¶å¤±è´¥: {e}")

# æ¸…ç†ä¸´æ—¶æ–‡ä»¶
if os.path.exists(TEMP_MERGED_NODES_RAW_FILE):
    os.remove(TEMP_MERGED_NODES_RAW_FILE)
    print(f"å·²åˆ é™¤ä¸´æ—¶æ–‡ä»¶ï¼š{TEMP_MERGED_NODES_RAW_FILE}")

# æ‰“å°æœ€ç»ˆè¿è¡Œæ‘˜è¦
print("\n" + "=" * 50)
print("æœ€ç»ˆç»“æœï¼š")
print(f"åŸå§‹æ¥æºæ€»æ¡ç›®æ•°ï¼š{len(raw_urls_from_source)}")
print(f"å…¶ä¸­éœ€è¦HTTP/HTTPSè¯·æ±‚çš„è®¢é˜…é“¾æ¥æ•°ï¼š{len(urls_to_fetch)}")
print(f"å…¶ä¸­ç›´æ¥è§£æçš„éURLå­—ç¬¦ä¸²æ•°ï¼š{len(raw_urls_from_source) - len(urls_to_fetch)}")
print(f"æˆåŠŸå¤„ç†çš„URL/å­—ç¬¦ä¸²æ€»æ•°ï¼š{len(successful_urls)}")
print(f"å¤±è´¥çš„URL/å­—ç¬¦ä¸²æ€»æ•°ï¼š{len(failed_urls)}")
print(f"åˆæ­¥èšåˆçš„åŸå§‹èŠ‚ç‚¹æ•°ï¼ˆå»é‡å’Œè¿‡æ»¤å‰ï¼‰ï¼š{len(all_parsed_nodes_raw)}")
print(f"å»é‡ã€æ ‡å‡†åŒ–å’Œè¿‡æ»¤åçš„å”¯ä¸€Clashä»£ç†æ•°ï¼š{len(final_unique_clash_proxies)}")
print(f"æœ€ç»ˆè¾“å‡ºåˆ°Clash YAMLæ–‡ä»¶çš„èŠ‚ç‚¹æ•°ï¼š{success_count}")
if len(final_unique_clash_proxies) > 0:
    print(f"æœ€ç»ˆæœ‰æ•ˆå†…å®¹ç‡ï¼ˆç›¸å¯¹äºå»é‡è¿‡æ»¤åï¼‰ï¼š{success_count/len(final_unique_clash_proxies):.1%}")
if success_count < MAX_SUCCESS:
    print(f"è­¦å‘Šï¼šæœªèƒ½è¾¾åˆ°ç›®æ ‡æ•°é‡ {MAX_SUCCESS}ï¼ŒåŸå§‹åˆ—è¡¨å¯èƒ½æœ‰æ•ˆURL/èŠ‚ç‚¹ä¸è¶³ï¼Œæˆ–éƒ¨åˆ†URLè·å–å¤±è´¥ã€‚")
print(f"ç»“æœæ–‡ä»¶å·²ä¿å­˜è‡³ï¼š{OUTPUT_FILE}")
print(f"ç»Ÿè®¡æ•°æ®å·²ä¿å­˜è‡³ï¼š{STATISTICS_FILE}")
print(f"æˆåŠŸURLåˆ—è¡¨å·²ä¿å­˜è‡³ï¼š{SUCCESS_URLS_FILE}")
print(f"å¤±è´¥URLåˆ—è¡¨å·²ä¿å­˜è‡³ï¼š{FAILED_URLS_FILE}")
print("=" * 50)
