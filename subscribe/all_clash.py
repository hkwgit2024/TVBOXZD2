# -*- coding: utf-8 -*-
import os
import requests
from urllib.parse import urlparse, parse_qs, urlencode
import base64
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
import argparse
import re
import yaml
import json
import csv
import hashlib
import ipaddress

# é…ç½®æ—¥å¿—
logging.basicConfig(filename='error.log', level=logging.ERROR,
                    format='%(asctime)s - %(levelname)s - %(message)s')

# è¯·æ±‚å¤´
headers = {
    'User-Agent': (
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
        'AppleWebKit/537.36 (KHTML, like Gecko) '
        'Chrome/91.0.4472.124 Safari/537.36'
    ),
    'Accept-Encoding': 'gzip, deflate'
}

# å‘½ä»¤è¡Œå‚æ•°è§£æ
parser = argparse.ArgumentParser(description="URLå†…å®¹è·å–è„šæœ¬ï¼Œæ”¯æŒå¤šä¸ªURLæ¥æºå’ŒèŠ‚ç‚¹è§£æ")
parser.add_argument('--max_success', type=int, default=99999, help="ç›®æ ‡æˆåŠŸæ•°é‡")
parser.add_argument('--timeout', type=int, default=60, help="è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰")
parser.add_argument('--output', type=str, default='data/all_clash.yaml', help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
args = parser.parse_args()

# å…¨å±€å˜é‡ï¼Œä»å‘½ä»¤è¡Œå‚æ•°æˆ–é»˜è®¤å€¼è·å–
MAX_SUCCESS = args.max_success
TIMEOUT = args.timeout
OUTPUT_FILE = args.output
TEMP_MERGED_NODES_RAW_FILE = 'temp_merged_nodes_raw.txt'
STATISTICS_FILE = 'data/url_statistics.csv'
SUCCESS_URLS_FILE = 'data/successful_urls.txt'
FAILED_URLS_FILE = 'data/failed_urls.txt'

# å®šä¹‰å¦‚æœèŠ‚ç‚¹åç§°åŒ…å«è¿™äº›å…³é”®è¯ï¼Œåˆ™ç›´æ¥åˆ é™¤è¯¥èŠ‚ç‚¹
DELETE_KEYWORDS = [
    'å‰©ä½™æµé‡', 'å¥—é¤åˆ°æœŸ', 'æµé‡', 'åˆ°æœŸ', 'è¿‡æœŸ', 'å…è´¹', 'è¯•ç”¨', 'ä½“éªŒ', 'é™æ—¶', 'é™åˆ¶',
    'å·²ç”¨', 'å¯ç”¨', 'ä¸è¶³', 'åˆ°æœŸæ—¶é—´', 'å€ç‡', 'è¿”åˆ©', 'å……å€¼', 'ç»­è´¹', 'ç”¨é‡', 'è®¢é˜…'
]

def is_valid_url(url):
    """éªŒè¯URLæ ¼å¼æ˜¯å¦åˆæ³•ï¼Œä»…æ¥å— http æˆ– https æ–¹æ¡ˆ"""
    try:
        result = urlparse(url)
        return all([result.scheme in ['http', 'https'], result.netloc])
    except Exception:
        return False

def is_valid_ip_address(host):
    """éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆçš„ IPv4 æˆ– IPv6 åœ°å€"""
    try:
        ipaddress.ip_address(host)
        return True
    except ValueError:
        try:
            # å¯¹äº IPv6 åœ°å€ï¼Œæ£€æŸ¥æ˜¯å¦è¢«æ­£ç¡®åŒ…è£¹åœ¨æ–¹æ‹¬å·ä¸­
            if host.startswith('[') and host.endswith(']'):
                ipaddress.ip_address(host[1:-1])
                return True
            return False
        except ValueError:
            return False

def get_url_list_from_remote(url_source):
    """ä»ç»™å®šçš„å…¬å¼€ç½‘å€è·å– URL åˆ—è¡¨"""
    try:
        response = requests.get(url_source, headers=headers, timeout=10)
        response.raise_for_status()
        text_content = response.text.strip()
        raw_urls = [line.strip() for line in text_content.splitlines() if line.strip()]
        print(f"ä» {url_source} è·å–åˆ° {len(raw_urls)} ä¸ªURL")
        return raw_urls
    except Exception as e:
        logging.error(f"è·å–URLåˆ—è¡¨å¤±è´¥: {url_source} - {e}")
        return []

def parse_content_to_nodes(content):
    """
    ä»æ–‡æœ¬å†…å®¹ä¸­è§£æå‡ºå„ç§ç±»å‹çš„èŠ‚ç‚¹ã€‚
    è¿”å›çš„èŠ‚ç‚¹æ ¼å¼ä¿æŒåŸå§‹å­—ç¬¦ä¸²æˆ–å­—å…¸å½¢å¼ã€‚
    """
    if not content:
        return []

    found_nodes = []
    processed_content = content

    # 1. å°è¯• Base64 è§£ç 
    try:
        decoded_bytes = base64.b64decode(content)
        processed_content = decoded_bytes.decode('utf-8')
        logging.info("å†…å®¹æˆåŠŸ Base64 è§£ç ã€‚")
    except Exception:
        pass

    # 2. å°è¯• YAML è§£æ (ä¸»è¦ç”¨äº Clash é…ç½®)
    try:
        parsed_data = yaml.safe_load(processed_content)
        if isinstance(parsed_data, dict) and 'proxies' in parsed_data and isinstance(parsed_data['proxies'], list):
            for proxy_entry in parsed_data['proxies']:
                if isinstance(proxy_entry, dict):
                    found_nodes.append(proxy_entry)
                elif isinstance(proxy_entry, str) and any(proxy_entry.startswith(p + '://') for p in ["vmess", "trojan", "ss", "ssr", "vless", "hy", "hy2", "hysteria", "hysteria2"]):
                    found_nodes.append(proxy_entry.strip())
            logging.info("å†…å®¹æˆåŠŸè§£æä¸º Clash YAMLã€‚")
        elif isinstance(parsed_data, list):
            for item in parsed_data:
                if isinstance(item, str):
                    found_nodes.append(item.strip())
                elif isinstance(item, dict):
                    found_nodes.append(item)
            logging.info("å†…å®¹æˆåŠŸè§£æä¸º YAML åˆ—è¡¨ã€‚")
    except yaml.YAMLError:
        pass
    except Exception as e:
        logging.error(f"YAML è§£æå¤±è´¥: {e}")
        pass

    # 3. é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼æå–èŠ‚ç‚¹ï¼ˆå¤„ç†æ˜æ–‡ã€éæ ‡å‡†æ ¼å¼ç­‰ï¼‰
    node_pattern = re.compile(
        r'(vmess://\S+|'
        r'trojan://\S+|'
        r'ss://\S+|'
        r'ssr://\S+|'
        r'vless://\S+|'
        r'hy://\S+|'
        r'hy2://\S+|'
        r'hysteria://\S+|'
        r'hysteria2://\S+)'
    )
    
    matches = node_pattern.findall(content)
    for match in matches:
        found_nodes.append(match.strip())
    
    if content != processed_content:
        matches_decoded = node_pattern.findall(processed_content)
        for match in matches_decoded:
            found_nodes.append(match.strip())

    return found_nodes

def fetch_and_parse_url(url):
    """
    è·å–URLå†…å®¹å¹¶è§£æå‡ºèŠ‚ç‚¹ã€‚
    è¿”å›ä¸€ä¸ªå…ƒç»„ï¼š(èŠ‚ç‚¹åˆ—è¡¨, æ˜¯å¦æˆåŠŸ, é”™è¯¯ä¿¡æ¯(å¦‚æœå¤±è´¥))
    """
    try:
        resp = requests.get(url, headers=headers, timeout=TIMEOUT)
        resp.raise_for_status()
        content = resp.text.strip()
        
        if len(content) < 10:
            logging.warning(f"è·å–åˆ°å†…å®¹è¿‡çŸ­ï¼Œå¯èƒ½æ— æ•ˆ: {url}")
            return [], False, "å†…å®¹è¿‡çŸ­"
        
        nodes = parse_content_to_nodes(content)
        return nodes, True, None
    except requests.exceptions.Timeout:
        logging.error(f"è¯·æ±‚è¶…æ—¶: {url}")
        return [], False, "è¯·æ±‚è¶…æ—¶"
    except requests.exceptions.RequestException as e:
        logging.error(f"è¯·æ±‚å¤±è´¥: {url} - {e}")
        return [], False, f"è¯·æ±‚å¤±è´¥: {e}"
    except Exception as e:
        logging.error(f"å¤„ç†URLå¼‚å¸¸: {url} - {e}")
        return [], False, f"æœªçŸ¥å¼‚å¸¸: {e}"

def write_statistics_to_csv(statistics_data, filename):
    """å°†ç»Ÿè®¡æ•°æ®å†™å…¥CSVæ–‡ä»¶"""
    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
        fieldnames = ['URL', 'èŠ‚ç‚¹æ•°é‡', 'çŠ¶æ€', 'é”™è¯¯ä¿¡æ¯']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

        writer.writeheader()
        for row in statistics_data:
            writer.writerow(row)
    print(f"ç»Ÿè®¡æ•°æ®å·²ä¿å­˜è‡³ï¼š{filename}")

def write_urls_to_file(urls, filename):
    """å°†URLåˆ—è¡¨å†™å…¥æ–‡ä»¶"""
    with open(filename, 'w', encoding='utf-8') as f:
        for url in urls:
            f.write(url + '\n')
    print(f"URLåˆ—è¡¨å·²ä¿å­˜è‡³ï¼š{filename}")

def clean_node_name(name):
    """
    æ¸…ç†èŠ‚ç‚¹åç§°ï¼Œç§»é™¤å†—ä½™ä¿¡æ¯ï¼Œåªä¿ç•™æ ¸å¿ƒå…³é”®å­—ã€‚
    """
    if not isinstance(name, str):
        return str(name)

    cleaned_name = name.strip()

    # 1. ç§»é™¤å„ç§æ‹¬å·åŠå…¶å†…éƒ¨å†…å®¹ (åŒ…æ‹¬å…¨è§’å’ŒåŠè§’)
    cleaned_name = re.sub(r'ã€[^ã€‘]*ã€‘', '', cleaned_name)
    cleaned_name = re.sub(r'\[[^\]]*\]', '', cleaned_name)
    cleaned_name = re.sub(r'\([^\)]*\)', '', cleaned_name)
    cleaned_name = re.sub(r'ï¼ˆ[^ï¼‰]*ï¼‰', '', cleaned_name)
    cleaned_name = re.sub(r'\{[^}]*\}', '', cleaned_name)
    cleaned_name = re.sub(r'ï¼œ[^ï¼]*ï¼', '', cleaned_name)
    cleaned_name = re.sub(r'<[^>]*>', '', cleaned_name)

    # 2. ç§»é™¤å¸¸è§çš„å†—ä½™å…³é”®è¯ï¼ˆä¸åŒ…å«åœ¨ DELETE_KEYWORDS ä¸­çš„ï¼‰
    redundant_keywords_to_remove = [
        r'\[\d+\]', # [1], [2] è¿™ç§åºå·
        r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}', # IPåœ°å€
        r'x\d+', # x1, x2 ç­‰å€ç‡æ ‡è¯†
        r'\d+%', # 100% è¿™ç§ç™¾åˆ†æ¯”
        r'\d{4}-\d{2}-\d{2}', # æ—¥æœŸ YYYY-MM-DD
        r'\d{2}-\d{2}', # æ—¥æœŸ MM-DD
        r'IPLC', r'IEPL', r'NAT', r'UDP', r'TCP', r'éš§é“', r'ç›´è¿', r'ä¸­è½¬', r'å›å›½',
        r'çº¿è·¯', r'å…¥å£', r'å‡ºå£', r'èŠ‚ç‚¹', r'è´Ÿè½½å‡è¡¡', r'æ™®é€š', r'ä¼˜è´¨', r'é«˜çº§', r'è¶…æ¸…',
        r'ç§’æ€', r'æ´»åŠ¨', r'æ–°å¹´', r'ç¦åˆ©', r'VIP', r'VIP\d+', r'Pro', r'Lite', r'Plus',
        r'SS', r'SSR', r'VMESS', r'VLESS', r'TROJAN', r'HYSTERIA', r'HYSTERIA2', r'HY', r'HY2', # åè®®å
        r'è‡ªåŠ¨', r'æ‰‹åŠ¨', r'è‡ªé€‰', r'é¦™æ¸¯', r'å°æ¹¾', r'æ—¥æœ¬', r'éŸ©å›½', r'æ–°åŠ å¡', r'ç¾å›½', r'è‹±å›½', r'å¾·å›½',
        r'France', r'Canada', r'Australia', r'Russia', r'Brazil', r'India', r'UAE',
        r'HK', r'TW', r'JP', r'KR', r'SG', r'US', r'UK', r'DE', r'FR', r'CA', r'AU', r'RU', r'BR', r'IN', r'AE',
        r'åœ°åŒº', r'åŸå¸‚', r'ç¼–å·', r'åºå·', r'æ•°å­—', r'å·', r'æœ', r'ç¾¤', r'ç»„', r'ä¸“çº¿', r'åŠ é€Ÿ',
        r'(\d+ms)', # 100ms è¿™ç§å»¶è¿Ÿæ ‡è®°
        r'(\d+\.\d+kbps)', r'(\d+\.\d+mbps)', r'(\d+kbps)', r'(\d+mbps)', # é€Ÿåº¦æ ‡è®°
        r'\\n', r'\\r', # æ¢è¡Œç¬¦
        r'\d+\.\d+G|\d+G', # æµé‡ä¿¡æ¯
        r'\[\d+\]' # å†æ¬¡å»é™¤æ•°å­—åœ¨æ–¹æ‹¬å·å†…
    ]

    for keyword in redundant_keywords_to_remove:
        cleaned_name = re.sub(keyword, ' ', cleaned_name, flags=re.IGNORECASE).strip()

    # 3. ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼ˆåªä¿ç•™æ±‰å­—ã€å­—æ¯ã€æ•°å­—ã€ç‚¹ã€æ¨ªçº¿ã€ä¸‹åˆ’çº¿ã€ç©ºæ ¼ï¼‰
    cleaned_name = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\s\.\-_]', ' ', cleaned_name)

    # 4. åˆå¹¶å¤šä¸ªç©ºæ ¼ä¸ºä¸€ä¸ªï¼Œå¹¶å»é™¤é¦–å°¾ç©ºæ ¼
    cleaned_name = re.sub(r'\s+', ' ', cleaned_name).strip()

    # 5. å¸¸è§ç¼©å†™æˆ–å˜ä½“çš„æ ‡å‡†åŒ–
    cleaned_name = cleaned_name.replace('é¦™æ¸¯', 'HK').replace('å°æ¹¾', 'TW').replace('æ—¥æœ¬', 'JP').replace('æ–°åŠ å¡', 'SG')
    cleaned_name = cleaned_name.replace('ç¾å›½', 'US').replace('è‹±å›½', 'UK').replace('å¾·å›½', 'DE').replace('éŸ©å›½', 'KR')
    cleaned_name = cleaned_name.replace('é©¬æ¥', 'MY').replace('æ³°å›½', 'TH').replace('è²å¾‹å®¾', 'PH').replace('è¶Šå—', 'VN')
    cleaned_name = cleaned_name.replace('å°å°¼', 'ID').replace('å°åº¦', 'IN').replace('æ¾³æ´²', 'AU').replace('åŠ æ‹¿å¤§', 'CA')
    cleaned_name = cleaned_name.replace('ä¿„ç½—æ–¯', 'RU').replace('å·´è¥¿', 'BR').replace('æ„å¤§åˆ©', 'IT').replace('è·å…°', 'NL')
    cleaned_name = cleaned_name.replace('ä¸­å›½', 'CN')

    # 6. æˆªæ–­è¿‡é•¿åç§°ï¼Œä¿ç•™å‰50ä¸ªå­—ç¬¦
    if len(cleaned_name) > 50:
        cleaned_name = cleaned_name[:50] + '...'

    return cleaned_name if cleaned_name else "Unknown Node"

def _generate_node_fingerprint(node):
    """
    ä¸ºClashä»£ç†å­—å…¸æˆ–èŠ‚ç‚¹é“¾æ¥ç”Ÿæˆä¸€ä¸ªå”¯ä¸€çš„æŒ‡çº¹ï¼ˆå“ˆå¸Œå€¼ï¼‰ã€‚
    """
    if isinstance(node, dict):
        fingerprint_data = {
            'type': node.get('type'),
            'server': node.get('server'),
            'port': node.get('port'),
        }

        node_type = node.get('type')
        if node_type == 'vmess':
            fingerprint_data['uuid'] = node.get('uuid') or node.get('id')
            fingerprint_data['alterId'] = node.get('alterId') or node.get('aid')
            fingerprint_data['network'] = node.get('network')
            fingerprint_data['tls'] = node.get('tls')
            fingerprint_data['sni'] = node.get('sni') or node.get('host')
            fingerprint_data['path'] = node.get('path')
        elif node_type == 'trojan':
            fingerprint_data['password'] = node.get('password')
            fingerprint_data['network'] = node.get('network')
            fingerprint_data['tls'] = node.get('tls')
            fingerprint_data['sni'] = node.get('sni') or node.get('host')
            fingerprint_data['skip-cert-verify'] = node.get('skip-cert-verify')
        elif node_type == 'ss':
            fingerprint_data['cipher'] = node.get('cipher')
            fingerprint_data['password'] = node.get('password')
        elif node_type == 'vless':
            fingerprint_data['uuid'] = node.get('uuid') or node.get('id')
            fingerprint_data['network'] = node.get('network')
            fingerprint_data['tls'] = node.get('tls')
            fingerprint_data['sni'] = node.get('sni') or node.get('host')
            fingerprint_data['path'] = node.get('path')
            fingerprint_data['flow'] = node.get('flow')
        elif node_type in ['hysteria', 'hysteria2', 'hy', 'hy2']:
            fingerprint_data['password'] = node.get('password')
            fingerprint_data['obfs'] = node.get('obfs')
            fingerprint_data['obfs-password'] = node.get('obfs-password')
            fingerprint_data['tls'] = node.get('tls')
            fingerprint_data['sni'] = node.get('sni') or node.get('host')
            fingerprint_data['alpn'] = node.get('alpn')
            fingerprint_data['skip-cert-verify'] = node.get('skip-cert-verify')

        normalized_data = {k: str(v).lower().strip() if v is not None else '' for k, v in fingerprint_data.items()}
        stable_json = json.dumps(normalized_data, sort_keys=True, ensure_ascii=False)
        return hashlib.sha256(stable_json.encode('utf-8')).hexdigest()
    elif isinstance(node, str):
        try:
            # éªŒè¯èŠ‚ç‚¹æ˜¯å¦ä¸ºæœ‰æ•ˆçš„åè®® URL
            if not any(node.startswith(p + '://') for p in ["vmess", "trojan", "ss", "ssr", "vless", "hy", "hy2", "hysteria", "hysteria2"]):
                logging.warning(f"æ— æ•ˆçš„èŠ‚ç‚¹åè®®: {node[:50]}...")
                return None

            parsed_url = urlparse(node)
            scheme = parsed_url.scheme
            netloc = parsed_url.netloc
            path = parsed_url.path
            query_params = parse_qs(parsed_url.query)
            
            # éªŒè¯ netloc ä¸­çš„ä¸»æœºéƒ¨åˆ†
            host = netloc.split(':')[0] if ':' in netloc else netloc
            if is_valid_ip_address(host) and host.startswith('[') and host.endswith(']'):
                host = host[1:-1]  # ç§»é™¤ IPv6 åœ°å€çš„æ–¹æ‹¬å·
            elif not is_valid_ip_address(host) and not re.match(r'^[a-zA-Z0-9\-\.]+$', host):
                logging.warning(f"æ— æ•ˆçš„ä¸»æœºå: {host} in {node[:50]}...")
                return None

            normalized_query_params = {}
            for k, v in query_params.items():
                normalized_query_params[k.lower()] = str(v[0]).lower().strip()
            
            fingerprint_parts = [
                scheme,
                host.lower(),
                netloc.lower().split(':')[-1] if ':' in netloc else '',
                path.lower()
            ]

            sorted_query_keys = sorted(normalized_query_params.keys())
            for k in sorted_query_keys:
                if k not in ['name', 'ps', 'remarks', 'info', 'flow', 'usage', 'expire', 'ud', 'up', 'dn', 'package', 'nodeName', 'nodeid', 'ver']:
                    fingerprint_parts.append(f"{k}={normalized_query_params[k]}")

            return hashlib.sha256("".join(fingerprint_parts).encode('utf-8')).hexdigest()
        except Exception as e:
            logging.warning(f"ç”ŸæˆURLèŠ‚ç‚¹æŒ‡çº¹å¤±è´¥: {node[:50]}... - {e}")
            return None
    return None

def deduplicate_and_standardize_nodes(raw_nodes_list):
    """
    å¯¹æ··åˆæ ¼å¼çš„èŠ‚ç‚¹è¿›è¡Œå»é‡ï¼Œæ ‡å‡†åŒ–ä¸ºClash YAMLä»£ç†å­—å…¸ï¼Œå¹¶æ ¹æ®å…³é”®è¯è¿‡æ»¤ã€‚
    è¿”å›ä¸€ä¸ªåˆ—è¡¨ï¼Œå…¶ä¸­åŒ…å«å”¯ä¸€çš„ã€æ ‡å‡†åŒ–çš„Clashä»£ç†å­—å…¸ã€‚
    """
    unique_node_fingerprints = set()
    final_clash_proxies = []

    for node in raw_nodes_list:
        clash_proxy_dict = None
        node_raw_name = ""  # ç”¨äºæ£€æŸ¥æ˜¯å¦åŒ…å«åˆ é™¤å…³é”®è¯çš„åŸå§‹åç§°

        if isinstance(node, dict):
            clash_proxy_dict = node
            node_raw_name = node.get('name', '')
        elif isinstance(node, str):
            try:
                parsed_url = urlparse(node)
                node_raw_name = parsed_url.fragment  # æå– # åé¢çš„éƒ¨åˆ†
                # éªŒè¯èŠ‚ç‚¹åè®®
                if not any(node.startswith(p + '://') for p in ["vmess", "trojan", "ss", "ssr", "vless", "hy", "hy2", "hysteria", "hysteria2"]):
                    logging.warning(f"è·³è¿‡æ— æ•ˆåè®®çš„èŠ‚ç‚¹: {node[:50]}...")
                    continue

                # éªŒè¯ä¸»æœºåæˆ– IP åœ°å€
                host = parsed_url.hostname or ''
                if host and not (is_valid_ip_address(host) or re.match(r'^[a-zA-Z0-9\-\.]+$', host)):
                    logging.warning(f"è·³è¿‡æ— æ•ˆä¸»æœºåçš„èŠ‚ç‚¹: {host} in {node[:50]}...")
                    continue

                # å°è¯•å°† URL è½¬æ¢ä¸º Clash ä»£ç†å­—å…¸
                if node.startswith("vmess://"):
                    decoded = base64.b64decode(node[len("vmess://"):].encode('utf-8')).decode('utf-8')
                    config = json.loads(decoded)
                    clash_proxy_dict = {
                        'name': config.get('ps', 'VMess Node'),
                        'type': 'vmess',
                        'server': config.get('add'),
                        'port': int(config.get('port')),
                        'uuid': config.get('id'),
                        'alterId': int(config.get('aid', 0)),
                        'cipher': 'auto',
                        'network': config.get('net'),
                        'tls': True if config.get('tls') == 'tls' else False,
                        'skip-cert-verify': True if config.get('scy') == 'true' else False,
                        'servername': config.get('sni') or config.get('host'),
                        'ws-opts': {'path': config.get('path', '/'), 'headers': {'Host': config.get('host')}} if config.get('net') == 'ws' else None,
                        'grpc-opts': {'serviceName': config.get('path', '')} if config.get('net') == 'grpc' else None,
                    }
                    if clash_proxy_dict.get('ws-opts') == {'path': '/', 'headers': {'Host': ''}}:
                        clash_proxy_dict['ws-opts'] = None
                    if clash_proxy_dict.get('grpc-opts') == {'serviceName': ''}:
                        clash_proxy_dict['grpc-opts'] = None
                elif node.startswith("trojan://"):
                    parsed = urlparse(node)
                    password = parsed.username
                    server = parsed.hostname
                    port = parsed.port
                    query = parse_qs(parsed.query)
                    clash_proxy_dict = {
                        'name': parsed.fragment or 'Trojan Node',
                        'type': 'trojan',
                        'server': server,
                        'port': port,
                        'password': password,
                        'network': query.get('type', ['tcp'])[0],
                        'tls': True,
                        'skip-cert-verify': query.get('allowInsecure', ['0'])[0] == '1',
                        'sni': query.get('sni', [server])[0]
                    }
                elif node.startswith("ss://"):
                    decoded_part = node[len("ss://"):].split('#', 1)[0]
                    try:
                        decoded_info = base64.b64decode(decoded_part.encode('utf-8')).decode('utf-8')
                        parts = decoded_info.split('@', 1)
                        method_password = parts[0].split(':', 1)
                        method = method_password[0]
                        password = method_password[1] if len(method_password) > 1 else ''
                        server_port = parts[1].split(':', 1)
                        server = server_port[0]
                        port = int(server_port[1])
                        
                        clash_proxy_dict = {
                            'name': parsed_url.fragment or 'SS Node',
                            'type': 'ss',
                            'server': server,
                            'port': port,
                            'cipher': method,
                            'password': password,
                        }
                    except Exception as e:
                        logging.warning(f"SSèŠ‚ç‚¹è§£æå¤±è´¥: {node[:50]}... - {e}")
                        clash_proxy_dict = None
                elif node.startswith("vless://"):
                    parsed = urlparse(node)
                    uuid = parsed.username
                    server = parsed.hostname
                    port = parsed.port
                    query = parse_qs(parsed.query)
                    
                    clash_proxy_dict = {
                        'name': parsed.fragment or 'VLESS Node',
                        'type': 'vless',
                        'server': server,
                        'port': port,
                        'uuid': uuid,
                        'network': query.get('type', ['tcp'])[0],
                        'tls': True if query.get('security', [''])[0] == 'tls' else False,
                        'skip-cert-verify': query.get('flow', [''])[0] == 'xtls-rprx-direct',
                        'servername': query.get('sni', [server])[0],
                        'flow': query.get('flow', [''])[0],
                        'ws-opts': {'path': query.get('path', ['/'])[0], 'headers': {'Host': query.get('host', [''])[0]}} if query.get('type', [''])[0] == 'ws' else None,
                        'grpc-opts': {'serviceName': query.get('serviceName', [''])[0]} if query.get('type', [''])[0] == 'grpc' else None,
                    }
                    if clash_proxy_dict.get('ws-opts') == {'path': '/', 'headers': {'Host': ''}}:
                        clash_proxy_dict['ws-opts'] = None
                    if clash_proxy_dict.get('grpc-opts') == {'serviceName': ''}:
                        clash_proxy_dict['grpc-opts'] = None
                elif node.startswith("hysteria://") or node.startswith("hy://"):
                    parsed = urlparse(node)
                    server = parsed.hostname
                    port = parsed.port
                    query = parse_qs(parsed.query)

                    clash_proxy_dict = {
                        'name': parsed.fragment or 'Hysteria Node',
                        'type': 'hysteria',
                        'server': server,
                        'port': port,
                        'auth_str': query.get('auth', [''])[0],
                        'alpn': query.get('alpn', [''])[0].split(','),
                        'network': query.get('protocol', ['udp'])[0],
                        'skip-cert-verify': query.get('insecure', ['0'])[0] == '1',
                        'sni': query.get('peer', [server])[0]
                    }
                    if not clash_proxy_dict['alpn']:
                        del clash_proxy_dict['alpn']
                elif node.startswith("hysteria2://") or node.startswith("hy2://"):
                    parsed = urlparse(node)
                    password = parsed.username
                    server = parsed.hostname
                    port = parsed.port
                    query = parse_qs(parsed.query)

                    clash_proxy_dict = {
                        'name': parsed.fragment or 'Hysteria2 Node',
                        'type': 'hysteria2',
                        'server': server,
                        'port': port,
                        'password': password,
                        'obfs': query.get('obfs', [''])[0],
                        'obfs-password': query.get('obfsParam', [''])[0],
                        'tls': True,
                        'skip-cert-verify': query.get('insecure', ['0'])[0] == '1',
                        'sni': query.get('sni', [server])[0],
                        'alpn': query.get('alpn', [''])[0].split(',')
                    }
                    if not clash_proxy_dict['obfs']:
                        del clash_proxy_dict['obfs']
                    if not clash_proxy_dict['obfs-password']:
                        del clash_proxy_dict['obfs-password']
                    if not clash_proxy_dict['alpn']:
                        del clash_proxy_dict['alpn']
            except Exception as e:
                logging.warning(f"URLèŠ‚ç‚¹è½¬æ¢ä¸ºClashå­—å…¸å¤±è´¥: {node[:50]}... - {e}")
                clash_proxy_dict = None

        if clash_proxy_dict:
            # æ£€æŸ¥åŸå§‹åç§°æ˜¯å¦åŒ…å«åˆ é™¤å…³é”®è¯
            should_delete_node = False
            name_to_check = node_raw_name or clash_proxy_dict.get('name', '')

            for keyword in DELETE_KEYWORDS:
                if keyword.lower() in name_to_check.lower():
                    logging.info(f"èŠ‚ç‚¹ '{name_to_check}' åŒ…å«åˆ é™¤å…³é”®è¯ '{keyword}'ï¼Œå·²è·³è¿‡ã€‚")
                    should_delete_node = True
                    break
            
            if should_delete_node:
                continue

            # éªŒè¯æœåŠ¡å™¨åœ°å€
            server = clash_proxy_dict.get('server', '')
            if server and not (is_valid_ip_address(server) or re.match(r'^[a-zA-Z0-9\-\.]+$', server)):
                logging.warning(f"è·³è¿‡æ— æ•ˆæœåŠ¡å™¨åœ°å€çš„èŠ‚ç‚¹: {server} in {clash_proxy_dict.get('name', 'Unknown')}")
                continue

            # æ¸…ç†èŠ‚ç‚¹åç§°
            clash_proxy_dict['name'] = clean_node_name(clash_proxy_dict.get('name', f"{clash_proxy_dict.get('type', 'Unknown')} {clash_proxy_dict.get('server', '')}:{clash_proxy_dict.get('port', '')}"))

            # ä½¿ç”¨æŒ‡çº¹è¿›è¡Œå»é‡
            fingerprint = _generate_node_fingerprint(clash_proxy_dict)
            if fingerprint and fingerprint not in unique_node_fingerprints:
                unique_node_fingerprints.add(fingerprint)
                final_clash_proxies.append(clash_proxy_dict)
            else:
                logging.debug(f"é‡å¤èŠ‚ç‚¹ï¼ˆæŒ‰æŒ‡çº¹ï¼‰ï¼š{clash_proxy_dict.get('name', '')} - {fingerprint}")

    return final_clash_proxies

# --- ä¸»ç¨‹åºæµç¨‹ ---

URL_SOURCE = os.environ.get("URL_SOURCE")
print(f"è°ƒè¯•ä¿¡æ¯ - è¯»å–åˆ°çš„ URL_SOURCE å€¼: {URL_SOURCE}")

if not URL_SOURCE:
    print("é”™è¯¯ï¼šç¯å¢ƒå˜é‡ 'URL_SOURCE' æœªè®¾ç½®ã€‚æ— æ³•è·å–è®¢é˜…é“¾æ¥ã€‚")
    exit(1)

os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)
os.makedirs(os.path.dirname(STATISTICS_FILE), exist_ok=True)

raw_urls_from_source = get_url_list_from_remote(URL_SOURCE)

urls_to_fetch = set()
url_statistics = []
successful_urls = []
failed_urls = []
all_parsed_nodes_raw = []

print("\n--- é¢„å¤„ç†åŸå§‹URL/å­—ç¬¦ä¸²åˆ—è¡¨ ---")
for entry in raw_urls_from_source:
    if is_valid_url(entry):
        urls_to_fetch.add(entry)
    else:
        print(f"å‘ç°éHTTP/HTTPSæ¡ç›®ï¼Œå°è¯•ç›´æ¥è§£æ: {entry[:80]}...")
        parsed_nodes = parse_content_to_nodes(entry)
        if parsed_nodes:
            all_parsed_nodes_raw.extend(parsed_nodes)
            stat_entry = {'URL': entry, 'èŠ‚ç‚¹æ•°é‡': len(parsed_nodes), 'çŠ¶æ€': 'ç›´æ¥è§£ææˆåŠŸ', 'é”™è¯¯ä¿¡æ¯': ''}
            url_statistics.append(stat_entry)
            successful_urls.append(entry)
        else:
            stat_entry = {'URL': entry, 'èŠ‚ç‚¹æ•°é‡': 0, 'çŠ¶æ€': 'ç›´æ¥è§£æå¤±è´¥', 'é”™è¯¯ä¿¡æ¯': 'éURLä¸”æ— æ³•è§£æä¸ºèŠ‚ç‚¹'}
            url_statistics.append(stat_entry)
            failed_urls.append(entry)

print("\n--- é˜¶æ®µä¸€ï¼šè·å–å¹¶åˆå¹¶æ‰€æœ‰è®¢é˜…é“¾æ¥ä¸­çš„èŠ‚ç‚¹ ---")
total_urls_to_process_via_http = len(urls_to_fetch)

if total_urls_to_process_via_http > 0:
    with ThreadPoolExecutor(max_workers=16) as executor:
        future_to_url = {executor.submit(fetch_and_parse_url, url): url for url in urls_to_fetch}
        for future in tqdm(as_completed(future_to_url), total=total_urls_to_process_via_http, desc="é€šè¿‡HTTP/HTTPSè¯·æ±‚å¹¶è§£æèŠ‚ç‚¹"):
            url = future_to_url[future]
            nodes, success, error_message = future.result()

            stat_entry = {'URL': url, 'èŠ‚ç‚¹æ•°é‡': len(nodes), 'çŠ¶æ€': 'æˆåŠŸ' if success else 'å¤±è´¥', 'é”™è¯¯ä¿¡æ¯': error_message if error_message else ''}
            url_statistics.append(stat_entry)

            if success:
                successful_urls.append(url)
                all_parsed_nodes_raw.extend(nodes)
            else:
                failed_urls.append(url)

final_unique_clash_proxies = deduplicate_and_standardize_nodes(all_parsed_nodes_raw)

with open(TEMP_MERGED_NODES_RAW_FILE, 'w', encoding='utf-8') as temp_file:
    for node in final_unique_clash_proxies:
        if isinstance(node, dict):
            temp_file.write(json.dumps(node, ensure_ascii=False) + '\n')
        else:
            temp_file.write(node.strip() + '\n')

print(f"\né˜¶æ®µä¸€å®Œæˆã€‚åˆå¹¶åˆ° {len(final_unique_clash_proxies)} ä¸ªå”¯ä¸€Clashä»£ç†å­—å…¸ï¼Œå·²ä¿å­˜è‡³ {TEMP_MERGED_NODES_RAW_FILE}")

write_statistics_to_csv(url_statistics, STATISTICS_FILE)
write_urls_to_file(successful_urls, SUCCESS_URLS_FILE)
write_urls_to_file(failed_urls, FAILED_URLS_FILE)

print("\n--- é˜¶æ®µäºŒï¼šè¾“å‡ºæœ€ç»ˆ Clash YAML é…ç½® ---")

if not OUTPUT_FILE.endswith(('.yaml', '.yml')):
    OUTPUT_FILE = os.path.splitext(OUTPUT_FILE)[0] + '.yaml'

proxies_to_output = final_unique_clash_proxies[:MAX_SUCCESS]

proxy_names_in_group = []
for node in proxies_to_output:
    if isinstance(node, dict) and 'name' in node:
        proxy_names_in_group.append(node['name'])
    else:
        proxy_names_in_group.append(f"{node.get('type', 'Unknown')} {node.get('server', '')}")

clash_config = {
    'proxies': proxies_to_output,
    'proxy-groups': [
        {
            'name': 'ğŸš€ èŠ‚ç‚¹é€‰æ‹©',
            'type': 'select',
            'proxies': ['DIRECT'] + proxy_names_in_group
        },
        {
            'name': 'â™»ï¸ è‡ªåŠ¨é€‰æ‹©',
            'type': 'url-test',
            'url': 'http://www.gstatic.com/generate_204',
            'interval': 300,
            'proxies': proxy_names_in_group
        }
    ],
    'rules': [
        'MATCH,ğŸš€ èŠ‚ç‚¹é€‰æ‹©'
    ]
}

success_count = len(proxies_to_output)

try:
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as out_file:
        yaml.dump(clash_config, out_file, allow_unicode=True, default_flow_style=False, sort_keys=False)
    print(f"æœ€ç»ˆ Clash YAML é…ç½®å·²ä¿å­˜è‡³ï¼š{OUTPUT_FILE}")
except Exception as e:
    logging.error(f"å†™å…¥æœ€ç»ˆ Clash YAML æ–‡ä»¶å¤±è´¥: {e}")
    print(f"é”™è¯¯ï¼šå†™å…¥æœ€ç»ˆ Clash YAML æ–‡ä»¶å¤±è´¥: {e}")

if os.path.exists(TEMP_MERGED_NODES_RAW_FILE):
    os.remove(TEMP_MERGED_NODES_RAW_FILE)
    print(f"å·²åˆ é™¤ä¸´æ—¶æ–‡ä»¶ï¼š{TEMP_MERGED_NODES_RAW_FILE}")

print("\n" + "=" * 50)
print("æœ€ç»ˆç»“æœï¼š")
print(f"åŸå§‹æ¥æºæ€»æ¡ç›®æ•°ï¼š{len(raw_urls_from_source)}")
print(f"å…¶ä¸­éœ€è¦HTTP/HTTPSè¯·æ±‚çš„è®¢é˜…é“¾æ¥æ•°ï¼š{len(urls_to_fetch)}")
print(f"å…¶ä¸­ç›´æ¥è§£æçš„éURLå­—ç¬¦ä¸²æ•°ï¼š{len(raw_urls_from_source) - len(urls_to_fetch)}")
print(f"æˆåŠŸå¤„ç†çš„URL/å­—ç¬¦ä¸²æ€»æ•°ï¼š{len(successful_urls)}")
print(f"å¤±è´¥çš„URL/å­—ç¬¦ä¸²æ€»æ•°ï¼š{len(failed_urls)}")
print(f"åˆæ­¥èšåˆçš„åŸå§‹èŠ‚ç‚¹æ•°ï¼ˆå»é‡å’Œè¿‡æ»¤å‰ï¼‰ï¼š{len(all_parsed_nodes_raw)}")
print(f"å»é‡ã€æ ‡å‡†åŒ–å’Œè¿‡æ»¤åçš„å”¯ä¸€Clashä»£ç†æ•°ï¼š{len(final_unique_clash_proxies)}")
print(f"æœ€ç»ˆè¾“å‡ºåˆ°Clash YAMLæ–‡ä»¶çš„èŠ‚ç‚¹æ•°ï¼š{success_count}")
if len(final_unique_clash_proxies) > 0:
    print(f"æœ€ç»ˆæœ‰æ•ˆå†…å®¹ç‡ï¼ˆç›¸å¯¹äºå»é‡è¿‡æ»¤åï¼‰ï¼š{success_count/len(final_unique_clash_proxies):.1%}")
if success_count < MAX_SUCCESS:
    print("è­¦å‘Šï¼šæœªèƒ½è¾¾åˆ°ç›®æ ‡æ•°é‡ï¼ŒåŸå§‹åˆ—è¡¨å¯èƒ½æœ‰æ•ˆURL/èŠ‚ç‚¹ä¸è¶³ï¼Œæˆ–éƒ¨åˆ†URLè·å–å¤±è´¥ã€‚")
print(f"ç»“æœæ–‡ä»¶å·²ä¿å­˜è‡³ï¼š{OUTPUT_FILE}")
print(f"ç»Ÿè®¡æ•°æ®å·²ä¿å­˜è‡³ï¼š{STATISTICS_FILE}")
print(f"æˆåŠŸURLåˆ—è¡¨å·²ä¿å­˜è‡³ï¼š{SUCCESS_URLS_FILE}")
print(f"å¤±è´¥URLåˆ—è¡¨å·²ä¿å­˜è‡³ï¼š{FAILED_URLS_FILE}")
print("=" * 50)
