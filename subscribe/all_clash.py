# -*- coding: utf-8 -*-
import os
import requests
from urllib.parse import urlparse, parse_qs, urlencode
import base64
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
import argparse
import re
import yaml
import json
import csv
import hashlib
import ipaddress
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# é…ç½®æ—¥å¿—
# æ—¥å¿—æ–‡ä»¶åä¸º error.logï¼Œçº§åˆ«è®¾ç½®ä¸º DEBUGï¼Œæ–¹ä¾¿è°ƒè¯•å’Œé—®é¢˜è¿½æº¯
logging.basicConfig(filename='error.log', level=logging.DEBUG,
                    format='%(asctime)s - %(levelname)s - %(message)s')

# è¯·æ±‚å¤´
# æ¨¡æ‹Ÿæµè§ˆå™¨è¡Œä¸ºï¼Œé˜²æ­¢è¢«æœåŠ¡å™¨è¯†åˆ«ä¸ºæœºå™¨äºº
headers = {
    'User-Agent': (
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
        'AppleWebKit/537.36 (KHTML, like Gecko) '
        'Chrome/91.0.4472.124 Safari/537.36'
    ),
    'Accept-Encoding': 'gzip, deflate' # æ¥å—gzipå’Œdeflateç¼–ç ï¼Œæé«˜ä¼ è¾“æ•ˆç‡
}

# å‘½ä»¤è¡Œå‚æ•°è§£æ
# å…è®¸ç”¨æˆ·é€šè¿‡å‘½ä»¤è¡Œè‡ªå®šä¹‰è„šæœ¬è¡Œä¸º
parser = argparse.ArgumentParser(description="URLå†…å®¹è·å–è„šæœ¬ï¼Œæ”¯æŒå¤šä¸ªURLæ¥æºå’ŒèŠ‚ç‚¹è§£æ")
parser.add_argument('--max_success', type=int, default=99999, help="ç›®æ ‡æˆåŠŸæ•°é‡ï¼Œè¾¾åˆ°æ­¤æ•°é‡åè„šæœ¬å¯èƒ½ä¼šæå‰ç»ˆæ­¢")
parser.add_argument('--timeout', type=int, default=30, help="è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰")
parser.add_argument('--output', type=str, default='data/all_clash.yaml', help="è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œç”Ÿæˆçš„Clash YAMLé…ç½®å°†ä¿å­˜åˆ°æ­¤æ–‡ä»¶")
args = parser.parse_args()

# å…¨å±€å˜é‡
MAX_SUCCESS = args.max_success
TIMEOUT = args.timeout
OUTPUT_FILE = args.output
TEMP_MERGED_NODES_RAW_FILE = 'temp_merged_nodes_raw.txt' # ä¸´æ—¶æ–‡ä»¶ï¼Œç”¨äºå­˜å‚¨åŸå§‹è§£æåˆ°çš„èŠ‚ç‚¹
STATISTICS_FILE = 'data/url_statistics.csv' # ç»Ÿè®¡æ–‡ä»¶ï¼Œè®°å½•æ¯ä¸ªURLçš„å¤„ç†ç»“æœ
SUCCESS_URLS_FILE = 'data/successful_urls.txt' # æˆåŠŸè·å–å¹¶è§£æçš„URLåˆ—è¡¨
FAILED_URLS_FILE = 'data/failed_urls.txt' # å¤±è´¥çš„URLåˆ—è¡¨

# å®šä¹‰åˆ é™¤å…³é”®è¯
# åŒ…å«è¿™äº›å…³é”®è¯çš„èŠ‚ç‚¹åç§°å°†è¢«è·³è¿‡ï¼Œé€šå¸¸æ˜¯å¹¿å‘Šã€æµé‡ä¿¡æ¯ç­‰
DELETE_KEYWORDS = [
    'å‰©ä½™æµé‡', 'å¥—é¤åˆ°æœŸ', 'æµé‡', 'åˆ°æœŸ', 'è¿‡æœŸ', 'å…è´¹', 'è¯•ç”¨', 'ä½“éªŒ', 'é™æ—¶', 'é™åˆ¶',
    'å·²ç”¨', 'å¯ç”¨', 'ä¸è¶³', 'åˆ°æœŸæ—¶é—´', 'å€ç‡', 'è¿”åˆ©', 'å……å€¼', 'ç»­è´¹', 'ç”¨é‡', 'è®¢é˜…'
]

# å®šä¹‰å¸¸è§å ä½ç¬¦ UUID/å¯†ç ï¼Œè¿™äº›ä¸ä¼šä½œä¸ºå”¯ä¸€æ€§åˆ¤æ–­ä¾æ®
COMMON_UUID_PLACEHOLDERS = [
    "aaaaaaa1-bbbb-4ccc-accc-eeeeeeeeeee1", # ä½ æä¾›çš„ç¤ºä¾‹ä¸­çš„å ä½ç¬¦
    "00000000-0000-0000-0000-000000000000",
    "d23b3208-d01d-40d3-b1d6-fe1e48edcb74" # å¸¸è§çš„ä¼ªé€ UUID
    # å¯ä»¥æ ¹æ®è§‚å¯Ÿåˆ°çš„å…¶ä»–å¸¸è§å ä½ç¬¦è¡¥å……
]

# å®šä¹‰å¸¸è§å ä½ç¬¦å¯†ç 
COMMON_PASSWORD_PLACEHOLDERS = [
    "aaaaaaa1-bbbb-4ccc-accc-eeeeeeeeeee1", # ä½ æä¾›çš„ç¤ºä¾‹ä¸­çš„å ä½ç¬¦
    "password", "123456", "000000", "test", "demo", "free"
    # å¯ä»¥æ ¹æ®è§‚å¯Ÿåˆ°çš„å…¶ä»–å¸¸è§å ä½ç¬¦è¡¥å……
]


def is_valid_url(url):
    """
    éªŒè¯URLæ ¼å¼æ˜¯å¦åˆæ³•ï¼Œä»…æ¥å— http æˆ– https æ–¹æ¡ˆã€‚
    ä½¿ç”¨ urllib.parse.urlparse è¿›è¡Œè§£æå’ŒéªŒè¯ã€‚
    """
    try:
        result = urlparse(url)
        return all([result.scheme in ['http', 'https'], result.netloc])
    except Exception:
        return False

def is_valid_ip_address(host):
    """
    éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆçš„ IPv4 æˆ– IPv6 åœ°å€ã€‚
    ä½¿ç”¨ ipaddress æ¨¡å—è¿›è¡ŒéªŒè¯ã€‚
    """
    try:
        # å°è¯•è§£æä¸ºIPv4æˆ–IPv6
        ipaddress.ip_address(host)
        return True
    except ValueError:
        # é’ˆå¯¹IPv6åœ°å€å¯èƒ½å¸¦æ–¹æ‹¬å·çš„æƒ…å†µè¿›è¡Œé¢å¤–å¤„ç†
        try:
            if host.startswith('[') and host.endswith(']'):
                ipaddress.ip_address(host[1:-1])
                return True
            return False
        except ValueError:
            return False

def get_url_list_from_remote(url_source):
    """
    ä»ç»™å®šçš„å…¬å¼€ç½‘å€è·å– URL åˆ—è¡¨ã€‚
    é€šå¸¸è¿™ä¸ªurl_sourceä¼šæ˜¯ä¸€ä¸ªåŒ…å«è®¢é˜…é“¾æ¥çš„æ–‡æœ¬æ–‡ä»¶ã€‚
    """
    try:
        session = requests.Session()
        # é…ç½®é‡è¯•ç­–ç•¥ï¼Œå¤„ç†å¸¸è§çš„ç½‘ç»œé”™è¯¯å’ŒçŠ¶æ€ç 
        retries = Retry(total=3, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])
        session.mount('http://', HTTPAdapter(max_retries=retries))
        session.mount('https://', HTTPAdapter(max_retries=retries))
        response = session.get(url_source, headers=headers, timeout=10)
        response.raise_for_status() # å¦‚æœçŠ¶æ€ç ä¸æ˜¯2xxï¼Œä¼šæŠ›å‡ºHTTPError
        text_content = response.text.strip()
        # å°†å†…å®¹æŒ‰è¡Œåˆ†å‰²ï¼Œè¿‡æ»¤ç©ºè¡Œ
        raw_urls = [line.strip() for line in text_content.splitlines() if line.strip()]
        print(f"ä» {url_source} è·å–åˆ° {len(raw_urls)} ä¸ªURLæˆ–å­—ç¬¦ä¸²")
        return raw_urls
    except Exception as e:
        logging.error(f"è·å–URLåˆ—è¡¨å¤±è´¥: {url_source} - {e}")
        return []

def parse_content_to_nodes(content):
    """
    ä»æ–‡æœ¬å†…å®¹ä¸­è§£æå‡ºèŠ‚ç‚¹ã€‚
    å†…å®¹å¯èƒ½æ˜¯Base64ç¼–ç çš„ï¼Œä¹Ÿå¯èƒ½æ˜¯Clash YAMLæ ¼å¼ï¼Œæˆ–è€…ç›´æ¥æ˜¯å¤šç§åè®®çš„URLåˆ—è¡¨ã€‚
    """
    if not content:
        return []

    found_nodes = []
    processed_content = content

    # 1. å°è¯• Base64 è§£ç 
    # è®¢é˜…å†…å®¹é€šå¸¸æ˜¯Base64ç¼–ç çš„ï¼Œä¼˜å…ˆå°è¯•è§£ç 
    try:
        decoded_bytes = base64.b64decode(content)
        processed_content = decoded_bytes.decode('utf-8')
        logging.info("å†…å®¹æˆåŠŸ Base64 è§£ç ã€‚")
    except Exception:
        # å¦‚æœä¸æ˜¯Base64ç¼–ç ï¼Œåˆ™ä¿æŒåŸæ ·
        pass

    # 2. å°è¯• YAML è§£æ
    # å¦‚æœæ˜¯Clash YAMLé…ç½®ï¼Œè§£æå…¶ä¸­çš„proxieséƒ¨åˆ†
    try:
        parsed_data = yaml.safe_load(processed_content)
        if isinstance(parsed_data, dict) and 'proxies' in parsed_data and isinstance(parsed_data['proxies'], list):
            for proxy_entry in parsed_data['proxies']:
                if isinstance(proxy_entry, dict):
                    # ç¡®ä¿nameæ˜¯å­—ç¬¦ä¸²ï¼Œä»¥é˜²æŸäº›é…ç½®ä¸­nameæ˜¯æ•°å­—
                    if 'name' in proxy_entry and not isinstance(proxy_entry['name'], str):
                        proxy_entry['name'] = str(proxy_entry['name'])
                    found_nodes.append(proxy_entry)
                elif isinstance(proxy_entry, str) and any(proxy_entry.startswith(p + '://') for p in ["vmess", "trojan", "ss", "ssr", "vless", "hy", "hy2", "hysteria", "hysteria2"]):
                    # ç›´æ¥æ˜¯URLå­—ç¬¦ä¸²å½¢å¼çš„èŠ‚ç‚¹
                    found_nodes.append(proxy_entry.strip())
            logging.info("å†…å®¹æˆåŠŸè§£æä¸º Clash YAMLã€‚")
        elif isinstance(parsed_data, list):
            # å¦‚æœè§£æç»“æœç›´æ¥æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼ˆä¾‹å¦‚ï¼ŒæŸäº›è®¢é˜…ç›´æ¥è¿”å›èŠ‚ç‚¹URLåˆ—è¡¨ï¼‰
            for item in parsed_data:
                if isinstance(item, str):
                    found_nodes.append(item.strip())
                elif isinstance(item, dict):
                    if 'name' in item and not isinstance(item['name'], str):
                        item['name'] = str(item['name'])
                    found_nodes.append(item)
            logging.info("å†…å®¹æˆåŠŸè§£æä¸º YAML åˆ—è¡¨ã€‚")
    except yaml.YAMLError:
        pass # ä¸æ˜¯YAMLæ ¼å¼ï¼Œç»§ç»­å°è¯•å…¶ä»–è§£ææ–¹å¼
    except Exception as e:
        logging.error(f"YAML è§£æå¤±è´¥: {e}")
        pass

    # 3. é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼æå–èŠ‚ç‚¹
    # å°è¯•ä»åŸå§‹å†…å®¹æˆ–è§£ç åçš„å†…å®¹ä¸­ç›´æ¥åŒ¹é…èŠ‚ç‚¹URL
    node_pattern = re.compile(
        r'(vmess://\S+|'
        r'trojan://\S+|'
        r'ss://\S+|'
        r'ssr://\S+|' # SSR ä¹Ÿæ˜¯ URL å½¢å¼
        r'vless://\S+|'
        r'hy://\S+|'
        r'hy2://\S+|'
        r'hysteria://\S+|'
        r'hysteria2://\S+)'
    )
    
    # æ£€æŸ¥åŸå§‹å†…å®¹
    matches = node_pattern.findall(content)
    for match in matches:
        found_nodes.append(match.strip())
    
    # å¦‚æœå†…å®¹è¢«è§£ç è¿‡ï¼Œå†æ£€æŸ¥è§£ç åçš„å†…å®¹
    if content != processed_content:
        matches_decoded = node_pattern.findall(processed_content)
        for match in matches_decoded:
            found_nodes.append(match.strip())

    return found_nodes

def fetch_and_parse_url(url):
    """
    è·å–URLå†…å®¹å¹¶è§£æå‡ºèŠ‚ç‚¹ã€‚
    æ­¤å‡½æ•°ä¼šå°è¯•è¯·æ±‚ç»™å®šçš„URLï¼Œç„¶åè°ƒç”¨ parse_content_to_nodes è¿›è¡Œè§£æã€‚
    è¿”å› (èŠ‚ç‚¹åˆ—è¡¨, æ˜¯å¦æˆåŠŸ, é”™è¯¯ä¿¡æ¯, çŠ¶æ€ç )
    """
    session = requests.Session()
    retries = Retry(total=2, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])
    session.mount('http://', HTTPAdapter(max_retries=retries))
    session.mount('https://', HTTPAdapter(max_retries=retries))

    try:
        logging.debug(f"å¼€å§‹è¯·æ±‚ URL: {url}")
        resp = session.get(url, headers=headers, timeout=TIMEOUT)
        resp.raise_for_status() # å¯¹äº 4xx æˆ– 5xx çŠ¶æ€ç ï¼ŒæŠ›å‡ºå¼‚å¸¸
        content = resp.text.strip()
        
        if len(content) < 10: # å†…å®¹è¿‡çŸ­å¯èƒ½æ˜¯æ— æ•ˆè®¢é˜…
            logging.warning(f"è·å–åˆ°å†…å®¹è¿‡çŸ­ï¼Œå¯èƒ½æ— æ•ˆ: {url}")
            return [], False, "å†…å®¹è¿‡çŸ­", resp.status_code
            
        nodes = parse_content_to_nodes(content)
        logging.debug(f"URL {url} è§£æåˆ° {len(nodes)} ä¸ªèŠ‚ç‚¹")
        return nodes, True, None, resp.status_code
    except requests.exceptions.Timeout:
        logging.error(f"è¯·æ±‚è¶…æ—¶: {url}")
        return [], False, "è¯·æ±‚è¶…æ—¶", None
    except requests.exceptions.ConnectionError as e:
        logging.error(f"è¿æ¥å¤±è´¥: {url} - {e}")
        return [], False, f"è¿æ¥å¤±è´¥: {e}", None
    except requests.exceptions.HTTPError as e:
        logging.error(f"HTTPé”™è¯¯: {url} - {e}")
        return [], False, f"HTTPé”™è¯¯: {e}", None
    except requests.exceptions.RequestException as e:
        logging.error(f"è¯·æ±‚å¤±è´¥: {url} - {e}")
        return [], False, f"è¯·æ±‚å¤±è´¥: {e}", None
    except Exception as e:
        logging.error(f"å¤„ç†URLå¼‚å¸¸: {url} - {e}")
        return [], False, f"æœªçŸ¥å¼‚å¸¸: {e}", None

def write_statistics_to_csv(statistics_data, filename):
    """å°†ç»Ÿè®¡æ•°æ®å†™å…¥CSVæ–‡ä»¶"""
    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
        fieldnames = ['URL', 'èŠ‚ç‚¹æ•°é‡', 'çŠ¶æ€', 'é”™è¯¯ä¿¡æ¯', 'çŠ¶æ€ç ']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

        writer.writeheader()
        for row in statistics_data:
            writer.writerow(row)
    print(f"ç»Ÿè®¡æ•°æ®å·²ä¿å­˜è‡³ï¼š{filename}")

def write_urls_to_file(urls, filename):
    """å°†URLåˆ—è¡¨å†™å…¥æ–‡ä»¶"""
    with open(filename, 'w', encoding='utf-8') as f:
        for url in urls:
            f.write(url + '\n')
    print(f"URLåˆ—è¡¨å·²ä¿å­˜è‡³ï¼š{filename}")

def clean_node_name(name, index=None):
    """
    æ¸…ç†èŠ‚ç‚¹åç§°ï¼Œå»é™¤å¤šä½™ä¿¡æ¯ï¼Œæ ‡å‡†åŒ–åŒºåŸŸåç§°ï¼Œå¹¶æ·»åŠ åºå·ã€‚
    è¿™ä¸ªå‡½æ•°ä»…ç”¨äºç¾åŒ–èŠ‚ç‚¹åç§°ï¼Œä¸å½±å“å»é‡é€»è¾‘ã€‚
    """
    if not isinstance(name, str):
        name = str(name)

    cleaned_name = name.strip()

    # ç§»é™¤åŒ…å«ç‰¹å®šå…³é”®è¯çš„æ‹¬å·å†…å®¹ï¼Œä¾‹å¦‚â€œã€å‰©ä½™æµé‡2Gã€‘â€
    cleaned_name = re.sub(r'ã€[^ã€‘]*?(æµé‡|åˆ°æœŸ|è¿‡æœŸ|å……å€¼|ç»­è´¹)[^ã€‘]*ã€‘', '', cleaned_name)
    cleaned_name = re.sub(r'\[[^]]*?(æµé‡|åˆ°æœŸ|è¿‡æœŸ|å……å€¼|ç»­è´¹)[^\]]*\]', '', cleaned_name)
    cleaned_name = re.sub(r'\([^)]*?(æµé‡|åˆ°æœŸ|è¿‡æœŸ|å……å€¼|ç»­è´¹)[^)]*\)', '', cleaned_name)
    cleaned_name = re.sub(r'ï¼ˆ[^ï¼‰]*?(æµé‡|åˆ°æœŸ|è¿‡æœŸ|å……å€¼|ç»­è´¹)[^ï¼‰]*ï¼‰', '', cleaned_name)

    # ç§»é™¤å…¶ä»–å†—ä½™å…³é”®è¯æˆ–æ¨¡å¼
    redundant_keywords_to_remove = [
        r'\d+%', r'\d{4}-\d{2}-\d{2}', r'\d{2}-\d{2}', r'x\d+', # 100%, æ—¥æœŸ, x2å€ç‡
        r'ç§’æ€', r'æ´»åŠ¨', r'æ–°å¹´', r'ç¦åˆ©', r'VIP\d*', r'Pro', r'Lite', r'Plus', # ä¿ƒé”€è¯
        r'è‡ªåŠ¨', r'æ‰‹åŠ¨', r'è‡ªé€‰', # é€‰æ‹©æ–¹å¼
        r'(\d+\.\d+kbps)', r'(\d+\.\d+mbps)', r'(\d+kbps)', r'(\d+mbps)', # é€Ÿåº¦ä¿¡æ¯
        r'\\n', r'\\r', r'\d+\.\d+G|\d+G', # æ¢è¡Œç¬¦ï¼Œæµé‡Gæ•°
    ]

    for keyword in redundant_keywords_to_remove:
        cleaned_name = re.sub(keyword, ' ', cleaned_name, flags=re.IGNORECASE).strip()

    # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œåªä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—ã€ç©ºæ ¼å’Œä¸€äº›å¸¸ç”¨ç¬¦å·
    cleaned_name = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\s\.\-_@#|]', ' ', cleaned_name)
    # åˆå¹¶å¤šä¸ªç©ºæ ¼ä¸ºå•ä¸ªç©ºæ ¼
    cleaned_name = re.sub(r'\s+', ' ', cleaned_name).strip()

    # å°†ä¸­æ–‡åŒºåŸŸåæ›¿æ¢ä¸ºè‹±æ–‡ç®€ç§°
    region_map = {
        'é¦™æ¸¯': 'HK', 'å°æ¹¾': 'TW', 'æ—¥æœ¬': 'JP', 'æ–°åŠ å¡': 'SG', 'ç¾å›½': 'US', 'è‹±å›½': 'UK',
        'å¾·å›½': 'DE', 'éŸ©å›½': 'KR', 'é©¬æ¥': 'MY', 'æ³°å›½': 'TH', 'PH': 'PH', 'è¶Šå—': 'VN',
        'å°å°¼': 'ID', 'å°åº¦': 'IN', 'æ¾³æ´²': 'AU', 'åŠ æ‹¿å¤§': 'CA', 'ä¿„ç½—æ–¯': 'RU', 'å·´è¥¿': 'BR',
        'æ„å¤§åˆ©': 'IT', 'è·å…°': 'NL', 'ä¸­å›½': 'CN', 'æ·±åœ³': 'SZ', 'ä¸Šæµ·': 'SH', 'åŒ—äº¬': 'BJ',
        'å¹¿å·': 'GZ', 'æ­å·': 'HZ' # å¢åŠ ä¸€äº›åŸå¸‚ç®€ç§°
    }
    for full_name, short_name in region_map.items():
        cleaned_name = cleaned_name.replace(full_name, short_name)

    # å°è¯•ä¿ç•™ä¸€äº›æœ‰æ„ä¹‰çš„å…³é”®è¯ï¼Œä¾‹å¦‚ä¸“çº¿ä¿¡æ¯
    meaningful_keywords = ['IPLC', 'IEPL', 'ä¸“çº¿', 'ä¸­è½¬', 'ç›´è¿', 'CDN']
    preserved_info = []
    for keyword in meaningful_keywords:
        if keyword.lower() in cleaned_name.lower():
            preserved_info.append(keyword)
    
    # å°è¯•ä¿ç•™èŠ‚ç‚¹ç¼–å·
    node_number_match = re.search(r'(?<!\d)(?:[Nn]ode|Server)?\s?(\d{1,3})(?!\d)', cleaned_name) # åŒ¹é… Node1, Server 2, 123
    if node_number_match:
        preserved_info.append(node_number_match.group(1))

    # å¦‚æœæ¸…ç†ååç§°è¿‡çŸ­æˆ–ä¸ºç©ºï¼Œå°è¯•ä½¿ç”¨åŒºåŸŸåæˆ–é»˜è®¤åç§°
    if not cleaned_name or len(cleaned_name) <= 3:
        cleaned_name = 'Node'
        # å¦‚æœåŸå§‹åç§°ä¸­åŒ…å«åŒºåŸŸä¿¡æ¯ï¼Œä¼˜å…ˆä½¿ç”¨åŒºåŸŸä¿¡æ¯
        if any(region in name for region in region_map.values()):
            for region in region_map.values():
                if region in name:
                    cleaned_name = region
                    break
        if preserved_info:
            cleaned_name += ' ' + ' '.join(preserved_info) # è¡¥å……ä¿ç•™çš„ä¿¡æ¯

    # æ·»åŠ åºå·ï¼Œç¡®ä¿åç§°å”¯ä¸€æ€§ (åœ¨æœ€ç»ˆè¾“å‡ºæ—¶å†ç»Ÿä¸€æ·»åŠ ï¼Œè¿™é‡Œåªæ˜¯ä¸€ä¸ªé€šç”¨æ¸…ç†å‡½æ•°)
    # è„šæœ¬çš„å®é™…å®ç°ä¸­ï¼Œåºå·æ˜¯åœ¨ deduplicate_and_standardize_nodes ä¸­æ·»åŠ çš„
    if index is not None:
        # ç¡®ä¿åºå·æ ¼å¼ä¸€è‡´ï¼Œä¾‹å¦‚ä¸¤ä½æ•°
        cleaned_name += f"-{index:03d}" # æ›´æ”¹ä¸ºä¸‰ä½æ•°åºå·

    # é™åˆ¶åç§°é•¿åº¦
    if len(cleaned_name) > 80:
        cleaned_name = cleaned_name[:80].rstrip() + '...'

    return cleaned_name if cleaned_name else f"Node-{index:03d}" if index is not None else "Unknown Node"

def _normalize_dict_for_fingerprint(data):
    """
    é€’å½’åœ°æ ‡å‡†åŒ–å­—å…¸ï¼Œä»¥ä¾¿ç”Ÿæˆç¨³å®šçš„æŒ‡çº¹ã€‚
    - é”®è½¬æ¢ä¸ºå°å†™
    - å€¼å»é™¤é¦–å°¾ç©ºç™½
    - ç§»é™¤ None æˆ–ç©ºå­—ç¬¦ä¸²çš„å€¼
    - åˆ—è¡¨è¿›è¡Œæ’åº
    - åµŒå¥—å­—å…¸é€’å½’å¤„ç†
    """
    if not isinstance(data, dict):
        return data # éå­—å…¸ç±»å‹ç›´æ¥è¿”å›

    normalized = {}
    for k, v in data.items():
        if isinstance(v, dict):
            normalized_v = _normalize_dict_for_fingerprint(v)
            if normalized_v: # åªæœ‰éç©ºå­—å…¸æ‰ä¿ç•™
                normalized[k.lower()] = normalized_v
        elif isinstance(v, list):
            # å¯¹åˆ—è¡¨å…ƒç´ è¿›è¡Œæ ‡å‡†åŒ–å¹¶æ’åº
            normalized_list = sorted([str(item).lower().strip() for item in v if str(item).strip()])
            if normalized_list: # åªæœ‰éç©ºåˆ—è¡¨æ‰ä¿ç•™
                normalized[k.lower()] = normalized_list
        elif v is not None and str(v).strip() != '': # å¿½ç•¥ None å’Œç©ºå­—ç¬¦ä¸²
            # ç»Ÿä¸€å¸ƒå°”å€¼è¡¨ç¤º
            if isinstance(v, bool):
                normalized[k.lower()] = str(v).lower()
            else:
                normalized[k.lower()] = str(v).lower().strip()
    return normalized

def _get_node_core_params(node_dict):
    """
    ä»æ ‡å‡†åŒ–çš„ClashèŠ‚ç‚¹å­—å…¸ä¸­æå–æ ¸å¿ƒå‚æ•°é›†ï¼Œç”¨äºç”ŸæˆæŒ‡çº¹ã€‚
    è¿™æ˜¯å»é‡é€»è¾‘çš„æ ¸å¿ƒï¼Œæ—¨åœ¨å¿½ç•¥éæ ¸å¿ƒæˆ–åŠ¨æ€å˜åŒ–çš„å‚æ•°ã€‚
    """
    core_params = {
        'type': node_dict.get('type'),
        'server': node_dict.get('server'),
        'port': node_dict.get('port'),
    }

    node_type = node_dict.get('type')

    # å¤„ç† servername/sniï¼šå¦‚æœä¸ server ä¸åŒï¼Œåˆ™åŠ å…¥ï¼Œå¦åˆ™å¿½ç•¥
    servername = node_dict.get('servername') or node_dict.get('sni')
    if servername and str(servername).lower().strip() != str(node_dict.get('server', '')).lower().strip():
        core_params['servername'] = servername

    # å¤„ç† skip-cert-verify, ç»Ÿä¸€ä¸ºå¸ƒå°”å€¼
    if node_dict.get('skip-cert-verify') is not None:
        core_params['skip-cert-verify'] = bool(node_dict['skip-cert-verify'])
    elif node_type in ['trojan', 'vless', 'vmess', 'hysteria', 'hysteria2'] and node_dict.get('tls'):
         # å¯¹äºå¼€å¯TLSçš„èŠ‚ç‚¹ï¼Œå¦‚æœæ˜ç¡®æŒ‡å®š skip-cert-verify=Falseï¼Œåˆ™è®¤ä¸ºè¯ä¹¦éªŒè¯æ˜¯ä¸¥æ ¼çš„ï¼Œå¦åˆ™é»˜è®¤ä¸ºTrue
        core_params['skip-cert-verify'] = bool(node_dict.get('skip-cert-verify', False))


    # åè®®ç‰¹å®šå‚æ•°
    if node_type == 'vmess':
        uuid = node_dict.get('uuid') or node_dict.get('id')
        if uuid and str(uuid).lower() not in COMMON_UUID_PLACEHOLDERS:
            core_params['uuid'] = uuid
        core_params['alterId'] = int(node_dict.get('alterId', 0) or node_dict.get('aid', 0)) # alterId å½±å“è¿æ¥
        core_params['cipher'] = node_dict.get('cipher') # VMess çš„åŠ å¯†æ–¹å¼
        core_params['network'] = node_dict.get('network')
        core_params['tls'] = bool(node_dict.get('tls')) # VMess çš„ tls
        
        # å¤„ç† ws-opts
        ws_opts = node_dict.get('ws-opts')
        if isinstance(ws_opts, dict):
            standardized_ws_opts = {}
            # åªæœ‰å½“ path ä¸ä¸º '/' æˆ–å­˜åœ¨æœ‰æ„ä¹‰çš„ headers æ—¶æ‰åŠ å…¥
            if ws_opts.get('path') and ws_opts['path'] != '/':
                standardized_ws_opts['path'] = ws_opts['path']
            if ws_opts.get('headers') and isinstance(ws_opts['headers'], dict):
                # å¯¹ headers å­—å…¸è¿›è¡Œæ ‡å‡†åŒ–ï¼šé”®å°å†™ï¼Œå€¼å°å†™å¹¶å»ç©ºç™½
                standardized_headers = {k.lower(): str(v).lower().strip() for k, v in ws_opts['headers'].items() if str(v).strip()}
                if standardized_headers:
                    standardized_ws_opts['headers'] = standardized_headers
            if standardized_ws_opts:
                core_params['ws-opts'] = standardized_ws_opts

        # å¤„ç† grpc-opts
        grpc_opts = node_dict.get('grpc-opts')
        if isinstance(grpc_opts, dict) and grpc_opts.get('serviceName'):
            core_params['grpc-opts'] = {'serviceName': grpc_opts['serviceName']} # åªå…³å¿ƒ serviceName

    elif node_type == 'trojan':
        password = node_dict.get('password')
        if password and str(password).lower() not in COMMON_PASSWORD_PLACEHOLDERS:
            core_params['password'] = password
        core_params['network'] = node_dict.get('network')
        core_params['tls'] = bool(node_dict.get('tls'))
        
        # Trojan å¯èƒ½æœ‰ ws-opts/grpc-opts
        ws_opts = node_dict.get('ws-opts')
        if isinstance(ws_opts, dict):
            standardized_ws_opts = {}
            if ws_opts.get('path') and ws_opts['path'] != '/':
                standardized_ws_opts['path'] = ws_opts['path']
            if ws_opts.get('headers') and isinstance(ws_opts['headers'], dict):
                standardized_headers = {k.lower(): str(v).lower().strip() for k, v in ws_opts['headers'].items() if str(v).strip()}
                if standardized_headers:
                    standardized_ws_opts['headers'] = standardized_headers
            if standardized_ws_opts:
                core_params['ws-opts'] = standardized_ws_opts
        
        grpc_opts = node_dict.get('grpc-opts')
        if isinstance(grpc_opts, dict) and grpc_opts.get('serviceName'):
            core_params['grpc-opts'] = {'serviceName': grpc_opts['serviceName']}

    elif node_type == 'ss':
        core_params['cipher'] = node_dict.get('cipher')
        password = node_dict.get('password')
        if password and str(password).lower() not in COMMON_PASSWORD_PLACEHOLDERS:
            core_params['password'] = password
        
        # å¤„ç† plugin å’Œ plugin-opts
        if node_dict.get('plugin'):
            core_params['plugin'] = node_dict['plugin']
            if node_dict.get('plugin-opts'):
                # å¯¹ plugin-opts å­—å…¸è¿›è¡Œæ ‡å‡†åŒ–
                standardized_plugin_opts = {k.lower(): str(v).lower().strip() for k, v in node_dict['plugin-opts'].items() if str(v).strip()}
                if standardized_plugin_opts:
                    core_params['plugin-opts'] = standardized_plugin_opts

    elif node_type == 'vless':
        uuid = node_dict.get('uuid') or node_dict.get('id')
        if uuid and str(uuid).lower() not in COMMON_UUID_PLACEHOLDERS:
            core_params['uuid'] = uuid
        core_params['network'] = node_dict.get('network')
        core_params['tls'] = bool(node_dict.get('tls'))
        # åªæœ‰éç©ºå­—ç¬¦ä¸²æ‰åŠ å…¥ flow
        if node_dict.get('flow') and node_dict['flow'] != '':
            core_params['flow'] = node_dict['flow']
        
        core_params['xudp'] = bool(node_dict.get('xudp'))
        core_params['udp-over-tcp'] = bool(node_dict.get('udp-over-tcp'))

        # å¤„ç† ws-opts
        ws_opts = node_dict.get('ws-opts')
        if isinstance(ws_opts, dict):
            standardized_ws_opts = {}
            if ws_opts.get('path') and ws_opts['path'] != '/':
                standardized_ws_opts['path'] = ws_opts['path']
            if ws_opts.get('headers') and isinstance(ws_opts['headers'], dict):
                standardized_headers = {k.lower(): str(v).lower().strip() for k, v in ws_opts['headers'].items() if str(v).strip()}
                if standardized_headers:
                    standardized_ws_opts['headers'] = standardized_headers
            if standardized_ws_opts:
                core_params['ws-opts'] = standardized_ws_opts
        
        # å¤„ç† grpc-opts
        grpc_opts = node_dict.get('grpc-opts')
        if isinstance(grpc_opts, dict) and grpc_opts.get('serviceName'):
            core_params['grpc-opts'] = {'serviceName': grpc_opts['serviceName']}
            
    elif node_type in ['hysteria', 'hy']:
        password = node_dict.get('password') # Hysteria ä½¿ç”¨ password
        auth_str = node_dict.get('auth_str') # Hysteria ä¹Ÿå¯èƒ½ä½¿ç”¨ auth_str
        if password and str(password).lower() not in COMMON_PASSWORD_PLACEHOLDERS:
            core_params['password'] = password
        elif auth_str and str(auth_str).lower() not in COMMON_PASSWORD_PLACEHOLDERS:
            core_params['auth_str'] = auth_str
        
        core_params['network'] = node_dict.get('protocol', 'udp') # Hysteria çš„ protocol å­—æ®µ
        core_params['tls'] = bool(node_dict.get('tls'))

        alpn_list = [a.strip().lower() for a in node_dict.get('alpn', []) if a.strip()]
        if alpn_list:
            core_params['alpn'] = sorted(alpn_list) # ALPN åˆ—è¡¨æ’åº

    elif node_type in ['hysteria2', 'hy2']:
        password = node_dict.get('password')
        if password and str(password).lower() not in COMMON_PASSWORD_PLACEHOLDERS:
            core_params['password'] = password
        
        core_params['obfs'] = node_dict.get('obfs')
        core_params['obfs-password'] = node_dict.get('obfs-password')
        core_params['tls'] = bool(node_dict.get('tls'))

        alpn_list = [a.strip().lower() for a in node_dict.get('alpn', []) if a.strip()]
        if alpn_list:
            core_params['alpn'] = sorted(alpn_list) # ALPN åˆ—è¡¨æ’åº

    # å¯¹æ•´ä¸ªæ ¸å¿ƒå‚æ•°å­—å…¸è¿›è¡Œæ ‡å‡†åŒ–ï¼Œå»é™¤ç©ºå€¼ç­‰
    return _normalize_dict_for_fingerprint(core_params)

def _generate_stable_fingerprint_from_params(params_dict):
    """
    å°†æ ‡å‡†åŒ–çš„æ ¸å¿ƒå‚æ•°å­—å…¸è½¬æ¢ä¸ºç¨³å®šçš„JSONå­—ç¬¦ä¸²ï¼Œå¹¶è®¡ç®—SHA256æŒ‡çº¹ã€‚
    """
    if not params_dict:
        return None

    # ç¡®ä¿JSONåºåˆ—åŒ–æ˜¯ç¨³å®šçš„ï¼ˆé”®æ’åºï¼ŒéASCIIå­—ç¬¦ä¿ç•™ï¼‰
    stable_json = json.dumps(params_dict, sort_keys=True, ensure_ascii=False)
    # logging.debug(f"Fingerprint JSON: {stable_json}") # ç”¨äºè°ƒè¯•
    return hashlib.sha256(stable_json.encode('utf-8')).hexdigest()

def deduplicate_and_standardize_nodes(raw_nodes_list):
    """
    å¯¹èŠ‚ç‚¹è¿›è¡Œå»é‡å’Œæ ‡å‡†åŒ–ã€‚
    å°†å„ç§åŸå§‹èŠ‚ç‚¹æ ¼å¼è½¬æ¢ä¸ºç»Ÿä¸€çš„Clashä»£ç†å­—å…¸æ ¼å¼ï¼Œå¹¶åŸºäºæ ¸å¿ƒå‚æ•°æŒ‡çº¹è¿›è¡Œå»é‡ã€‚
    """
    unique_node_fingerprints = set()
    final_clash_proxies = []

    for idx, node in enumerate(raw_nodes_list):
        clash_proxy_dict = None
        node_raw_name = "" # ç”¨äºä¿ç•™åŸå§‹åç§°ä»¥è¿›è¡Œå…³é”®è¯æ£€æŸ¥

        if isinstance(node, dict):
            # å¦‚æœå·²ç»æ˜¯å­—å…¸æ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨
            clash_proxy_dict = node
            node_raw_name = str(node.get('name', '')) # è·å–åŸå§‹åç§°
        elif isinstance(node, str):
            # å¦‚æœæ˜¯URLå­—ç¬¦ä¸²ï¼Œå°è¯•è§£æä¸ºClashå­—å…¸æ ¼å¼
            try:
                parsed_url = urlparse(node)
                # æå–åŸå§‹èŠ‚ç‚¹åç§°ï¼ˆé€šå¸¸åœ¨URLç‰‡æ®µä¸­ï¼‰
                node_raw_name = str(parsed_url.fragment or '') 
                
                # æ£€æŸ¥åè®®æ˜¯å¦æœ‰æ•ˆ
                protocol_scheme = parsed_url.scheme.lower()
                if not any(protocol_scheme == p for p in ["vmess", "trojan", "ss", "ssr", "vless", "hy", "hy2", "hysteria", "hysteria2"]):
                    logging.warning(f"è·³è¿‡æ— æ•ˆåè®®çš„èŠ‚ç‚¹: {node[:50]}...")
                    continue
                
                # æ£€æŸ¥ä¸»æœºåæ˜¯å¦æœ‰æ•ˆ
                host = parsed_url.hostname or ''
                if host and not (is_valid_ip_address(host) or re.match(r'^[a-zA-Z0-9\-\.]+$', host)):
                    logging.warning(f"è·³è¿‡æ— æ•ˆä¸»æœºåçš„èŠ‚ç‚¹: {host} in {node[:50]}...")
                    continue

                # æ ¹æ®åè®®ç±»å‹è¿›è¡Œè§£æå¹¶è½¬æ¢ä¸ºClashå­—å…¸æ ¼å¼
                if protocol_scheme == "vmess":
                    decoded = base64.b64decode(node[len("vmess://"):].encode('utf-8')).decode('utf-8')
                    config = json.loads(decoded)
                    
                    clash_proxy_dict = {
                        'name': str(config.get('ps', 'VMess Node')),
                        'type': 'vmess',
                        'server': config.get('add'),
                        'port': int(config.get('port')),
                        'uuid': config.get('id'),
                        'alterId': int(config.get('aid', 0)),
                        'cipher': config.get('scy', 'auto'), 
                        'network': config.get('net'),
                        'tls': (config.get('tls') == 'tls'),
                        'skip-cert-verify': (config.get('scy', 'false') == 'true'), # scy å­—æ®µåœ¨æŸäº›å·¥å…·ä¸­ä¹Ÿè¡¨ç¤º skip-cert-verify
                        'servername': config.get('sni') or config.get('host') or config.get('add'),
                    }
                    # ws-opts
                    if config.get('net') == 'ws':
                        ws_opts = {}
                        if config.get('path') and config['path'] != '/':
                            ws_opts['path'] = config['path']
                        if config.get('host'):
                            ws_opts['headers'] = {'Host': config['host']}
                        if ws_opts:
                            clash_proxy_dict['ws-opts'] = ws_opts
                    # grpc-opts
                    if config.get('net') == 'grpc':
                        grpc_opts = {}
                        if config.get('path'):
                            grpc_opts['serviceName'] = config['path']
                        if grpc_opts:
                            clash_proxy_dict['grpc-opts'] = grpc_opts
                    
                elif protocol_scheme == "trojan":
                    parsed = urlparse(node)
                    password = parsed.username
                    server = parsed.hostname
                    port = parsed.port
                    query = parse_qs(parsed.query)
                    
                    clash_proxy_dict = {
                        'name': str(parsed.fragment or 'Trojan Node'),
                        'type': 'trojan',
                        'server': server,
                        'port': port,
                        'password': password,
                        'network': query.get('type', ['tcp'])[0],
                        'tls': True,
                        'skip-cert-verify': (query.get('allowInsecure', ['0'])[0] == '1'),
                        'servername': query.get('sni', [server])[0]
                    }
                    if query.get('type', [''])[0] == 'ws':
                        ws_opts = {}
                        if query.get('path', [''])[0] and query['path'][0] != '/':
                            ws_opts['path'] = query['path'][0]
                        if query.get('host', [''])[0]:
                            ws_opts['headers'] = {'Host': query['host'][0]}
                        if ws_opts:
                            clash_proxy_dict['ws-opts'] = ws_opts
                    if query.get('type', [''])[0] == 'grpc':
                        grpc_opts = {}
                        if query.get('serviceName', [''])[0]:
                            grpc_opts['serviceName'] = query['serviceName'][0]
                        if grpc_opts:
                            clash_proxy_dict['grpc-opts'] = grpc_opts

                elif protocol_scheme == "ss":
                    decoded_part = node[len("ss://"):].split('#', 1)[0]
                    try:
                        decoded_info = base64.b64decode(decoded_part.encode('utf-8')).decode('utf-8')
                        parts = decoded_info.split('@', 1)
                        method_password = parts[0].split(':', 1)
                        method = method_password[0]
                        password = method_password[1] if len(method_password) > 1 else ''
                        server_port = parts[1].split(':', 1)
                        server = server_port[0]
                        port = int(server_port[1])
                        
                        clash_proxy_dict = {
                            'name': str(parsed_url.fragment or 'SS Node'),
                            'type': 'ss',
                            'server': server,
                            'port': port,
                            'cipher': method,
                            'password': password,
                        }
                        query_params = parse_qs(parsed_url.query)
                        if 'plugin' in query_params:
                            clash_proxy_dict['plugin'] = query_params.get('plugin', [''])[0]
                            plugin_opts_str = query_params.get('plugin_opts', [''])[0]
                            if plugin_opts_str:
                                plugin_opts = {}
                                for opt in plugin_opts_str.split(';'):
                                    if '=' in opt:
                                        k, v = opt.split('=', 1)
                                        plugin_opts[k] = v
                                clash_proxy_dict['plugin-opts'] = plugin_opts
                    except Exception as e:
                        logging.warning(f"SSèŠ‚ç‚¹è§£æå¤±è´¥: {node[:50]}... - {e}")
                        clash_proxy_dict = None
                
                elif protocol_scheme == "ssr":
                    # SSR é“¾æ¥è§£ææ›´å¤æ‚ï¼Œéœ€è¦å•ç‹¬å¤„ç†
                    # SSR é“¾æ¥æ ¼å¼é€šå¸¸æ˜¯ ssr://<base64_encoded_info>
                    # <base64_encoded_info> = <server>:<port>:<protocol>:<method>:<obfs>:<password_base64_encoded>/?obfsparam=<obfsparam_base64>&protoparam=<protoparam_base64>&remarks=<remarks_base64>&group=<group_base64>&udp=<udp_enabled>
                    try:
                        decoded_info = base64.b64decode(node[len("ssr://"):].split('#', 1)[0]).decode('utf-8')
                        parts = decoded_info.split(':', 5) # server:port:protocol:method:obfs:password
                        server = parts[0]
                        port = int(parts[1])
                        protocol = parts[2]
                        method = parts[3]
                        obfs = parts[4]
                        password_b64 = parts[5].split('/?', 1)[0]
                        password = base64.b64decode(password_b64.encode('utf-8')).decode('utf-8')

                        clash_proxy_dict = {
                            'name': str(parsed_url.fragment or 'SSR Node'),
                            'type': 'ssr', # Clash å¯¹ SSR æ”¯æŒå¯èƒ½æœ‰é™ï¼Œè¿™é‡Œä¿ç•™
                            'server': server,
                            'port': port,
                            'cipher': method,
                            'password': password,
                            'protocol': protocol,
                            'obfs': obfs
                        }
                        
                        query_params_str = parts[5].split('/?', 1)[1] if '/?' in parts[5] else ''
                        query_params = parse_qs(query_params_str)

                        if 'obfsparam' in query_params:
                            clash_proxy_dict['obfs-param'] = base64.b64decode(query_params['obfsparam'][0].encode('utf-8')).decode('utf-8')
                        if 'protoparam' in query_params:
                            clash_proxy_dict['protocol-param'] = base64.b64decode(query_params['protoparam'][0].encode('utf-8')).decode('utf-8')
                        if 'udp' in query_params:
                            clash_proxy_dict['udp'] = (query_params['udp'][0] == '1')
                        
                        # SSR çš„ name é€šå¸¸åœ¨ remarks ä¸­
                        if 'remarks' in query_params:
                             clash_proxy_dict['name'] = base64.b64decode(query_params['remarks'][0].encode('utf-8')).decode('utf-8')
                    except Exception as e:
                        logging.warning(f"SSRèŠ‚ç‚¹è§£æå¤±è´¥: {node[:50]}... - {e}")
                        clash_proxy_dict = None

                elif protocol_scheme == "vless":
                    parsed = urlparse(node)
                    uuid = parsed.username
                    server = parsed.hostname
                    port = parsed.port
                    query = parse_qs(parsed.query)
                    
                    clash_proxy_dict = {
                        'name': str(parsed.fragment or 'VLESS Node'),
                        'type': 'vless',
                        'server': server,
                        'port': port,
                        'uuid': uuid,
                        'network': query.get('type', ['tcp'])[0],
                        'tls': (query.get('security', [''])[0] == 'tls'),
                        'skip-cert-verify': (query.get('allowInsecure', ['0'])[0] == '1'),
                        'servername': query.get('sni', [server])[0],
                        'xudp': (query.get('xudp', ['0'])[0] == '1'),
                        'udp-over-tcp': (query.get('udp_over_tcp', ['false'])[0] == 'true'),
                    }
                    if query.get('flow', [''])[0]:
                        clash_proxy_dict['flow'] = query['flow'][0]

                    if query.get('type', [''])[0] == 'ws':
                        ws_opts = {}
                        if query.get('path', [''])[0] and query['path'][0] != '/':
                            ws_opts['path'] = query['path'][0]
                        if query.get('host', [''])[0]:
                            ws_opts['headers'] = {'Host': query['host'][0]}
                        if ws_opts:
                            clash_proxy_dict['ws-opts'] = ws_opts
                    if query.get('type', [''])[0] == 'grpc':
                        grpc_opts = {}
                        if query.get('serviceName', [''])[0]:
                            grpc_opts['serviceName'] = query['serviceName'][0]
                        if grpc_opts:
                            clash_proxy_dict['grpc-opts'] = grpc_opts
                        
                elif protocol_scheme in ["hysteria", "hy"]:
                    parsed = urlparse(node)
                    server = parsed.hostname
                    port = parsed.port
                    query = parse_qs(parsed.query)

                    clash_proxy_dict = {
                        'name': str(parsed.fragment or 'Hysteria Node'),
                        'type': 'hysteria',
                        'server': server,
                        'port': port,
                        'auth_str': query.get('auth', [''])[0],
                        'network': query.get('protocol', ['udp'])[0],
                        'skip-cert-verify': (query.get('insecure', ['0'])[0] == '1'),
                        'servername': query.get('peer', [server])[0],
                        'tls': True # Hysteria é»˜è®¤å¸¦ TLS
                    }
                    alpn_list = [a.strip() for a in query.get('alpn', [''])[0].split(',') if a.strip()]
                    if alpn_list:
                        clash_proxy_dict['alpn'] = alpn_list
                    if query.get('up_mbps', ['0'])[0] != '0':
                        clash_proxy_dict['up'] = int(query['up_mbps'][0])
                    if query.get('down_mbps', ['0'])[0] != '0':
                        clash_proxy_dict['down'] = int(query['down_mbps'][0])
                    
                elif protocol_scheme in ["hysteria2", "hy2"]:
                    parsed = urlparse(node)
                    password = parsed.username
                    server = parsed.hostname
                    port = parsed.port
                    query = parse_qs(parsed.query)

                    clash_proxy_dict = {
                        'name': str(parsed.fragment or 'Hysteria2 Node'),
                        'type': 'hysteria2',
                        'server': server,
                        'port': port,
                        'password': password,
                        'tls': True,
                        'skip-cert-verify': (query.get('insecure', ['0'])[0] == '1'),
                        'servername': query.get('sni', [server])[0],
                    }
                    if query.get('obfs', [''])[0]:
                        clash_proxy_dict['obfs'] = query['obfs'][0]
                    if query.get('obfsParam', [''])[0]:
                        clash_proxy_dict['obfs-password'] = query['obfsParam'][0]
                    alpn_list = [a.strip() for a in query.get('alpn', [''])[0].split(',') if a.strip()]
                    if alpn_list:
                        clash_proxy_dict['alpn'] = alpn_list

            except Exception as e:
                logging.warning(f"URLèŠ‚ç‚¹è½¬æ¢ä¸ºClashå­—å…¸å¤±è´¥: {node[:50]}... - {e}")
                clash_proxy_dict = None

        if clash_proxy_dict:
            # æ£€æŸ¥èŠ‚ç‚¹åç§°æ˜¯å¦åŒ…å«åˆ é™¤å…³é”®è¯
            name_to_check = str(node_raw_name or clash_proxy_dict.get('name', '')) # ä¼˜å…ˆä½¿ç”¨åŸå§‹åç§°è¿›è¡Œæ£€æŸ¥
            
            should_delete_node = False
            for keyword in DELETE_KEYWORDS:
                try:
                    if keyword.lower() in name_to_check.lower():
                        logging.info(f"èŠ‚ç‚¹ '{name_to_check}' åŒ…å«åˆ é™¤å…³é”®è¯ '{keyword}'ï¼Œå·²è·³è¿‡ã€‚")
                        should_delete_node = True
                        break
                except AttributeError as e: # é˜²æ­¢ name_to_check ä¸æ˜¯å­—ç¬¦ä¸²
                    logging.error(f"æ£€æŸ¥åˆ é™¤å…³é”®è¯æ—¶å‡ºé”™: name_to_check={name_to_check}, type={type(name_to_check)}, node={clash_proxy_dict.get('name', 'Unknown')} - {e}")
                    should_delete_node = True
                    break
            
            if should_delete_node:
                continue

            # æ£€æŸ¥æœåŠ¡å™¨åœ°å€æ˜¯å¦æœ‰æ•ˆ
            server = clash_proxy_dict.get('server', '')
            if server and not (is_valid_ip_address(server) or re.match(r'^[a-zA-Z0-9\-\.]+$', server)):
                logging.warning(f"è·³è¿‡æ— æ•ˆæœåŠ¡å™¨åœ°å€çš„èŠ‚ç‚¹: {server} in {clash_proxy_dict.get('name', 'Unknown')}")
                continue

            # ç”ŸæˆæŒ‡çº¹å¹¶è¿›è¡Œå»é‡
            core_params = _get_node_core_params(clash_proxy_dict)
            fingerprint = _generate_stable_fingerprint_from_params(core_params)
            
            if fingerprint and fingerprint not in unique_node_fingerprints:
                unique_node_fingerprints.add(fingerprint)
                # æ¸…ç†èŠ‚ç‚¹åç§°ï¼Œå¹¶æ·»åŠ åºå·
                clash_proxy_dict['name'] = clean_node_name(
                    clash_proxy_dict.get('name', f"{clash_proxy_dict.get('type', 'Unknown')} {clash_proxy_dict.get('server', '')}:{clash_proxy_dict.get('port', '')}"),
                    index=len(final_clash_proxies) + 1 # ä½¿ç”¨å½“å‰å·²æ”¶é›†åˆ°çš„èŠ‚ç‚¹æ•°é‡ä½œä¸ºåºå·
                )
                final_clash_proxies.append(clash_proxy_dict)
            else:
                logging.debug(f"é‡å¤èŠ‚ç‚¹ï¼ˆæŒ‰æŒ‡çº¹ï¼‰ï¼š{clash_proxy_dict.get('name', '')} - {fingerprint}")
        else:
            logging.debug(f"æ— æ³•è½¬æ¢ä¸ºClashå­—å…¸çš„åŸå§‹èŠ‚ç‚¹æˆ–URL: {str(node)[:80]}...")

    return final_clash_proxies

# --- ä¸»ç¨‹åºæµç¨‹ ---

# ä»ç¯å¢ƒå˜é‡ä¸­è·å– URL_SOURCE
URL_SOURCE = os.environ.get("URL_SOURCE")
print(f"è°ƒè¯•ä¿¡æ¯ - è¯»å–åˆ°çš„ URL_SOURCE å€¼: {URL_SOURCE}")

if not URL_SOURCE:
    print("é”™è¯¯ï¼šç¯å¢ƒå˜é‡ 'URL_SOURCE' æœªè®¾ç½®ã€‚è¯·è®¾ç½®ä¸€ä¸ªåŒ…å«è®¢é˜…é“¾æ¥çš„è¿œç¨‹æ–‡æœ¬æ–‡ä»¶URLã€‚")
    exit(1)

# åˆ›å»ºè¾“å‡ºç›®å½•
os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)
os.makedirs(os.path.dirname(STATISTICS_FILE), exist_ok=True)

# é˜¶æ®µä¸€ï¼šè·å–åŸå§‹URL/å­—ç¬¦ä¸²åˆ—è¡¨
raw_urls_from_source = get_url_list_from_remote(URL_SOURCE)

urls_to_fetch = set() # éœ€è¦é€šè¿‡HTTP/HTTPSè¯·æ±‚çš„URL
url_statistics = [] # ç”¨äºè®°å½•å¤„ç†ç»Ÿè®¡
successful_urls = [] # æˆåŠŸå¤„ç†çš„URLåˆ—è¡¨
failed_urls = [] # å¤±è´¥çš„URLåˆ—è¡¨
all_parsed_nodes_raw = [] # æ‰€æœ‰è§£æåˆ°çš„åŸå§‹èŠ‚ç‚¹ï¼ˆæœªå»é‡å’Œæ ‡å‡†åŒ–ï¼‰

print("\n--- é¢„å¤„ç†åŸå§‹URL/å­—ç¬¦ä¸²åˆ—è¡¨ ---")
for entry in raw_urls_from_source:
    if is_valid_url(entry):
        # å¦‚æœæ˜¯æœ‰æ•ˆçš„HTTP/HTTPS URLï¼ŒåŠ å…¥å¾…è¯·æ±‚åˆ—è¡¨
        urls_to_fetch.add(entry)
    else:
        # å¦‚æœä¸æ˜¯æœ‰æ•ˆçš„URLï¼Œå°è¯•ç›´æ¥è§£æå…¶å†…å®¹ï¼ˆå¯èƒ½æ˜¯Base64ç¼–ç çš„èŠ‚ç‚¹åˆ—è¡¨æˆ–Clashé…ç½®ç‰‡æ®µï¼‰
        print(f"å‘ç°éHTTP/HTTPSæ¡ç›®ï¼Œå°è¯•ç›´æ¥è§£æ: {entry[:80]}...")
        parsed_nodes = parse_content_to_nodes(entry)
        if parsed_nodes:
            all_parsed_nodes_raw.extend(parsed_nodes)
            stat_entry = {'URL': entry, 'èŠ‚ç‚¹æ•°é‡': len(parsed_nodes), 'çŠ¶æ€': 'ç›´æ¥è§£ææˆåŠŸ', 'é”™è¯¯ä¿¡æ¯': '', 'çŠ¶æ€ç ': None}
            url_statistics.append(stat_entry)
            successful_urls.append(entry)
        else:
            stat_entry = {'URL': entry, 'èŠ‚ç‚¹æ•°é‡': 0, 'çŠ¶æ€': 'ç›´æ¥è§£æå¤±è´¥', 'é”™è¯¯ä¿¡æ¯': 'éURLä¸”æ— æ³•è§£æä¸ºèŠ‚ç‚¹', 'çŠ¶æ€ç ': None}
            url_statistics.append(stat_entry)
            failed_urls.append(entry)

print("\n--- é˜¶æ®µä¸€ï¼šå¹¶è¡Œè·å–å¹¶åˆå¹¶æ‰€æœ‰è®¢é˜…é“¾æ¥ä¸­çš„èŠ‚ç‚¹ ---")
total_urls_to_process_via_http = len(urls_to_fetch)

if total_urls_to_process_via_http > 0:
    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œè¯·æ±‚URLï¼Œæé«˜æ•ˆç‡
    # max_workers=16 æ˜¯ä¸€ä¸ªå¸¸ç”¨å€¼ï¼Œå¯ä»¥æ ¹æ®ç½‘ç»œå’ŒCPUæƒ…å†µè°ƒæ•´
    with ThreadPoolExecutor(max_workers=16) as executor:
        future_to_url = {executor.submit(fetch_and_parse_url, url): url for url in urls_to_fetch}
        # tqdm ç”¨äºæ˜¾ç¤ºè¿›åº¦æ¡
        for future in tqdm(as_completed(future_to_url), total=total_urls_to_process_via_http, desc="é€šè¿‡HTTP/HTTPSè¯·æ±‚å¹¶è§£æèŠ‚ç‚¹", mininterval=1.0):
            url = future_to_url[future]
            nodes, success, error_message, status_code = future.result()

            stat_entry = {
                'URL': url,
                'èŠ‚ç‚¹æ•°é‡': len(nodes),
                'çŠ¶æ€': 'æˆåŠŸ' if success else 'å¤±è´¥',
                'é”™è¯¯ä¿¡æ¯': error_message if error_message else '',
                'çŠ¶æ€ç ': status_code
            }
            url_statistics.append(stat_entry)

            if success:
                successful_urls.append(url)
                all_parsed_nodes_raw.extend(nodes)
                print(f"æˆåŠŸå¤„ç† URL: {url}, èŠ‚ç‚¹æ•°: {len(nodes)}, çŠ¶æ€ç : {status_code}")
            else:
                failed_urls.append(url)
                print(f"å¤±è´¥ URL: {url}, é”™è¯¯: {error_message}")

            # æå‰ç»ˆæ­¢æœºåˆ¶ï¼šå¦‚æœå·²æ”¶é›†åˆ°è¶³å¤Ÿå¤šçš„åŸå§‹èŠ‚ç‚¹ï¼ˆMAX_SUCCESSçš„ä¸¤å€ï¼Œè€ƒè™‘åˆ°å»é‡æŸå¤±ï¼‰ï¼Œåˆ™åœæ­¢è¯·æ±‚
            # è¿™é‡Œçš„åˆ¤æ–­æ¡ä»¶å¯ä»¥æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´ï¼Œæ¯”å¦‚ len(all_parsed_nodes_raw) > MAX_SUCCESS * 1.5 
            # ä¹Ÿå¯ä»¥ç›´æ¥ä¸è®¾ç½®æå‰ç»ˆæ­¢ï¼Œç­‰å¾…æ‰€æœ‰URLå¤„ç†å®Œæ¯•
            if len(all_parsed_nodes_raw) >= MAX_SUCCESS * 2:
                print(f"å·²æ”¶é›†è¶³å¤ŸåŸå§‹èŠ‚ç‚¹ ({len(all_parsed_nodes_raw)})ï¼Œè¾¾åˆ° MAX_SUCCESS * 2ï¼Œæå‰ç»ˆæ­¢åç»­è¯·æ±‚ã€‚")
                # æ˜¾å¼å…³é—­çº¿ç¨‹æ± ä¸­çš„çº¿ç¨‹
                executor.shutdown(wait=True, cancel_futures=True) # ç¡®ä¿æ‰€æœ‰ä»»åŠ¡è¢«å–æ¶ˆå¹¶çº¿ç¨‹å…³é—­
                break

# å¯¹æ‰€æœ‰æ”¶é›†åˆ°çš„åŸå§‹èŠ‚ç‚¹è¿›è¡Œå»é‡å’Œæ ‡å‡†åŒ–
final_unique_clash_proxies = deduplicate_and_standardize_nodes(all_parsed_nodes_raw)

# å°†å»é‡åçš„åŸå§‹èŠ‚ç‚¹ï¼ˆå­—å…¸å½¢å¼ï¼‰å†™å…¥ä¸´æ—¶æ–‡ä»¶ï¼Œæ–¹ä¾¿è°ƒè¯•
with open(TEMP_MERGED_NODES_RAW_FILE, 'w', encoding='utf-8') as temp_file:
    for node in final_unique_clash_proxies:
        if isinstance(node, dict):
            temp_file.write(json.dumps(node, ensure_ascii=False) + '\n')
        else: # ç†è®ºä¸Šåˆ°è¿™é‡Œéƒ½åº”è¯¥æ˜¯dictäº†ï¼Œä»¥é˜²ä¸‡ä¸€
            temp_file.write(str(node).strip() + '\n') # ç¡®ä¿å†™å…¥çš„æ˜¯å­—ç¬¦ä¸²

print(f"\né˜¶æ®µä¸€å®Œæˆã€‚åˆå¹¶åˆ° {len(final_unique_clash_proxies)} ä¸ªå”¯ä¸€Clashä»£ç†å­—å…¸ï¼Œå·²ä¿å­˜è‡³ {TEMP_MERGED_NODES_RAW_FILE}")

# å†™å…¥ç»Ÿè®¡æ•°æ®å’ŒURLåˆ—è¡¨
write_statistics_to_csv(url_statistics, STATISTICS_FILE)
write_urls_to_file(successful_urls, SUCCESS_URLS_FILE)
write_urls_to_file(failed_urls, FAILED_URLS_FILE)

print("\n--- é˜¶æ®µäºŒï¼šè¾“å‡ºæœ€ç»ˆ Clash YAML é…ç½® ---")

# ç¡®ä¿è¾“å‡ºæ–‡ä»¶æ˜¯ .yaml æˆ– .yml æ‰©å±•å
if not OUTPUT_FILE.endswith(('.yaml', '.yml')):
    OUTPUT_FILE = os.path.splitext(OUTPUT_FILE)[0] + '.yaml'

# å–å‡ºæœ€å¤š MAX_SUCCESS ä¸ªèŠ‚ç‚¹è¿›è¡Œè¾“å‡º
proxies_to_output = final_unique_clash_proxies[:MAX_SUCCESS]

# æ„å»ºä»£ç†ç»„çš„åç§°åˆ—è¡¨
proxy_names_in_group = []
for node in proxies_to_output:
    if isinstance(node, dict) and 'name' in node:
        proxy_names_in_group.append(node['name'])
    else:
        # å…œåº•å¤„ç†ï¼Œç¡®ä¿å³ä½¿æ²¡æœ‰nameä¹Ÿèƒ½æ·»åŠ åˆ°ç»„
        # è¿™ç§æƒ…å†µé€šå¸¸ä¸åº”è¯¥å‘ç”Ÿï¼Œå› ä¸º clean_node_name ä¼šç¡®ä¿æœ‰åç§°
        proxy_names_in_group.append(f"{node.get('type', 'Unknown')} {node.get('server', '')}")

# æ„å»ºæœ€ç»ˆçš„Clashé…ç½®å­—å…¸
clash_config = {
    'proxies': proxies_to_output,
    'proxy-groups': [
        {
            'name': 'ğŸš€ èŠ‚ç‚¹é€‰æ‹©', # æ‰‹åŠ¨é€‰æ‹©èŠ‚ç‚¹ç»„
            'type': 'select',
            'proxies': ['DIRECT'] + proxy_names_in_group # åŒ…å«ç›´è¿é€‰é¡¹
        },
        {
            'name': 'â™»ï¸ è‡ªåŠ¨é€‰æ‹©', # è‡ªåŠ¨æµ‹é€Ÿé€‰æ‹©æœ€ä½³èŠ‚ç‚¹ç»„
            'type': 'url-test',
            'url': 'http://www.gstatic.com/generate_204', # Googleçš„æ— å†…å®¹å“åº”é¡µé¢ï¼Œå¸¸ç”¨äºæµ‹é€Ÿ
            'interval': 300, # æµ‹é€Ÿé—´éš”300ç§’
            'proxies': proxy_names_in_group
        },
        {
            'name': 'ğŸ“ˆ æ‰‹åŠ¨æ’åº', # å¢åŠ ä¸€ä¸ªæŒ‰pingæ’åºçš„ç»„ï¼Œæ–¹ä¾¿æ‰‹åŠ¨é€‰æ‹©
            'type': 'select',
            'proxies': ['DIRECT'] + sorted(proxy_names_in_group) # æŒ‰åç§°æ’åº
        },
        # å¢åŠ ä¸€äº›å¸¸è§çš„ç­–ç•¥ç»„
        {
            'name': 'ğŸŒ å›½å¤–æµé‡',
            'type': 'select',
            'proxies': ['â™»ï¸ è‡ªåŠ¨é€‰æ‹©', 'ğŸš€ èŠ‚ç‚¹é€‰æ‹©']
        },
        {
            'name': 'ğŸªœ æ¼ç½‘ä¹‹é±¼',
            'type': 'select',
            'proxies': ['â™»ï¸ è‡ªåŠ¨é€‰æ‹©', 'ğŸš€ èŠ‚ç‚¹é€‰æ‹©', 'DIRECT']
        },
        {
            'name': 'ğŸ›‘ å¹¿å‘Šæ‹¦æˆª',
            'type': 'select',
            'proxies': ['REJECT', 'DIRECT']
        },
        {
            'name': 'ğŸ“¢ å…¶ä»–',
            'type': 'select',
            'proxies': ['DIRECT', 'â™»ï¸ è‡ªåŠ¨é€‰æ‹©']
        }
    ],
    'rules': [
        # æ·»åŠ ä¸€äº›åŸºç¡€è§„åˆ™
        'DOMAIN-SUFFIX,cn,DIRECT',
        'GEOIP,CN,DIRECT',
        'MATCH,ğŸš€ èŠ‚ç‚¹é€‰æ‹©' # é»˜è®¤è§„åˆ™ï¼Œæ‰€æœ‰æœªåŒ¹é…çš„æµé‡èµ°èŠ‚ç‚¹é€‰æ‹©ç»„
    ]
}

success_count = len(proxies_to_output)

# å°†Clashé…ç½®å†™å…¥YAMLæ–‡ä»¶
try:
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as out_file:
        # allow_unicode=True ç¡®ä¿ä¸­æ–‡æ­£ç¡®ç¼–ç 
        # default_flow_style=False ç¡®ä¿è¾“å‡ºä¸ºå—æ ·å¼ï¼Œæé«˜å¯è¯»æ€§
        # sort_keys=False ä¿æŒå­—å…¸æ’å…¥é¡ºåºï¼ˆå¯¹äºproxieså’Œproxy-groupså¾ˆé‡è¦ï¼‰
        yaml.dump(clash_config, out_file, allow_unicode=True, default_flow_style=False, sort_keys=False)
    print(f"æœ€ç»ˆ Clash YAML é…ç½®å·²ä¿å­˜è‡³ï¼š{OUTPUT_FILE}")
except Exception as e:
    logging.error(f"å†™å…¥æœ€ç»ˆ Clash YAML æ–‡ä»¶å¤±è´¥: {e}")
    print(f"é”™è¯¯ï¼šå†™å…¥æœ€ç»ˆ Clash YAML æ–‡ä»¶å¤±è´¥: {e}")

# æ¸…ç†ä¸´æ—¶æ–‡ä»¶
if os.path.exists(TEMP_MERGED_NODES_RAW_FILE):
    os.remove(TEMP_MERGED_NODES_RAW_FILE)
    print(f"å·²åˆ é™¤ä¸´æ—¶æ–‡ä»¶ï¼š{TEMP_MERGED_NODES_RAW_FILE}")

# æ‰“å°æœ€ç»ˆè¿è¡Œæ‘˜è¦
print("\n" + "=" * 50)
print("æœ€ç»ˆç»“æœï¼š")
print(f"åŸå§‹æ¥æºæ€»æ¡ç›®æ•°ï¼š{len(raw_urls_from_source)}")
print(f"å…¶ä¸­éœ€è¦HTTP/HTTPSè¯·æ±‚çš„è®¢é˜…é“¾æ¥æ•°ï¼š{len(urls_to_fetch)}")
print(f"å…¶ä¸­ç›´æ¥è§£æçš„éURLå­—ç¬¦ä¸²æ•°ï¼š{len(raw_urls_from_source) - len(urls_to_fetch)}")
print(f"æˆåŠŸå¤„ç†çš„URL/å­—ç¬¦ä¸²æ€»æ•°ï¼š{len(successful_urls)}")
print(f"å¤±è´¥çš„URL/å­—ç¬¦ä¸²æ€»æ•°ï¼š{len(failed_urls)}")
print(f"åˆæ­¥èšåˆçš„åŸå§‹èŠ‚ç‚¹æ•°ï¼ˆå»é‡å’Œè¿‡æ»¤å‰ï¼‰ï¼š{len(all_parsed_nodes_raw)}")
print(f"å»é‡ã€æ ‡å‡†åŒ–å’Œè¿‡æ»¤åçš„å”¯ä¸€Clashä»£ç†æ•°ï¼š{len(final_unique_clash_proxies)}")
print(f"æœ€ç»ˆè¾“å‡ºåˆ°Clash YAMLæ–‡ä»¶çš„èŠ‚ç‚¹æ•°ï¼š{success_count}")
if len(final_unique_clash_proxies) > 0:
    print(f"æœ€ç»ˆæœ‰æ•ˆå†…å®¹ç‡ï¼ˆç›¸å¯¹äºå»é‡è¿‡æ»¤åï¼‰ï¼š{success_count/len(final_unique_clash_proxies):.1%}")
if success_count < MAX_SUCCESS:
    print(f"è­¦å‘Šï¼šæœªèƒ½è¾¾åˆ°ç›®æ ‡æ•°é‡ {MAX_SUCCESS}ï¼ŒåŸå§‹åˆ—è¡¨å¯èƒ½æœ‰æ•ˆURL/èŠ‚ç‚¹ä¸è¶³ï¼Œæˆ–éƒ¨åˆ†URLè·å–å¤±è´¥ã€‚")
print(f"ç»“æœæ–‡ä»¶å·²ä¿å­˜è‡³ï¼š{OUTPUT_FILE}")
print(f"ç»Ÿè®¡æ•°æ®å·²ä¿å­˜è‡³ï¼š{STATISTICS_FILE}")
print(f"æˆåŠŸURLåˆ—è¡¨å·²ä¿å­˜è‡³ï¼š{SUCCESS_URLS_FILE}")
print(f"å¤±è´¥URLåˆ—è¡¨å·²ä¿å­˜è‡³ï¼š{FAILED_URLS_FILE}")
print("=" * 50)
