# -*- coding: utf-8 -*-
import os
import requests
from urllib.parse import urlparse
import base64
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
import argparse
import re
import yaml
import json
import csv

# é…ç½®æ—¥å¿—
logging.basicConfig(filename='error.log', level=logging.ERROR,
                    format='%(asctime)s - %(levelname)s - %(message)s')

# è¯·æ±‚å¤´
headers = {
    'User-Agent': (
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
        'AppleWebKit/537.36 (KHTML, like Gecko) '
        'Chrome/91.0.4472.124 Safari/537.36'
    ),
    'Accept-Encoding': 'gzip, deflate'
}

# å‘½ä»¤è¡Œå‚æ•°è§£æ
parser = argparse.ArgumentParser(description="URLå†…å®¹è·å–è„šæœ¬ï¼Œæ”¯æŒå¤šä¸ªURLæ¥æºå’ŒèŠ‚ç‚¹è§£æ")
parser.add_argument('--max_success', type=int, default=99999, help="ç›®æ ‡æˆåŠŸæ•°é‡")
parser.add_argument('--timeout', type=60, default=60, help="è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰")
parser.add_argument('--output', type=str, default='data/all_clash.yaml', help="è¾“å‡ºæ–‡ä»¶è·¯å¾„") # æ›´æ”¹é»˜è®¤è¾“å‡ºæ–‡ä»¶ç±»å‹ä¸º .yaml
args = parser.parse_args()

# å…¨å±€å˜é‡ï¼Œä»å‘½ä»¤è¡Œå‚æ•°æˆ–é»˜è®¤å€¼è·å–
MAX_SUCCESS = args.max_success
TIMEOUT = args.timeout
OUTPUT_FILE = args.output # ç°åœ¨å°†æ˜¯ .yaml æ–‡ä»¶
TEMP_MERGED_NODES_RAW_FILE = 'temp_merged_nodes_raw.txt' # ä¸´æ—¶å­˜å‚¨åŸå§‹ï¼ˆå»é‡åï¼‰èŠ‚ç‚¹å­—ç¬¦ä¸²
STATISTICS_FILE = 'data/url_statistics.csv'
SUCCESS_URLS_FILE = 'data/successful_urls.txt'
FAILED_URLS_FILE = 'data/failed_urls.txt'

def is_valid_url(url):
    """éªŒè¯URLæ ¼å¼æ˜¯å¦åˆæ³•ï¼Œä»…æ¥å— http æˆ– https æ–¹æ¡ˆ"""
    try:
        result = urlparse(url)
        return all([result.scheme in ['http', 'https'], result.netloc])
    except Exception:
        return False

def get_url_list_from_remote(url_source):
    """ä»ç»™å®šçš„å…¬å¼€ç½‘å€è·å– URL åˆ—è¡¨"""
    try:
        response = requests.get(url_source, headers=headers, timeout=10)
        response.raise_for_status()
        text_content = response.text.strip()
        raw_urls = [line.strip() for line in text_content.splitlines() if line.strip()]
        print(f"ä» {url_source} è·å–åˆ° {len(raw_urls)} ä¸ªURL")
        return raw_urls
    except Exception as e:
        logging.error(f"è·å–URLåˆ—è¡¨å¤±è´¥: {url_source} - {e}")
        return []

def parse_content_to_nodes(content):
    """
    ä»æ–‡æœ¬å†…å®¹ä¸­è§£æå‡ºå„ç§ç±»å‹çš„èŠ‚ç‚¹ã€‚
    è¿”å›çš„èŠ‚ç‚¹æ ¼å¼ä¿æŒåŸå§‹å­—ç¬¦ä¸²æˆ–å­—å…¸å½¢å¼ï¼Œç”¨äºåç»­ç»Ÿä¸€å¤„ç†ã€‚
    """
    if not content:
        return []

    found_nodes = [] # ä½¿ç”¨åˆ—è¡¨ï¼Œå› ä¸ºè¿™é‡Œå¯èƒ½åŒ…å«å­—å…¸ï¼Œæ–¹ä¾¿åç»­å¤„ç†
    processed_content = content

    # 1. å°è¯• Base64 è§£ç 
    try:
        decoded_bytes = base64.b64decode(content)
        processed_content = decoded_bytes.decode('utf-8')
        logging.info("å†…å®¹æˆåŠŸ Base64 è§£ç ã€‚")
    except Exception:
        pass

    # 2. å°è¯• YAML è§£æ (ä¸»è¦ç”¨äº Clash é…ç½®)
    try:
        parsed_data = yaml.safe_load(processed_content)
        if isinstance(parsed_data, dict) and 'proxies' in parsed_data and isinstance(parsed_data['proxies'], list):
            # è¿™æ˜¯ Clash é…ç½®çš„ proxies éƒ¨åˆ†
            for proxy_entry in parsed_data['proxies']:
                if isinstance(proxy_entry, dict):
                    found_nodes.append(proxy_entry) # ç›´æ¥æ·»åŠ å­—å…¸
                elif isinstance(proxy_entry, str) and (
                    proxy_entry.startswith("vmess://") or 
                    proxy_entry.startswith("trojan://") or 
                    proxy_entry.startswith("ss://") or 
                    proxy_entry.startswith("ssr://") or
                    proxy_entry.startswith("vless://") or
                    proxy_entry.startswith("hy://") or 
                    proxy_entry.startswith("hy2://") or 
                    proxy_entry.startswith("hysteria://") or 
                    proxy_entry.startswith("hysteria2://")
                ):
                    found_nodes.append(proxy_entry.strip())
            logging.info("å†…å®¹æˆåŠŸè§£æä¸º Clash YAMLã€‚")
        elif isinstance(parsed_data, list):
            # æœ‰äº›è®¢é˜…å¯èƒ½ç›´æ¥è¿”å›ä¸€ä¸ªèŠ‚ç‚¹åˆ—è¡¨ï¼ˆYAMLæ ¼å¼ï¼‰
            for item in parsed_data:
                if isinstance(item, str):
                    found_nodes.append(item.strip())
                elif isinstance(item, dict): # å…¼å®¹ç›´æ¥è¿”å›å­—å…¸åˆ—è¡¨
                    found_nodes.append(item)
            logging.info("å†…å®¹æˆåŠŸè§£æä¸º YAML åˆ—è¡¨ã€‚")
    except yaml.YAMLError:
        pass
    except Exception as e:
        logging.error(f"YAML è§£æå¤±è´¥: {e}")
        pass

    # 3. é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼æå–èŠ‚ç‚¹ï¼ˆå¤„ç†æ˜æ–‡ã€éæ ‡å‡†æ ¼å¼ç­‰ï¼‰
    node_pattern = re.compile(
        r'(vmess://\S+|'
        r'trojan://\S+|'
        r'ss://\S+|'
        r'ssr://\S+|'
        r'vless://\S+|'
        r'hy://\S+|'
        r'hy2://\S+|'
        r'hysteria://\S+|'
        r'hysteria2://\S+)'
    )
    
    matches = node_pattern.findall(content)
    for match in matches:
        found_nodes.append(match.strip())
    
    if content != processed_content:
        matches_decoded = node_pattern.findall(processed_content)
        for match in matches_decoded:
            found_nodes.append(match.strip())

    return found_nodes

def fetch_and_parse_url(url):
    """
    è·å–URLå†…å®¹å¹¶è§£æå‡ºèŠ‚ç‚¹ã€‚
    è¿”å›ä¸€ä¸ªå…ƒç»„ï¼š(èŠ‚ç‚¹åˆ—è¡¨, æ˜¯å¦æˆåŠŸ, é”™è¯¯ä¿¡æ¯(å¦‚æœå¤±è´¥))
    """
    try:
        resp = requests.get(url, headers=headers, timeout=TIMEOUT)
        resp.raise_for_status()
        content = resp.text.strip()
        
        if len(content) < 10:
            logging.warning(f"è·å–åˆ°å†…å®¹è¿‡çŸ­ï¼Œå¯èƒ½æ— æ•ˆ: {url}")
            return [], False, "å†…å®¹è¿‡çŸ­"
        
        nodes = parse_content_to_nodes(content)
        return nodes, True, None
    except requests.exceptions.Timeout:
        logging.error(f"è¯·æ±‚è¶…æ—¶: {url}")
        return [], False, "è¯·æ±‚è¶…æ—¶"
    except requests.exceptions.RequestException as e:
        logging.error(f"è¯·æ±‚å¤±è´¥: {url} - {e}")
        return [], False, f"è¯·æ±‚å¤±è´¥: {e}"
    except Exception as e:
        logging.error(f"å¤„ç†URLå¼‚å¸¸: {url} - {e}")
        return [], False, f"æœªçŸ¥å¼‚å¸¸: {e}"

def write_statistics_to_csv(statistics_data, filename):
    """å°†ç»Ÿè®¡æ•°æ®å†™å…¥CSVæ–‡ä»¶"""
    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
        fieldnames = ['URL', 'èŠ‚ç‚¹æ•°é‡', 'çŠ¶æ€', 'é”™è¯¯ä¿¡æ¯']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

        writer.writeheader()
        for row in statistics_data:
            writer.writerow(row)
    print(f"ç»Ÿè®¡æ•°æ®å·²ä¿å­˜è‡³ï¼š{filename}")

def write_urls_to_file(urls, filename):
    """å°†URLåˆ—è¡¨å†™å…¥æ–‡ä»¶"""
    with open(filename, 'w', encoding='utf-8') as f:
        for url in urls:
            f.write(url + '\n')
    print(f"URLåˆ—è¡¨å·²ä¿å­˜è‡³ï¼š{filename}")

def deduplicate_and_format_nodes(raw_nodes_list):
    """
    å¯¹æ··åˆæ ¼å¼çš„èŠ‚ç‚¹è¿›è¡Œå»é‡ï¼Œå¹¶è½¬æ¢ä¸ºç»Ÿä¸€çš„Clash YAMLä»£ç†å­—å…¸æˆ–èŠ‚ç‚¹é“¾æ¥ã€‚
    è¿”å›ä¸€ä¸ªåˆ—è¡¨ï¼Œå…¶ä¸­åŒ…å«å”¯ä¸€çš„ä»£ç†å­—å…¸æˆ–èŠ‚ç‚¹é“¾æ¥å­—ç¬¦ä¸²ã€‚
    """
    unique_nodes_processed = set() # ç”¨äºå­˜å‚¨å”¯ä¸€èŠ‚ç‚¹çš„å­—ç¬¦ä¸²è¡¨ç¤ºå½¢å¼ï¼ˆç”¨äºå»é‡ï¼‰
    final_nodes_list = [] # å­˜å‚¨æœ€ç»ˆçš„ä»£ç†å­—å…¸æˆ–èŠ‚ç‚¹é“¾æ¥

    for node in raw_nodes_list:
        if isinstance(node, dict):
            # å°†å­—å…¸è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²ç”¨äºå»é‡ï¼Œç¡®ä¿é”®æ’åºå’ŒéASCIIå­—ç¬¦
            node_identifier = json.dumps(node, sort_keys=True, ensure_ascii=False)
            if node_identifier not in unique_nodes_processed:
                unique_nodes_processed.add(node_identifier)
                final_nodes_list.append(node) # å­˜å‚¨åŸå§‹å­—å…¸
        elif isinstance(node, str):
            # å¯¹äºå­—ç¬¦ä¸²èŠ‚ç‚¹ï¼Œç›´æ¥ä½¿ç”¨å­—ç¬¦ä¸²è¿›è¡Œå»é‡
            if node not in unique_nodes_processed:
                unique_nodes_processed.add(node)
                final_nodes_list.append(node) # å­˜å‚¨åŸå§‹å­—ç¬¦ä¸²
    return final_nodes_list


# --- ä¸»ç¨‹åºæµç¨‹ ---

# ä»ç¯å¢ƒå˜é‡ä¸­è¯»å– URL_SOURCE
URL_SOURCE = os.environ.get("URL_SOURCE")
print(f"è°ƒè¯•ä¿¡æ¯ - è¯»å–åˆ°çš„ URL_SOURCE å€¼: {URL_SOURCE}")

if not URL_SOURCE:
    print("é”™è¯¯ï¼šç¯å¢ƒå˜é‡ 'URL_SOURCE' æœªè®¾ç½®ã€‚æ— æ³•è·å–è®¢é˜…é“¾æ¥ã€‚")
    exit(1)

# ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)
os.makedirs(os.path.dirname(STATISTICS_FILE), exist_ok=True)

# è·å–åŸå§‹çš„URLåˆ—è¡¨ï¼ˆåŒ…å«å¯èƒ½ä¸æ˜¯HTTP/HTTPSçš„æ¡ç›®ï¼‰
raw_urls_from_source = get_url_list_from_remote(URL_SOURCE)

# ç”¨äºå­˜å‚¨éœ€è¦è¿›è¡Œ HTTP è¯·æ±‚çš„è®¢é˜… URL
urls_to_fetch = set()

# å­˜å‚¨æ‰€æœ‰ URL çš„ç»Ÿè®¡ä¿¡æ¯ï¼ŒåŒ…æ‹¬æˆåŠŸå’Œå¤±è´¥çš„ HTTP è¯·æ±‚ä»¥åŠç›´æ¥è§£æçš„ç»“æœ
url_statistics = []
# å­˜å‚¨æˆåŠŸè·å–èŠ‚ç‚¹æˆ–ç›´æ¥è§£ææˆåŠŸçš„åŸå§‹ URL/å­—ç¬¦ä¸²
successful_urls = []
# å­˜å‚¨è·å–å¤±è´¥æˆ–ç›´æ¥è§£æå¤±è´¥çš„åŸå§‹ URL/å­—ç¬¦ä¸²
failed_urls = []

# ç”¨äºé˜¶æ®µä¸€åˆå¹¶æ‰€æœ‰è§£æåˆ°çš„åŸå§‹èŠ‚ç‚¹ï¼ˆå¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–å­—å…¸ï¼‰
all_parsed_nodes_raw = []

# é¢„å¤„ç† raw_urls_from_sourceï¼Œåˆ†ç¦»å‡ºçœŸæ­£éœ€è¦è¯·æ±‚çš„URLå’Œç›´æ¥è§£æçš„èŠ‚ç‚¹å­—ç¬¦ä¸²
print("\n--- é¢„å¤„ç†åŸå§‹URL/å­—ç¬¦ä¸²åˆ—è¡¨ ---")
for entry in raw_urls_from_source:
    if is_valid_url(entry):
        urls_to_fetch.add(entry)
    else:
        # å¦‚æœä¸æ˜¯æœ‰æ•ˆçš„HTTP/HTTPS URLï¼Œå°è¯•å°†å…¶ä½œä¸ºå†…å®¹ç›´æ¥è§£æä¸ºèŠ‚ç‚¹
        print(f"å‘ç°éHTTP/HTTPSæ¡ç›®ï¼Œå°è¯•ç›´æ¥è§£æ: {entry[:80]}...")
        parsed_nodes = parse_content_to_nodes(entry)
        if parsed_nodes:
            all_parsed_nodes_raw.extend(parsed_nodes) # å°†ç›´æ¥è§£æçš„èŠ‚ç‚¹åŠ å…¥æ€»åˆ—è¡¨
            stat_entry = {'URL': entry, 'èŠ‚ç‚¹æ•°é‡': len(parsed_nodes), 'çŠ¶æ€': 'ç›´æ¥è§£ææˆåŠŸ', 'é”™è¯¯ä¿¡æ¯': ''}
            url_statistics.append(stat_entry)
            successful_urls.append(entry)
        else:
            stat_entry = {'URL': entry, 'èŠ‚ç‚¹æ•°é‡': 0, 'çŠ¶æ€': 'ç›´æ¥è§£æå¤±è´¥', 'é”™è¯¯ä¿¡æ¯': 'éURLä¸”æ— æ³•è§£æä¸ºèŠ‚ç‚¹'}
            url_statistics.append(stat_entry)
            failed_urls.append(entry)

# é˜¶æ®µä¸€ï¼šå¹¶è¡Œè·å–å¹¶è§£ææ‰€æœ‰ HTTP/HTTPS è®¢é˜…é“¾æ¥
print("\n--- é˜¶æ®µä¸€ï¼šè·å–å¹¶åˆå¹¶æ‰€æœ‰è®¢é˜…é“¾æ¥ä¸­çš„èŠ‚ç‚¹ ---")
total_urls_to_process_via_http = len(urls_to_fetch)

if total_urls_to_process_via_http > 0:
    with ThreadPoolExecutor(max_workers=16) as executor:
        future_to_url = {executor.submit(fetch_and_parse_url, url): url for url in urls_to_fetch}
        for future in tqdm(as_completed(future_to_url), total=total_urls_to_process_via_http, desc="é€šè¿‡HTTP/HTTPSè¯·æ±‚å¹¶è§£æèŠ‚ç‚¹"):
            url = future_to_url[future]
            nodes, success, error_message = future.result()

            stat_entry = {'URL': url, 'èŠ‚ç‚¹æ•°é‡': len(nodes), 'çŠ¶æ€': 'æˆåŠŸ' if success else 'å¤±è´¥', 'é”™è¯¯ä¿¡æ¯': error_message if error_message else ''}
            url_statistics.append(stat_entry)

            if success:
                successful_urls.append(url)
                all_parsed_nodes_raw.extend(nodes) # å°† HTTP è·å–çš„èŠ‚ç‚¹åŠ å…¥æ€»åˆ—è¡¨
            else:
                failed_urls.append(url)

# å¯¹æ‰€æœ‰æ”¶é›†åˆ°çš„åŸå§‹èŠ‚ç‚¹è¿›è¡Œå»é‡å’Œæ ¼å¼åŒ–
final_unique_clash_proxies = deduplicate_and_format_nodes(all_parsed_nodes_raw)

# å°†å»é‡åçš„åŸå§‹èŠ‚ç‚¹æ•°æ®å†™å…¥ä¸´æ—¶æ–‡ä»¶ï¼ˆç”¨äºäºŒæ¬¡å»é‡å‰çš„ä¿å­˜ï¼‰
# è¿™é‡Œçš„ç›®çš„æ˜¯ä¿å­˜å»é‡åçš„åŸå§‹æ ¼å¼ï¼Œæ–¹ä¾¿åç»­å¤„ç†æˆ–è°ƒè¯•
with open(TEMP_MERGED_NODES_RAW_FILE, 'w', encoding='utf-8') as temp_file:
    for node in final_unique_clash_proxies:
        if isinstance(node, dict):
            # å°†å­—å…¸å†™å…¥ä¸ºYAMLæ ¼å¼çš„å•ä¸ªä»£ç†æ¡ç›®
            yaml.dump([node], temp_file, allow_unicode=True, default_flow_style=False, sort_keys=False)
        else:
            temp_file.write(node.strip() + '\n')

print(f"\né˜¶æ®µä¸€å®Œæˆã€‚åˆå¹¶åˆ° {len(final_unique_clash_proxies)} ä¸ªå”¯ä¸€åŸå§‹èŠ‚ç‚¹ï¼Œå·²ä¿å­˜è‡³ {TEMP_MERGED_NODES_RAW_FILE}")


# å†™å…¥ç»Ÿè®¡æ•°æ®å’ŒURLåˆ—è¡¨æ–‡ä»¶
write_statistics_to_csv(url_statistics, STATISTICS_FILE)
write_urls_to_file(successful_urls, SUCCESS_URLS_FILE)
write_urls_to_file(failed_urls, FAILED_URLS_FILE)


# é˜¶æ®µäºŒï¼šå°†å»é‡å¹¶æ ¼å¼åŒ–åçš„èŠ‚ç‚¹è¾“å‡ºä¸º Clash YAML é…ç½®
print("\n--- é˜¶æ®µäºŒï¼šè¾“å‡ºæœ€ç»ˆ Clash YAML é…ç½® ---")

# ç¡®ä¿è¾“å‡ºæ–‡ä»¶æ˜¯ .yaml æ ¼å¼
if not OUTPUT_FILE.endswith(('.yaml', '.yml')):
    OUTPUT_FILE = os.path.splitext(OUTPUT_FILE)[0] + '.yaml'

# æ„å»ºæœ€ç»ˆçš„ Clash é…ç½®å­—å…¸
clash_config = {
    'proxies': final_unique_clash_proxies[:MAX_SUCCESS], # å–æœ€å¤š MAX_SUCCESS ä¸ªèŠ‚ç‚¹
    'proxy-groups': [
        {
            'name': 'ğŸš€ èŠ‚ç‚¹é€‰æ‹©',
            'type': 'select',
            'proxies': ['DIRECT'] + [p['name'] if isinstance(p, dict) else p.split('#')[-1] for p in final_unique_clash_proxies[:MAX_SUCCESS]]
            # è¿™é‡Œçš„ä»£ç†åç§°éœ€è¦ç»Ÿä¸€å¤„ç†ï¼Œå¦‚æœèŠ‚ç‚¹æ˜¯URLï¼Œéœ€è¦æå–å…¶åç§°éƒ¨åˆ†
            # ä¸ºäº†ç®€åŒ–ï¼Œå¦‚æœèŠ‚ç‚¹æ˜¯URLï¼Œæš‚ç”¨å…¶å®Œæ•´URLä½œä¸ºåç§°ï¼Œå®¢æˆ·ç«¯ä¼šå¤„ç†
            # å®é™…ç”Ÿäº§ä¸­ï¼Œä¼šæ›´å¤æ‚åœ°è§£æURLå¹¶æå–åç§°
        }
    ],
    'rules': [
        'MATCH,ğŸš€ èŠ‚ç‚¹é€‰æ‹©'
    ]
}

# åŠ¨æ€ç”Ÿæˆ proxy-groups ä¸­çš„ä»£ç†åç§°
proxy_names_in_group = []
for node in final_unique_clash_proxies[:MAX_SUCCESS]:
    if isinstance(node, dict):
        if 'name' in node:
            proxy_names_in_group.append(node['name'])
    elif isinstance(node, str):
        # å°è¯•ä»URLä¸­æå–åç§°ï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨æ•´ä¸ªURL
        match = re.search(r'#(.*)$', node)
        if match:
            proxy_names_in_group.append(match.group(1))
        else:
            proxy_names_in_group.append(node) # æ²¡æœ‰åç§°ï¼Œç›´æ¥ä½¿ç”¨URL


# é‡æ–°æ„å»º proxy-groups
clash_config['proxy-groups'] = [
    {
        'name': 'ğŸš€ èŠ‚ç‚¹é€‰æ‹©',
        'type': 'select',
        'proxies': ['DIRECT'] + proxy_names_in_group
    }
]


success_count = len(final_unique_clash_proxies[:MAX_SUCCESS])

# å°† Clash é…ç½®å†™å…¥ YAML æ–‡ä»¶
try:
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as out_file:
        yaml.dump(clash_config, out_file, allow_unicode=True, default_flow_style=False, sort_keys=False)
    print(f"æœ€ç»ˆ Clash YAML é…ç½®å·²ä¿å­˜è‡³ï¼š{OUTPUT_FILE}")
except Exception as e:
    logging.error(f"å†™å…¥æœ€ç»ˆ Clash YAML æ–‡ä»¶å¤±è´¥: {e}")
    print(f"é”™è¯¯ï¼šå†™å…¥æœ€ç»ˆ Clash YAML æ–‡ä»¶å¤±è´¥: {e}")


# æ¸…ç†ä¸´æ—¶æ–‡ä»¶
if os.path.exists(TEMP_MERGED_NODES_RAW_FILE):
    os.remove(TEMP_MERGED_NODES_RAW_FILE)
    print(f"å·²åˆ é™¤ä¸´æ—¶æ–‡ä»¶ï¼š{TEMP_MERGED_NODES_RAW_FILE}")

# æœ€ç»ˆç»“æœæŠ¥å‘Š
print("\n" + "=" * 50)
print("æœ€ç»ˆç»“æœï¼š")
print(f"åŸå§‹æ¥æºæ€»æ¡ç›®æ•°ï¼š{len(raw_urls_from_source)}")
print(f"å…¶ä¸­éœ€è¦HTTP/HTTPSè¯·æ±‚çš„è®¢é˜…é“¾æ¥æ•°ï¼š{len(urls_to_fetch)}")
print(f"å…¶ä¸­ç›´æ¥è§£æçš„éURLå­—ç¬¦ä¸²æ•°ï¼š{len(raw_urls_from_source) - len(urls_to_fetch)}")
print(f"æˆåŠŸå¤„ç†çš„URL/å­—ç¬¦ä¸²æ€»æ•°ï¼š{len(successful_urls)}")
print(f"å¤±è´¥çš„URL/å­—ç¬¦ä¸²æ€»æ•°ï¼š{len(failed_urls)}")
print(f"åˆæ­¥èšåˆçš„å”¯ä¸€åŸå§‹èŠ‚ç‚¹æ•°ï¼ˆå»é‡å‰ï¼‰ï¼š{len(all_parsed_nodes_raw)}")
print(f"å»é‡å¹¶æ ¼å¼åŒ–åçš„å”¯ä¸€èŠ‚ç‚¹æ•°ï¼š{len(final_unique_clash_proxies)}")
print(f"æœ€ç»ˆè¾“å‡ºåˆ°Clash YAMLæ–‡ä»¶çš„èŠ‚ç‚¹æ•°ï¼š{success_count}")
if len(final_unique_clash_proxies) > 0:
    print(f"æœ€ç»ˆæœ‰æ•ˆå†…å®¹ç‡ï¼ˆç›¸å¯¹äºå»é‡ååŸå§‹èŠ‚ç‚¹ï¼‰ï¼š{success_count/len(final_unique_clash_proxies):.1%}")
if success_count < MAX_SUCCESS:
    print("è­¦å‘Šï¼šæœªèƒ½è¾¾åˆ°ç›®æ ‡æ•°é‡ï¼ŒåŸå§‹åˆ—è¡¨å¯èƒ½æœ‰æ•ˆURL/èŠ‚ç‚¹ä¸è¶³ï¼Œæˆ–éƒ¨åˆ†URLè·å–å¤±è´¥ã€‚")
print(f"ç»“æœæ–‡ä»¶å·²ä¿å­˜è‡³ï¼š{OUTPUT_FILE}")
print(f"ç»Ÿè®¡æ•°æ®å·²ä¿å­˜è‡³ï¼š{STATISTICS_FILE}")
print(f"æˆåŠŸURLåˆ—è¡¨å·²ä¿å­˜è‡³ï¼š{SUCCESS_URLS_FILE}")
print(f"å¤±è´¥URLåˆ—è¡¨å·²ä¿å­˜è‡³ï¼š{FAILED_URLS_FILE}")
print("=" * 50)
