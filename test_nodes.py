import aiohttp
import asyncio
import yaml
import os
import subprocess
import time
import argparse
import base64
import json
from urllib.parse import urlparse, parse_qs, unquote
import logging
import psutil
import tempfile
import socket
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Set
import aiofiles

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def load_invalid_nodes(file_path: str) -> List[Dict]:
    """åŠ è½½ä¸Šæ¬¡çš„ä¸å¯ç”¨èŠ‚ç‚¹"""
    if not os.path.exists(file_path):
        return []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            nodes = yaml.safe_load(f) or []
        return nodes
    except Exception as e:
        logger.error(f"åŠ è½½ä¸å¯ç”¨èŠ‚ç‚¹æ–‡ä»¶ {file_path} å¤±è´¥: {e}")
        return []

async def save_nodes(file_path: str, nodes: List[Dict]):
    """å¼‚æ­¥ä¿å­˜èŠ‚ç‚¹åˆ°æ–‡ä»¶"""
    async with aiofiles.open(file_path, 'w', encoding='utf-8') as f:
        await f.write(yaml.dump(nodes, allow_unicode=True))

def get_node_key(proxy: Dict) -> str:
    """ç”ŸæˆèŠ‚ç‚¹å”¯ä¸€æ ‡è¯†"""
    return f"{proxy['server']}:{proxy['port']}:{proxy['name']}"

async def fetch_proxies(url: str) -> List[Dict]:
    """ä»è¿œç¨‹ URL ä¸‹è½½å¹¶è§£æä»£ç†èŠ‚ç‚¹"""
    async with aiohttp.ClientSession() as session:
        try:
            async with session.get(url, timeout=30) as response:
                if response.status != 200:
                    logger.error(f"æ— æ³•ä» {url} è·å–ä»£ç†èŠ‚ç‚¹: HTTP {response.status}")
                    return []
                content = await response.text()
            proxies = [proxy for line in content.splitlines() if (proxy := parse_proxy_line(line.strip()))]
            logger.info(f"ä» {url} åŠ è½½äº† {len(proxies)} ä¸ªä»£ç†èŠ‚ç‚¹")
            return proxies
        except Exception as e:
            logger.error(f"è·å–ä»£ç†èŠ‚ç‚¹å¤±è´¥: {e}")
            return []

def parse_proxy_line(line: str) -> Optional[Dict]:
    """è§£æå•è¡Œä»£ç† URI"""
    try:
        parts = line.split('#', 1)
        uri = parts[0]
        name = unquote(parts[1]) if len(parts) > 1 else f"æœªçŸ¥èŠ‚ç‚¹_{time.time()}"
        url_parts = urlparse(uri)
        scheme = url_parts.scheme.lower()
        proxy = {'name': name, 'tested_at': datetime.now().isoformat()}  # æ·»åŠ æ—¶é—´æˆ³

        if scheme == 'trojan':
            auth_data = url_parts.netloc.split('@')
            if len(auth_data) != 2:
                logger.warning(f"è§£æ Trojan èŠ‚ç‚¹å¤±è´¥ï¼Œæ ¼å¼é”™è¯¯: {uri}")
                return None
            proxy['type'] = 'trojan'
            proxy['password'] = auth_data[0]
            server_port = auth_data[1]
            if ipv6_match := re.match(r'\[(.*?)\]:(\d+)', server_port):
                proxy['server'], proxy['port'] = ipv6_match.group(1), int(ipv6_match.group(2))
            else:
                proxy['server'], proxy['port'] = server_port.split(':')
                proxy['port'] = int(proxy['port'])
            params = parse_qs(url_parts.query)
            proxy['sni'] = params.get('sni', [''])[0]
            proxy['skip-cert-verify'] = params.get('allowInsecure', ['0'])[0] == '1'
            proxy['network'] = params.get('type', ['tcp'])[0]
            proxy['path'] = params.get('path', [''])[0]
            proxy['host'] = params.get('host', [''])[0]
            return proxy
        # å…¶ä»–åè®®è§£æï¼ˆå¦‚ ss, vmess ç­‰ï¼‰ç±»ä¼¼ï¼Œç•¥
        else:
            logger.warning(f"ä¸æ”¯æŒçš„åè®®: {scheme}")
            return None
    except Exception as e:
        logger.warning(f"è§£æä»£ç†è¡Œå¤±è´¥ {line}: {e}")
        return None

def get_free_port() -> int:
    """è·å–ç©ºé—²ç«¯å£"""
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        s.bind(('localhost', 0))
        return s.getsockname()[1]

async def test_proxy(proxy: Dict, session: aiohttp.ClientSession, clash_bin: str, clash_port: int) -> Dict:
    """æµ‹è¯•å•ä¸ªä»£ç†èŠ‚ç‚¹"""
    result = {'proxy': proxy, 'status': 'ä¸å¯ç”¨', 'latency': 0, 'error': ''}
    config = {
        'port': clash_port,
        'socks-port': clash_port + 1,
        'external-controller': f'127.0.0.1:{clash_port + 2}',
        'allow-lan': False,
        'mode': 'rule',
        'log-level': 'error',
        'proxies': [proxy],
        'proxy-groups': [{'name': 'auto', 'type': 'select', 'proxies': [proxy['name']]}],
        'rules': ['MATCH,auto']
    }
    with tempfile.NamedTemporaryFile('w', suffix='.yaml', delete=False, encoding='utf-8') as f:
        config_path = f.name
        yaml.dump(config, f, allow_unicode=True)

    try:
        proc = subprocess.Popen([clash_bin, '-f', config_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        await asyncio.sleep(2)
        try:
            start_time = time.time()
            async with session.get(
                'http://www.cloudflare.com',  # ä½¿ç”¨ Cloudflare ä½œä¸ºæµ‹è¯•ç›®æ ‡
                proxy=f'http://127.0.0.1:{clash_port}',
                timeout=10
            ) as response:
                if response.status == 200:
                    result['status'] = 'å¯ç”¨'
                    result['latency'] = (time.time() - start_time) * 1000
        except Exception as e:
            result['error'] = str(e)
            if proxy['type'] == 'hysteria2':
                stderr = proc.stderr.read().decode()
                if stderr:
                    result['error'] += f" | Mihomo æ—¥å¿—: {stderr}"
        finally:
            proc.terminate()
            await asyncio.sleep(0.2)
    except Exception as e:
        result['error'] = f"é…ç½®ç”Ÿæˆå¤±è´¥: {str(e)}"
    finally:
        try:
            os.remove(config_path)
        except Exception as e:
            logger.warning(f"åˆ é™¤é…ç½®æ–‡ä»¶ {config_path} å¤±è´¥: {e}")
    logger.info(f"ğŸ”’ {proxy['type'].upper()}-{proxy.get('network', 'TCP').upper()}-{'TLS' if proxy.get('sni') else 'NA'} "
                f"{proxy['name']}: {result['status']}, å»¶è¿Ÿ: {result['latency']:.2f}ms")
    return result

async def main():
    """ä¸»å‡½æ•°ï¼Œè¿è¡Œä»£ç†æµ‹è¯•"""
    parser = argparse.ArgumentParser(description='æµ‹è¯•ä»£ç†èŠ‚ç‚¹')
    parser.add_argument('--proxy-url', default='https://raw.githubusercontent.com/qjlxg/aggregator/refs/heads/main/ss.txt',
                        help='ä»£ç†èŠ‚ç‚¹ URL')
    parser.add_argument('--clash-bin', default='./tools/clash', help='Clash äºŒè¿›åˆ¶è·¯å¾„')
    parser.add_argument('--batch-size', type=int, default=max(10, psutil.cpu_count() * 10), help='æ‰¹é‡æµ‹è¯•èŠ‚ç‚¹æ•°')
    parser.add_argument('--invalid-file', default='data/invalid_nodes.yaml', help='ä¸å¯ç”¨èŠ‚ç‚¹æ–‡ä»¶')
    parser.add_argument('--valid-file', default='data/521.yaml', help='å¯ç”¨èŠ‚ç‚¹æ–‡ä»¶')
    parser.add_argument('--expire-days', type=int, default=7, help='ä¸å¯ç”¨èŠ‚ç‚¹è¿‡æœŸå¤©æ•°')
    args = parser.parse_args()

    os.makedirs('data', exist_ok=True)
    
    # åŠ è½½ä¸Šæ¬¡çš„ä¸å¯ç”¨å’Œå¯ç”¨èŠ‚ç‚¹
    invalid_nodes = load_invalid_nodes(args.invalid_file)
    valid_nodes = load_invalid_nodes(args.valid_file)
    invalid_keys = {get_node_key(node) for node in invalid_nodes}
    valid_keys = {get_node_key(node) for node in valid_nodes}

    async with aiohttp.ClientSession() as session:
        # è·å–æœ€æ–°èŠ‚ç‚¹
        proxies = await fetch_proxies(args.proxy_url)
        if not proxies:
            logger.error("æ²¡æœ‰å¯æµ‹è¯•çš„ä»£ç†èŠ‚ç‚¹")
            return

        # è¿‡æ»¤æ–°å¢èŠ‚ç‚¹
        new_proxies = [p for p in proxies if get_node_key(p) not in invalid_keys and get_node_key(p) not in valid_keys]
        logger.info(f"æ€»èŠ‚ç‚¹æ•°: {len(proxies)}, æ–°å¢èŠ‚ç‚¹: {len(new_proxies)}, å·²çŸ¥å¯ç”¨: {len(valid_nodes)}, å·²çŸ¥ä¸å¯ç”¨: {len(invalid_nodes)}")

        # æµ‹è¯•æ–°å¢èŠ‚ç‚¹
        results = []
        base_port = get_free_port()
        for i in range(0, len(new_proxies), args.batch_size):
            batch = new_proxies[i:i + args.batch_size]
            batch_results = await asyncio.gather(
                *(test_proxy(proxy, session, args.clash_bin, base_port + j * 3) for j, proxy in enumerate(batch))
            )
            results.extend(batch_results)

        # åˆå¹¶ç»“æœ
        new_valid = [r['proxy'] for r in results if r['status'] == 'å¯ç”¨']
        new_invalid = [r['proxy'] for r in results if r['status'] == 'ä¸å¯ç”¨']
        
        # æ›´æ–°å¯ç”¨èŠ‚ç‚¹ï¼ˆä¿ç•™æ—§çš„å¯ç”¨èŠ‚ç‚¹ + æ–°æµ‹è¯•çš„å¯ç”¨èŠ‚ç‚¹ï¼‰
        all_valid = valid_nodes + new_valid
        valid_keys = {get_node_key(node) for node in all_valid}
        all_valid = [node for node in all_valid if get_node_key(node) in valid_keys]  # å»é‡

        # æ›´æ–°ä¸å¯ç”¨èŠ‚ç‚¹ï¼ˆä¿ç•™æœªè¿‡æœŸçš„æ—§èŠ‚ç‚¹ + æ–°æµ‹è¯•çš„ä¸å¯ç”¨èŠ‚ç‚¹ï¼‰
        expire_time = datetime.now() - timedelta(days=args.expire_days)
        all_invalid = [node for node in invalid_nodes if 'tested_at' in node and datetime.fromisoformat(node['tested_at']) > expire_time]
        all_invalid.extend(new_invalid)
        invalid_keys = {get_node_key(node) for node in all_invalid}
        all_invalid = [node for node in all_invalid if get_node_key(node) in invalid_keys]  # å»é‡

        # ä¿å­˜ç»“æœ
        await save_nodes(args.valid_file, all_valid)
        await save_nodes(args.invalid_file, all_invalid)

        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        logger.info(f"æµ‹è¯•å®Œæˆ: æ€»èŠ‚ç‚¹æ•°={len(proxies)}, å¯ç”¨èŠ‚ç‚¹={len(all_valid)}, "
                    f"ä¸å¯ç”¨èŠ‚ç‚¹={len(all_invalid)}, å¯ç”¨ç‡={len(all_valid)/len(proxies)*100:.2f}%")

if __name__ == '__main__':
    asyncio.run(main())
