
import httpx
import yaml
import asyncio
import base64
import json
import os  # Corrected import
import urllib.parse
import subprocess
import time
import socket
import re

CLASH_BASE_CONFIG_URLS = [
    "https://raw.githubusercontent.com/qjlxg/collectSub/refs/heads/main/config_all_merged_nodes.txt",
    "https://raw.githubusercontent.com/qjlxg/collectSub/refs/heads/main/all_nodes.txt",
                         ]

def parse_node_link_to_clash_proxy(link: str, index: int = 0) -> dict | None:
    """å°è¯•å°†èŠ‚ç‚¹é“¾æ¥ï¼ˆss, vmess, trojan, hy2, vlessï¼‰è§£æä¸º Clash ä»£ç†å­—å…¸æ ¼å¼ã€‚"""
    if not link or "://" not in link:
        print(f"âŒ é”™è¯¯ï¼šæ— æ•ˆé“¾æ¥ï¼Œæ— åè®®åˆ†éš”ç¬¦ï¼š{link}")
        return None
    try:
        scheme, remainder = link.split("://", 1)
        name_part = None
        if "#" in remainder:
            remainder, name_part = remainder.split("#", 1)
            try:
                name_part = urllib.parse.unquote(name_part)
            except Exception as e:
                print(f"âš ï¸ è­¦å‘Šï¼šèŠ‚ç‚¹åç§°è§£ç å¤±è´¥ï¼š{name_part} - {e}")
                name_part = None
        # ç”Ÿæˆå”¯ä¸€åç§°ï¼Œé¿å…é‡å¤
        proxy = {
            "name": name_part if name_part else f"{scheme.upper()}-{index}-{remainder.split('@')[1].split('?')[0].replace(':', '-')}",
            "type": scheme.lower()
        }
        if scheme == "ss":
            try:
                if "@" not in remainder:
                    print(f"âŒ é”™è¯¯ï¼šSS é“¾æ¥æ ¼å¼ä¸æ­£ç¡®ï¼Œç¼ºå°‘ @ åˆ†éš”ç¬¦ï¼š{link}")
                    return None
                base64_part_raw, server_port = remainder.split("@", 1)
                print(f"è°ƒè¯•ï¼šåŸå§‹ Base64 éƒ¨åˆ† = {base64_part_raw}")
                # æ¸…ç† URL ç¼–ç å­—ç¬¦
                base64_part_raw = urllib.parse.unquote(base64_part_raw)
                # éªŒè¯ Base64 å­—ç¬¦
                valid_base64_chars = set("ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/=")
                if not all(c in valid_base64_chars for c in base64_part_raw):
                    print(f"âŒ é”™è¯¯ï¼šSS é“¾æ¥åŒ…å«æ— æ•ˆ Base64 å­—ç¬¦ï¼š{link}")
                    return None
                if len(base64_part_raw) < 4:
                    print(f"âŒ é”™è¯¯ï¼šSS é“¾æ¥ Base64 éƒ¨åˆ†è¿‡çŸ­ï¼š{link}")
                    return None
                missing_padding = len(base64_part_raw) % 4
                base64_part = base64_part_raw + '=' * (4 - missing_padding) if missing_padding else base64_part_raw
                decoded_userinfo = base64.urlsafe_b64decode(base64_part).decode('utf-8')
                method, password = decoded_userinfo.split(":", 1)
                if ":" not in server_port:
                    print(f"âŒ é”™è¯¯ï¼šSS é“¾æ¥æœåŠ¡å™¨ç«¯å£æ ¼å¼é”™è¯¯ï¼š{link}")
                    return None
                server, port = server_port.split(":", 1)
                proxy.update({
                    "type": "ss",
                    "server": server,
                    "port": int(port),
                    "cipher": method,
                    "password": password
                })
            except base64.binascii.Error as e:
                print(f"âŒ é”™è¯¯ï¼šSS é“¾æ¥ Base64 è§£ç å¤±è´¥ï¼š{link} - {e}")
                return None
            except ValueError as e:
                print(f"âŒ é”™è¯¯ï¼šSS é“¾æ¥æ ¼å¼é”™è¯¯ï¼š{link} - {e}")
                return None
            except Exception as e:
                print(f"âŒ é”™è¯¯ï¼šè§£æ SS é“¾æ¥æœªçŸ¥é”™è¯¯ï¼š{link} - {e}")
                return None
        elif scheme == "vmess":
            try:
                missing_padding = len(remainder) % 4
                vmess_base64 = remainder + '=' * (4 - missing_padding) if missing_padding else remainder
                decoded_json_str = base64.urlsafe_b64decode(vmess_base64).decode('utf-8')
                vmess_config = json.loads(decoded_json_str)
                proxy.update({
                    "type": "vmess",
                    "server": vmess_config.get("add"),
                    "port": int(vmess_config.get("port")),
                    "uuid": vmess_config.get("id"),
                    "alterId": int(vmess_config.get("aid", 0)),
                    "cipher": vmess_config.get("scy", "auto"),
                    "network": vmess_config.get("net", "tcp"),
                    "tls": vmess_config.get("tls") == "tls",
                    "servername": vmess_config.get("sni"),
                    "ws-path": vmess_config.get("path", "/"),
                    "ws-headers": {"Host": vmess_config.get("host")} if vmess_config.get("host") else {}
                })
                proxy = {k: v for k, v in proxy.items() if v not in [None, '', {}]}
            except Exception as e:
                print(f"âŒ é”™è¯¯ï¼šè§£æ Vmess é“¾æ¥å¤±è´¥ï¼š{link} - {e}")
                return None
        elif scheme == "trojan":
            try:
                password_server_port, query_params_str = remainder.split("?", 1) if "?" in remainder else (remainder, "")
                password, server_port = password_server_port.split("@", 1)
                server, port = server_port.split(":", 1)
                proxy.update({
                    "type": "trojan",
                    "server": server,
                    "port": int(port),
                    "password": urllib.parse.unquote(password)
                })
                if query_params_str:
                    query_params = urllib.parse.parse_qs(query_params_str)
                    if "security" in query_params and query_params["security"][0] == "tls":
                        proxy["tls"] = True
                    if "sni" in query_params:
                        proxy["servername"] = query_params["sni"][0]
                    if "allowInsecure" in query_params and query_params["allowInsecure"][0] == "1":
                        proxy["skip-cert-verify"] = True
                    if "type" in query_params:
                        proxy["network"] = query_params["type"][0]
            except Exception as e:
                print(f"âŒ é”™è¯¯ï¼šè§£æ Trojan é“¾æ¥å¤±è´¥ï¼š{link} - {e}")
                return None
        elif scheme == "hy2":
            try:
                password_server_port, query_params_str = remainder.split("?", 1) if "?" in remainder else (remainder, "")
                password_encoded, server_port = password_server_port.split("@", 1)
                server, port = server_port.split(":", 1)
                proxy.update({
                    "type": "hysteria2",
                    "server": server,
                    "port": int(port),
                    "password": password_encoded,
                })
                if query_params_str:
                    query_params = urllib.parse.parse_qs(query_params_str)
                    if "insecure" in query_params and query_params["insecure"][0] == "1":
                        proxy["skip-cert-verify"] = True
                    if "sni" in query_params:
                        proxy["servername"] = query_params["sni"][0]
                if not proxy.get("name"):
                    proxy["name"] = f"HY2-{server}-{port}"
            except Exception as e:
                print(f"âŒ é”™è¯¯ï¼šè§£æ Hysteria2 é“¾æ¥å¤±è´¥ï¼š{link} - {e}")
                return None
        elif scheme == "vless":
            try:
                uuid_server_port, query_params_str = remainder.split("?", 1) if "?" in remainder else (remainder, "")
                uuid, server_port = uuid_server_port.split("@", 1)
                server, port = server_port.split(":", 1)
                proxy.update({
                    "type": "vless",
                    "server": server,
                    "port": int(port),
                    "uuid": uuid,
                    "cipher": "auto"
                })
                if query_params_str:
                    query_params = urllib.parse.parse_qs(query_params_str)
                    if "security" in query_params and query_params["security"][0] == "tls":
                        proxy["tls"] = True
                    if "sni" in query_params:
                        proxy["servername"] = query_params["sni"][0]
                    if "type" in query_params:
                        proxy["network"] = query_params["type"][0]
                    if "path" in query_params:
                        proxy["ws-path"] = query_params["path"][0]
                    if "host" in query_params:
                        proxy["ws-headers"] = {"Host": query_params["host"][0]}
                if not proxy.get("name"):
                    proxy["name"] = f"VLESS-{server}-{port}"
            except Exception as e:
                print(f"âŒ é”™è¯¯ï¼šè§£æ Vless é“¾æ¥å¤±è´¥ï¼š{link} - {e}")
                return None
        else:
            print(f"âš ï¸ è­¦å‘Šï¼šè·³è¿‡ä¸æ”¯æŒçš„åè®®ç±»å‹ï¼š{scheme} (é“¾æ¥: {link})")
            return None
        return proxy
    except Exception as e:
        print(f"âŒ é”™è¯¯ï¼šè§£ææœªçŸ¥é“¾æ¥æ ¼å¼å¤±è´¥ï¼š{link} - {e}")
        return None

def generate_plaintext_node_link(proxy: dict) -> str | None:
    """æ ¹æ® Clash ä»£ç†å­—å…¸ç”Ÿæˆæ˜æ–‡èŠ‚ç‚¹é“¾æ¥ï¼ˆä¾‹å¦‚ ss://, vmess://ï¼‰ã€‚"""
    p_type = proxy.get("type")
    p_name = proxy.get("name", "Unnamed Node")
    if p_type == "ss":
        server = proxy.get("server")
        port = proxy.get("port")
        password = proxy.get("password")
        cipher = proxy.get("cipher")
        if all([server, port, password, cipher]):
            userinfo = f"{cipher}:{password}@{server}:{port}"
            encoded_userinfo = base64.urlsafe_b64encode(userinfo.encode()).decode().rstrip('=')
            safe_name = urllib.parse.quote(p_name)
            return f"ss://{encoded_userinfo}#{safe_name}"
    elif p_type == "vmess":
        server = proxy.get("server")
        port = proxy.get("port")
        uuid = proxy.get("uuid")
        alterId = proxy.get("alterId", 0)
        cipher = proxy.get("cipher", "auto")
        network = proxy.get("network", "tcp")
        tls = proxy.get("tls", False)
        servername = proxy.get("servername", "")
        ws_path = proxy.get("ws-path", "")
        ws_headers = proxy.get("ws-headers", {}).get("Host", "")
        if all([server, port, uuid]):
            vmess_obj = {
                "v": "2",
                "ps": p_name,
                "add": server,
                "port": str(port),
                "id": uuid,
                "aid": str(alterId),
                "scy": cipher,
                "net": network,
            }
            if ws_path:
                vmess_obj["path"] = ws_path
            if ws_headers:
                vmess_obj["host"] = ws_headers
            if tls:
                vmess_obj["tls"] = "tls"
            if servername:
                vmess_obj["sni"] = servername
            vmess_obj = {k: v for k, v in vmess_obj.items() if v}
            try:
                vmess_json = json.dumps(vmess_obj, ensure_ascii=False)
                encoded_vmess = base64.urlsafe_b64encode(vmess_json.encode('utf-8')).decode('utf-8').rstrip('=')
                return f"vmess://{encoded_vmess}"
            except Exception as e:
                print(f"âŒ é”™è¯¯ï¼šç”Ÿæˆ Vmess é“¾æ¥å¤±è´¥ï¼ŒèŠ‚ç‚¹ï¼š{p_name}ï¼Œé”™è¯¯ï¼š{e}")
                return None
    elif p_type == "trojan":
        server = proxy.get("server")
        port = proxy.get("port")
        password = proxy.get("password")
        tls = proxy.get("tls", False)
        sni = proxy.get("servername", "")
        skip_cert_verify = proxy.get("skip-cert-verify", False)
        network = proxy.get("network", "tcp")
        if all([server, port, password]):
            params = []
            if tls:
                params.append("security=tls")
            if sni:
                params.append(f"sni={sni}")
            if skip_cert_verify:
                params.append("allowInsecure=1")
            if network != "tcp":
                params.append(f"type={network}")
            param_str = "&".join(params)
            encoded_password = urllib.parse.quote(password)
            safe_name = urllib.parse.quote(p_name)
            link = f"trojan://{encoded_password}@{server}:{port}"
            if param_str:
                link += f"?{param_str}"
            link += f"#{safe_name}"
            return link
    elif p_type == "hysteria2":
        server = proxy.get("server")
        port = proxy.get("port")
        password = proxy.get("password")
        skip_cert_verify = proxy.get("skip-cert-verify", False)
        servername = proxy.get("servername", "")
        if all([server, port, password]):
            params = []
            if skip_cert_verify:
                params.append("insecure=1")
            if servername:
                params.append(f"sni={servername}")
            param_str = "&".join(params)
            encoded_password = urllib.parse.quote(password)
            safe_name = urllib.parse.quote(p_name)
            link = f"hy2://{encoded_password}@{server}:{port}"
            if param_str:
                link += f"?{param_str}"
            link += f"#{safe_name}"
            return link
    elif p_type == "vless":
        server = proxy.get("server")
        port = proxy.get("port")
        uuid = proxy.get("uuid")
        tls = proxy.get("tls", False)
        servername = proxy.get("servername", "")
        network = proxy.get("network", "tcp")
        ws_path = proxy.get("ws-path", "")
        ws_host = proxy.get("ws-headers", {}).get("Host", "")
        if all([server, port, uuid]):
            params = []
            if tls:
                params.append("security=tls")
            if servername:
                params.append(f"sni={servername}")
            if network:
                params.append(f"type={network}")
            if ws_path:
                params.append(f"path={urllib.parse.quote(ws_path)}")
            if ws_host:
                params.append(f"host={urllib.parse.quote(ws_host)}")
            param_str = "&".join(params)
            safe_name = urllib.parse.quote(p_name)
            link = f"vless://{uuid}@{server}:{port}"
            if param_str:
                link += f"?{param_str}"
            link += f"#{safe_name}"
            return link
    return None

async def fetch_all_configs(urls: list[str]) -> list:
    """ä» URL åˆ—è¡¨è·å–çº¯æ–‡æœ¬èŠ‚ç‚¹é“¾æ¥ï¼Œå¹¶è§£æä¸º Clash ä»£ç†å­—å…¸ã€‚"""
    all_proxies = []
    async with httpx.AsyncClient(timeout=30.0) as client:
        for url in urls:
            try:
                print(f"ğŸ”„ æ­£åœ¨ä» {url} è·å–èŠ‚ç‚¹é“¾æ¥åˆ—è¡¨...")
                response = await client.get(url)
                response.raise_for_status()
                node_links_content = response.text
                lines = node_links_content.strip().split("\n")
                parsed_count = 0
                for i, line in enumerate(lines):
                    line = line.strip()
                    if not line:
                        continue
                    proxy_obj = parse_node_link_to_clash_proxy(line, index=i)
                    if proxy_obj:
                        all_proxies.append(proxy_obj)
                        parsed_count += 1
                print(f"âœ… æˆåŠŸä» {url} è§£æåˆ° {parsed_count} ä¸ªä»£ç†èŠ‚ç‚¹ã€‚")
            except httpx.RequestError as e:
                print(f"âŒ é”™è¯¯ï¼šä» {url} è·å–èŠ‚ç‚¹é“¾æ¥å¤±è´¥ï¼š{e}")
            except Exception as e:
                print(f"âŒ å‘ç”ŸæœªçŸ¥é”™è¯¯ï¼Œå¤„ç† {url} æ—¶å‡ºç°ï¼š{e}")
    return all_proxies

async def test_clash_meta_nodes(clash_core_path: str, config_path: str, api_port: int = 9090, retries: int = 3) -> list:
    """å¯åŠ¨ Clash.Meta æ ¸å¿ƒï¼ŒåŠ è½½é…ç½®æ–‡ä»¶ï¼Œæµ‹è¯•ä»£ç†èŠ‚ç‚¹å»¶è¿Ÿã€‚"""
    tested_nodes_info = []
    async def read_stream_and_print(stream, name, log_file):
        with open(log_file, "a", encoding="utf-8") as f:
            while True:
                line = await stream.readline()
                if not line:
                    break
                line_str = line.decode('utf-8', errors='ignore').strip()
                print(f"[{name}] {line_str}")
                f.write(f"[{name}] {line_str}\n")
            print(f"[{name}] Stream finished.")
            f.write(f"[{name}] Stream finished.\n")
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        if s.connect_ex(('127.0.0.1', api_port)) == 0:
            print(f"âŒ é”™è¯¯ï¼šç«¯å£ {api_port} å·²è¢«å ç”¨ï¼Œè¯·æ›´æ¢ç«¯å£æˆ–é‡Šæ”¾ç«¯å£")
            return []
    for attempt in range(retries):
        clash_process = None
        stdout_task = None
        stderr_task = None
        print(f"\nğŸš€ å°è¯•å¯åŠ¨ Clash.Meta æ ¸å¿ƒ (ç¬¬ {attempt + 1}/{retries})...")
        try:
            if not os.path.isfile(clash_core_path) or not os.access(clash_core_path, os.X_OK):
                print(f"âŒ é”™è¯¯ï¼šClash.Meta å¯æ‰§è¡Œæ–‡ä»¶ä¸å¯ç”¨æˆ–æ— æ‰§è¡Œæƒé™ï¼š{clash_core_path}")
                return []
            clash_process = await asyncio.create_subprocess_exec(
                clash_core_path,
                "-f", config_path,
                "-d", "./data",
                "-ext-ctl", f"0.0.0.0:{api_port}",
                "-ext-ui", "ui",
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE
            )
            print(f"Clash.Meta è¿›ç¨‹å·²å¯åŠ¨ï¼ŒPID: {clash_process.pid}")
            stdout_task = asyncio.create_task(read_stream_and_print(clash_process.stdout, "Clash_STDOUT", "data/clash_stdout.log"))
            stderr_task = asyncio.create_task(read_stream_and_print(clash_process.stderr, "Clash_STDERR", "data/clash_stderr.log"))
            api_url_base = f"http://127.0.0.1:{api_port}"
            proxies_api_url = f"{api_url_base}/proxies"
            max_wait_time = 75
            wait_interval = 2
            print(f"æ­£åœ¨å°è¯•è¿æ¥ Clash.Meta API ({api_url_base})...")
            async with httpx.AsyncClient(timeout=10.0) as client:
                connected = False
                for i in range(int(max_wait_time / wait_interval)):
                    try:
                        response = await client.get(proxies_api_url, timeout=wait_interval)
                        response.raise_for_status()
                        print(f"âœ… æˆåŠŸè¿æ¥åˆ° Clash.Meta API (è€—æ—¶çº¦ {i * wait_interval} ç§’)ã€‚")
                        connected = True
                        break
                    except httpx.RequestError:
                        if clash_process.returncode is not None:
                            print(f"âš ï¸ Clash.Meta è¿›ç¨‹å·²æå‰é€€å‡º (Exit Code: {clash_process.returncode})")
                            break
                        print(f"â³ ç­‰å¾… Clash.Meta API ({i * wait_interval + wait_interval}s/{max_wait_time}s)...")
                        await asyncio.sleep(wait_interval)
                if not connected:
                    print(f"âŒ è¶…è¿‡ {max_wait_time} ç§’æœªè¿æ¥åˆ° Clash.Meta API")
                    continue
                all_proxies_data = response.json()
                proxy_names = []
                for proxy_name, details in all_proxies_data.get("proxies", {}).items():
                    if details.get("type") not in ["Fallback", "Selector", "URLTest", "LoadBalance", "Direct", "Reject"]:
                        proxy_names.append(proxy_name)
                print(f"æˆåŠŸè·å–åˆ° {len(proxy_names)} ä¸ªå¯æµ‹è¯•ä»£ç†çš„åç§°ã€‚")
                if not proxy_names:
                    print("ğŸ¤· æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å¯æµ‹è¯•çš„ä»£ç†èŠ‚ç‚¹ã€‚")
                    return []
                print("\nğŸ”¬ æ­£åœ¨æµ‹è¯•ä»£ç†èŠ‚ç‚¹å»¶è¿Ÿ...")
                tasks = []
                for name in proxy_names:
                    test_url = f"{proxies_api_url}/{urllib.parse.quote(name)}/delay?timeout=5000&url=http://www.google.com/generate_204"
                    tasks.append(client.get(test_url, timeout=10))
                results = await asyncio.gather(*tasks, return_exceptions=True)
                for i, result in enumerate(results):
                    node_name = proxy_names[i]
                    if isinstance(result, httpx.Response):
                        try:
                            delay_data = result.json()
                            delay = delay_data.get("delay", -1)
                            if delay > 0:
                                print(f"âœ… {node_name}: {delay}ms")
                                tested_nodes_info.append({"name": node_name, "delay": delay})
                            else:
                                print(f"ğŸ’” {node_name}: æµ‹è¯•å¤±è´¥/è¶…æ—¶ ({delay_data.get('message', 'æœªçŸ¥é”™è¯¯')})")
                        except json.JSONDecodeError:
                            print(f"ğŸ’” {node_name}: å“åº”è§£æå¤±è´¥")
                    else:
                        print(f"ğŸ’” {node_name}: è¯·æ±‚é”™è¯¯ - {result}")
                tested_nodes_info.sort(key=lambda x: x["delay"])
                return tested_nodes_info
        except Exception as e:
            print(f"âŒ èŠ‚ç‚¹æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        finally:
            if clash_process and clash_process.returncode is None:
                print("ğŸ›‘ æ­£åœ¨åœæ­¢ Clash.Meta è¿›ç¨‹...")
                clash_process.terminate()
                try:
                    await asyncio.wait_for(clash_process.wait(), timeout=5)
                except asyncio.TimeoutError:
                    clash_process.kill()
            if stdout_task:
                stdout_task.cancel()
                try:
                    await stdout_task
                except asyncio.CancelledError:
                    pass
            if stderr_task:
                stderr_task.cancel()
                try:
                    await stderr_task
                except asyncio.CancelledError:
                    pass
    print(f"âŒ ç»è¿‡ {retries} æ¬¡å°è¯•ï¼ŒClash.Meta æµ‹è¯•å¤±è´¥")
    return tested_nodes_info

async def main():
    print("ğŸš€ å¼€å§‹ä» URL è·å–æ˜æ–‡èŠ‚ç‚¹é“¾æ¥åˆ—è¡¨å¹¶å¤„ç†...")
    os.makedirs("data", exist_ok=True)
    for log_file in ["data/clash_stdout.log", "data/clash_stderr.log"]:
        if os.path.exists(log_file):
            with open(log_file, "w", encoding="utf-8") as f:
                f.write("")
    all_proxies = await fetch_all_configs(CLASH_BASE_CONFIG_URLS)
    print(f"\nâœ… æ€»å…±ä»é“¾æ¥è§£æåˆ° {len(all_proxies)} ä¸ªä»£ç†èŠ‚ç‚¹ã€‚")
    if not all_proxies:
        print("ğŸ¤· æ²¡æœ‰æ‰¾åˆ°ä»»ä½•èŠ‚ç‚¹ï¼Œæ— æ³•è¿›è¡Œæµ‹è¯•å’Œç”Ÿæˆé“¾æ¥ã€‚")
        with open("data/all.txt", "w", encoding="utf-8") as f:
            f.write("")
        return
    unique_proxies_map = {}
    for proxy in all_proxies:
        key = (
            proxy.get("type"),
            proxy.get("server"),
            proxy.get("port"),
            proxy.get("password", ""),
            proxy.get("cipher", ""),
            proxy.get("uuid", ""),
            proxy.get("tls", False)
        )
        if key not in unique_proxies_map:
            unique_proxies_map[key] = proxy
        else:
            print(f"  â¡ï¸ è·³è¿‡é‡å¤èŠ‚ç‚¹: {proxy.get('name')} ({proxy.get('type')}, {proxy.get('server')}:{proxy.get('port')})")
    unique_proxies = list(unique_proxies_map.values())
    print(f"âœ¨ è¿‡æ»¤é‡å¤åå‰©ä½™ {len(unique_proxies)} ä¸ªå”¯ä¸€èŠ‚ç‚¹ã€‚")
    # æ£€æŸ¥ä»£ç†åç§°å”¯ä¸€æ€§
    proxy_names = set()
    for proxy in unique_proxies:
        name = proxy.get("name")
        if name in proxy_names:
            print(f"âš ï¸ è­¦å‘Šï¼šå‘ç°é‡å¤ä»£ç†åç§°ï¼š{name}ï¼Œæ­£åœ¨é‡å‘½å...")
            proxy["name"] = f"{name}-{len(proxy_names)}"
        proxy_names.add(proxy["name"])
    unified_clash_config = {
        "proxies": unique_proxies,
        "proxy-groups": [
            {
                "name": "Proxy All",
                "type": "select",
                "proxies": [p.get("name") for p in unique_proxies if p.get("name")]
            },
            {
                "name": "Auto Select (URLTest)",
                "type": "url-test",
                "proxies": [p.get("name") for p in unique_proxies if p.get("name")],
                "url": "http://www.google.com/generate_204",
                "interval": 300
            }
        ],
        "rules": [
            "MATCH,Proxy All"
        ],
        "dns": {
            "enable": True,
            "ipv6": False,
            "listen": "0.0.0.0:53",
            "enhanced-mode": "fake-ip",
            "default-nameserver": [
                "114.114.114.114",
                "8.8.8.8"
            ],
            "nameserver": [
                "tls://dns.google/dns-query",
                "https://dns.alidns.com/dns-query"
            ]
        },
        "log-level": "info",
        "port": 7890,
        "socks-port": 7891,
        "allow-lan": True,
        "external-controller": "0.0.0.0:9090",
        "external-ui": "ui"
    }
    unified_config_path = "data/unified_clash_config.yaml"
    try:
        with open(unified_config_path, "w", encoding="utf-8") as f:
            yaml.dump(unified_clash_config, f, allow_unicode=True, sort_keys=False, default_flow_style=False)
        with open(unified_config_path, "r", encoding="utf-8") as f:
            config_content = yaml.safe_load(f)
            if "mode" in config_content:
                print(f"âš ï¸ è­¦å‘Šï¼šé…ç½®æ–‡ä»¶ä¸­åŒ…å« mode å­—æ®µï¼š{config_content['mode']}")
            else:
                print(f"âœ… é…ç½®æ–‡ä»¶éªŒè¯é€šè¿‡ï¼Œæ—  mode å­—æ®µ")
        print(f"ğŸ“¦ ç»Ÿä¸€çš„ Clash é…ç½®æ–‡ä»¶å·²ç”Ÿæˆï¼š{unified_config_path}")
    except Exception as e:
        print(f"âŒ é”™è¯¯ï¼šç”Ÿæˆç»Ÿä¸€ Clash é…ç½®æ–‡ä»¶å¤±è´¥ï¼š{e}")
        return
    clash_core_path = os.environ.get("CLASH_CORE_PATH")
    if not clash_core_path:
        print("âŒ é”™è¯¯ï¼šç¯å¢ƒå˜é‡ CLASH_CORE_PATH æœªè®¾ç½®ï¼Œæ— æ³•æ‰§è¡Œ Clash.Meta æµ‹è¯•ã€‚")
        output_file_path = "data/all.txt"
        with open(output_file_path, "w", encoding="utf-8") as f:
            for link in [generate_plaintext_node_link(node) for node in unique_proxies if generate_plaintext_node_link(node)]:
                f.write(link + "\n")
        print(f"â¡ï¸ ä»…ç”Ÿæˆæ˜æ–‡é“¾æ¥åˆ°ï¼š{output_file_path}")
        print(f"æ€»å…±ç”Ÿæˆ {len(unique_proxies)} æ¡æ˜æ–‡é“¾æ¥ã€‚")
        return
    print("\n--- å¼€å§‹ä½¿ç”¨ Clash.Meta è¿›è¡ŒèŠ‚ç‚¹å»¶è¿Ÿæµ‹è¯• ---")
    tested_nodes = await test_clash_meta_nodes(clash_core_path, unified_config_path)
    final_output_links = []
    if tested_nodes:
        print("\n--- å»¶è¿Ÿæµ‹è¯•ç»“æœ (æŒ‰å»¶è¿Ÿå‡åº) ---")
        for node_info in tested_nodes:
            original_node = next((p for p in unique_proxies if p.get("name") == node_info["name"]), None)
            if original_node:
                link = generate_plaintext_node_link(original_node)
                if link:
                    final_output_links.append(f"{link} # {node_info['delay']}ms")
                    print(f"{node_info['name']}: {node_info['delay']}ms -> {link}")
                else:
                    print(f"{node_info['name']}: {node_info['delay']}ms -> æ— æ³•ç”Ÿæˆæ˜æ–‡é“¾æ¥")
            else:
                print(f"âš ï¸ è­¦å‘Šï¼šæ‰¾ä¸åˆ°åŸå§‹èŠ‚ç‚¹ä¿¡æ¯ '{node_info['name']}'")
    else:
        print("\nğŸ˜” æ²¡æœ‰èŠ‚ç‚¹é€šè¿‡å»¶è¿Ÿæµ‹è¯•ï¼Œè¾“å‡ºæ‰€æœ‰åŸå§‹èŠ‚ç‚¹é“¾æ¥ã€‚")
        final_output_links = [generate_plaintext_node_link(node) for node in unique_proxies if generate_plaintext_node_link(node)]
    output_file_path = "data/all.txt"
    with open(output_file_path, "w", encoding="utf-8") as f:
        for link in final_output_links:
            f.write(link + "\n")
    print(f"\nâœ… æœ€ç»ˆçš„æµ‹è¯•ç»“æœå’Œæ˜æ–‡é“¾æ¥å·²å†™å…¥ï¼š{output_file_path}")
    print(f"æ€»å…±è¾“å‡º {len(final_output_links)} æ¡ç»“æœã€‚")

if __name__ == "__main__":
    asyncio.run(main())
