name: Discover and Process Proxy Nodes # 工作流名称，现在包含“发现”功能

on:
  push:
    branches:
      - main # 当推送到 main 分支时触发
  workflow_dispatch: # 允许手动触发工作流

jobs:
  process-nodes:
    runs-on: ubuntu-latest # 在最新的 Ubuntu 环境上运行
    steps:
      - name: Checkout repository # 检出代码仓库
        uses: actions/checkout@v4

      - name: Set up Python # 设置 Python 环境
        uses: actions/setup-python@v5
        with:
          python-version: '3.x' # 使用最新的 Python 3 版本

      - name: Install dependencies # 安装 Python 依赖
        run: |
          python -m pip install --upgrade pip # 升级 pip
          pip install aiohttp pyyaml # 安装所需的库 (requests 在您的脚本中没有直接使用，可以省略)

      - name: Create data directory if not exists # 创建数据目录
        run: mkdir -p data

      - name: Run URL Discovery Script (extract_nodes.py) # 运行 URL 发现脚本
        env:
          BOT: ${{ secrets.GITHUB_TOKEN }} # 传递 GitHub Token 用于 API 认证
        run: |
          # 运行脚本并将输出同时打印到控制台和日志文件
          python extract_nodes.py | tee extract_nodes_discovery.log
          # 注意：这里移除了 'cat extract_nodes_discovery.log'，因为 tee 已经会将内容输出到 stdout，Actions 会自动捕获。
        timeout-minutes: 10 # 设置此步骤的超时时间

      - name: Run Node Extraction and Testing Script (extract_nodes_from_hy2.py) # 运行节点提取和测试脚本
        env:
          BOT: ${{ secrets.GITHUB_TOKEN }} # 传递 GitHub Token
          TEST_NODES: "true" # 启用节点测试
          TEST_MAX_NODES: "50" # 测试的最大节点数
          TEST_TIMEOUT: "1" # 测试超时时间
        run: |
          # 这个脚本现在会读取由 extract_nodes.py 生成的 data/hy2.txt
          # 运行脚本并将输出同时打印到控制台和日志文件
          python extract_nodes_from_hy2.py | tee extract_nodes_processing.log
          # 注意：这里移除了 'cat extract_nodes_processing.log'，原因同上。
        timeout-minutes: 15 # 设置此步骤的超时时间

      - name: Upload logs and data as artifacts # 上传日志和数据作为 Artifacts
        if: always() # 无论之前的步骤是否成功，都执行此步骤
        uses: actions/upload-artifact@v4
        with:
          name: processed-proxy-nodes-artifacts # Artifact 的名称
          path: |
            extract_nodes_discovery.log # 搜索脚本的日志
            extract_nodes_processing.log # 处理脚本的日志
            data/search_debug.log # 搜索脚本的调试日志
            data/extraction_debug.log # 处理脚本的调试日志 (注意文件名从 extract_debug.log 改为 extraction_debug.log 以匹配脚本)
            # data/temp_nodes.txt # 移除此行，因为该文件在脚本运行结束时会被删除，不应作为 Artifact 上传
            data/protocol_nodes.txt # 最终的有效协议节点列表
            data/yaml_nodes.yaml # 最终的 YAML 节点列表
            data/hy2.txt # extract_nodes.py 发现的原始 URL 列表
            data/invalid_urls.txt # 自动记录的无效 URL 列表
          if-no-files-found: warn # 如果文件未找到，只发出警告而不是失败

      - name: Commit and push changes # 提交并推送更改
        run: |
          git config --global user.name 'GitHub Action' # 配置 Git 用户名
          git config --global user.email 'action@github.com' # 配置 Git 用户邮箱
          
          # 确保临时文件 data/temp_nodes.txt 不会被 Git 追踪或尝试添加
          # 因为它在脚本运行结束后会被删除
          git rm --cached data/temp_nodes.txt || true # 如果之前被追踪，则从 Git 索引中移除
          git reset HEAD data/temp_nodes.txt || true # 如果被暂存，则取消暂存

          # 添加所有可能被修改或生成的文件到暂存区
          # 明确列出所有需要提交的文件，包括日志文件（如果希望它们被提交到仓库）
          git add data/hy2.txt data/invalid_urls.txt data/protocol_nodes.txt data/yaml_nodes.yaml data/search_debug.log data/extraction_debug.log
          
          # 只有当有实际的更改需要提交时才执行 commit
          git diff --staged --quiet || git commit -m "Automated: Update proxy nodes and logs"
          
          # 推送更改到远程仓库
          # 对于自动化工作流，直接 push 通常是可行的，如果存在并发推送，可能会有冲突，
          # 但对于这类定期更新数据的场景，通常不是大问题。
          git push
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} # 传递 GitHub Token 用于 Git 推送
